Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
VisDrone




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Datasets
Detection
Argoverse
COCO
COCO8
COCO8-Grayscale
COCO8-Multispectral
COCO128
LVIS
GlobalWheat2020
Objects365
OpenImagesV7
SKU-110K
HomeObjects-3K
Construction-PPE
VisDrone
VOC
xView
RF100
Brain-tumor
African-wildlife
Signature
Medical-pills
Segmentation
COCO
COCO8-seg
COCO128-seg
Crack-seg
Carparts-seg
Package-seg
Pose
COCO
COCO8-pose
Tiger-pose
Hand-keypoints
Dog-pose
Classification
Caltech 101
Caltech 256
CIFAR-10
CIFAR-100
Fashion-MNIST
ImageNet
ImageNet-10
Imagenette
Imagewoof
MNIST
Oriented Bounding Boxes (OBB)
DOTAv2
DOTA8
Multi-Object Tracking
Table of contents
Dataset Structure
Applications
Dataset YAML
Usage
Sample Data and Annotations
Citations and Acknowledgments
FAQ
What is the VisDrone Dataset and what are its key features?
How can I use the VisDrone Dataset to train a YOLO11 model with Ultralytics?
What are the main subsets of the VisDrone dataset and their applications?
Where can I find the configuration file for the VisDrone dataset in Ultralytics?
How can I cite the VisDrone dataset if I use it in my research?
VisDrone Dataset
The VisDrone Dataset is a large-scale benchmark created by the AISKYEYE team at the Lab of Machine Learning and Data Mining, Tianjin University, China. It contains carefully annotated ground truth data for various computer vision tasks related to drone-based image and video analysis.



Watch: How to Train Ultralytics YOLO Models on the VisDrone Dataset for Drone Image Analysis

VisDrone is composed of 288 video clips with 261,908 frames and 10,209 static images, captured by various drone-mounted cameras. The dataset covers a wide range of aspects, including location (14 different cities across China), environment (urban and rural), objects (pedestrians, vehicles, bicycles, etc.), and density (sparse and crowded scenes). The dataset was collected using various drone platforms under different scenarios and weather and lighting conditions. These frames are manually annotated with over 2.6 million bounding boxes of targets such as pedestrians, cars, bicycles, and tricycles. Attributes like scene visibility, object class, and occlusion are also provided for better data utilization.

Dataset Structure
The VisDrone dataset is organized into five main subsets, each focusing on a specific task:

Task 1: Object detection in images
Task 2: Object detection in videos
Task 3: Single-object tracking
Task 4: Multi-object tracking
Task 5: Crowd counting
Applications
The VisDrone dataset is widely used for training and evaluating deep learning models in drone-based computer vision tasks such as object detection, object tracking, and crowd counting. The dataset's diverse set of sensor data, object annotations, and attributes make it a valuable resource for researchers and practitioners in the field of drone-based computer vision.

Dataset YAML
A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the Visdrone dataset, the VisDrone.yaml file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/VisDrone.yaml.

ultralytics/cfg/datasets/VisDrone.yaml


# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

# VisDrone2019-DET dataset https://github.com/VisDrone/VisDrone-Dataset by Tianjin University
# Documentation: https://docs.ultralytics.com/datasets/detect/visdrone/
# Example usage: yolo train data=VisDrone.yaml
# parent
# ‚îú‚îÄ‚îÄ ultralytics
# ‚îî‚îÄ‚îÄ datasets
#     ‚îî‚îÄ‚îÄ VisDrone ‚Üê downloads here (2.3 GB)

# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: VisDrone # dataset root dir
train: images/train # train images (relative to 'path') 6471 images
val: images/val # val images (relative to 'path') 548 images
test: images/test # test-dev images (optional) 1610 images

# Classes
names:
  0: pedestrian
  1: people
  2: bicycle
  3: car
  4: van
  5: truck
  6: tricycle
  7: awning-tricycle
  8: bus
  9: motor

# Download script/URL (optional) ---------------------------------------------------------------------------------------
download: |
  import os
  from pathlib import Path
  import shutil

  from ultralytics.utils.downloads import download
  from ultralytics.utils import ASSETS_URL, TQDM


  def visdrone2yolo(dir, split, source_name=None):
      """Convert VisDrone annotations to YOLO format with images/{split} and labels/{split} structure."""
      from PIL import Image

      source_dir = dir / (source_name or f"VisDrone2019-DET-{split}")
      images_dir = dir / "images" / split
      labels_dir = dir / "labels" / split
      labels_dir.mkdir(parents=True, exist_ok=True)

      # Move images to new structure
      if (source_images_dir := source_dir / "images").exists():
          images_dir.mkdir(parents=True, exist_ok=True)
          for img in source_images_dir.glob("*.jpg"):
              img.rename(images_dir / img.name)

      for f in TQDM((source_dir / "annotations").glob("*.txt"), desc=f"Converting {split}"):
          img_size = Image.open(images_dir / f.with_suffix(".jpg").name).size
          dw, dh = 1.0 / img_size[0], 1.0 / img_size[1]
          lines = []

          with open(f, encoding="utf-8") as file:
              for row in [x.split(",") for x in file.read().strip().splitlines()]:
                  if row[4] != "0":  # Skip ignored regions
                      x, y, w, h = map(int, row[:4])
                      cls = int(row[5]) - 1
                      # Convert to YOLO format
                      x_center, y_center = (x + w / 2) * dw, (y + h / 2) * dh
                      w_norm, h_norm = w * dw, h * dh
                      lines.append(f"{cls} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\n")

          (labels_dir / f.name).write_text("".join(lines), encoding="utf-8")


  # Download (ignores test-challenge split)
  dir = Path(yaml["path"])  # dataset root dir
  urls = [
      f"{ASSETS_URL}/VisDrone2019-DET-train.zip",
      f"{ASSETS_URL}/VisDrone2019-DET-val.zip",
      f"{ASSETS_URL}/VisDrone2019-DET-test-dev.zip",
      # f"{ASSETS_URL}/VisDrone2019-DET-test-challenge.zip",
  ]
  download(urls, dir=dir, threads=4)

  # Convert
  splits = {"VisDrone2019-DET-train": "train", "VisDrone2019-DET-val": "val", "VisDrone2019-DET-test-dev": "test"}
  for folder, split in splits.items():
      visdrone2yolo(dir, split, folder)  # convert VisDrone annotations to YOLO labels
      shutil.rmtree(dir / folder)  # cleanup original directory
Usage
To train a YOLO11n model on the VisDrone dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.

Train Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data="VisDrone.yaml", epochs=100, imgsz=640)

Sample Data and Annotations
The VisDrone dataset contains a diverse set of images and videos captured by drone-mounted cameras. Here are some examples of data from the dataset, along with their corresponding annotations:

Dataset sample image

Task 1: Object detection in images - This image demonstrates an example of object detection in images, where objects are annotated with bounding boxes. The dataset provides a wide variety of images taken from different locations, environments, and densities to facilitate the development of models for this task.
The example showcases the variety and complexity of the data in the VisDrone dataset and highlights the importance of high-quality sensor data for drone-based computer vision tasks.

Citations and Acknowledgments
If you use the VisDrone dataset in your research or development work, please cite the following paper:


BibTeX

@ARTICLE{9573394,
  author={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Detection and Tracking Meet Drones Challenge},
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2021.3119563}}

We would like to acknowledge the AISKYEYE team at the Lab of Machine Learning and Data Mining, Tianjin University, China, for creating and maintaining the VisDrone dataset as a valuable resource for the drone-based computer vision research community. For more information about the VisDrone dataset and its creators, visit the VisDrone Dataset GitHub repository.

FAQ
What is the VisDrone Dataset and what are its key features?
The VisDrone Dataset is a large-scale benchmark created by the AISKYEYE team at Tianjin University, China. It is designed for various computer vision tasks related to drone-based image and video analysis. Key features include:

Composition: 288 video clips with 261,908 frames and 10,209 static images.
Annotations: Over 2.6 million bounding boxes for objects like pedestrians, cars, bicycles, and tricycles.
Diversity: Collected across 14 cities, in urban and rural settings, under different weather and lighting conditions.
Tasks: Split into five main tasks‚Äîobject detection in images and videos, single-object and multi-object tracking, and crowd counting.
How can I use the VisDrone Dataset to train a YOLO11 model with Ultralytics?
To train a YOLO11 model on the VisDrone dataset for 100 epochs with an image size of 640, you can follow these steps:

Train Example


Python
CLI

from ultralytics import YOLO

# Load a pretrained model
model = YOLO("yolo11n.pt")

# Train the model
results = model.train(data="VisDrone.yaml", epochs=100, imgsz=640)

For additional configuration options, please refer to the model Training page.

What are the main subsets of the VisDrone dataset and their applications?
The VisDrone dataset is divided into five main subsets, each tailored for a specific computer vision task:

Task 1: Object detection in images.
Task 2: Object detection in videos.
Task 3: Single-object tracking.
Task 4: Multi-object tracking.
Task 5: Crowd counting.
These subsets are widely used for training and evaluating deep learning models in drone-based applications such as surveillance, traffic monitoring, and public safety.

Where can I find the configuration file for the VisDrone dataset in Ultralytics?
The configuration file for the VisDrone dataset, VisDrone.yaml, can be found in the Ultralytics repository at the following link: VisDrone.yaml.

How can I cite the VisDrone dataset if I use it in my research?
If you use the VisDrone dataset in your research or development work, please cite the following paper:


BibTeX

@ARTICLE{9573394,
  author={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Detection and Tracking Meet Drones Challenge},
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2021.3119563}
}



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 7 months ago
glenn-jocher
RizwanMunawar
UltralyticsAssistant
MatthewNoyce
jk4e
Laughing-q

Tweet

Share

Comments

 Back to top
Previous
Construction-PPE
Next
VOC
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
 
Skip to content
Navigation Menu
Platform
Solutions
Resources
Open Source
Enterprise
Pricing

Search or jump to...
Sign in
Sign up
VisDrone
/
VisDrone-Dataset
Public
Code
Issues
47
Pull requests
3
Actions
Projects
Security
Insights
VisDrone/VisDrone-Dataset
Go to file
Name    
VisDrone
VisDrone
Update README.md
4364e82
 ¬∑ 
2 years ago
README.md
Update README.md
2 years ago
Repository files navigation
README
VisDrone-Dataset
VisDrone

Drones, or general UAVs, equipped with cameras have been fast deployed to a wide range of applications, including agricultural, aerial photography, fast delivery, and surveillance. Consequently, automatic understanding of visual data collected from these platforms become highly demanding, which brings computer vision to drones more and more closely. We are excited to present a large-scale benchmark with carefully annotated ground-truth for various important computer vision tasks, named VisDrone, to make vision meet drones. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining , Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization.

The challenge mainly focuses on four tasks:

(1) Task 1: object detection in images challenge. The task aims to detect objects of predefined categories (e.g., cars and pedestrians) from individual images taken from drones.

(2) Task 2: object detection in videos challenge. The task is similar to Task 1, except that objects are required to be detected from videos.

(3) Task 3: single-object tracking challenge. The task aims to estimate the state of a target, indicated in the first frame, in the subsequent video frames.

(4) Task 4: multi-object tracking challenge. The task aims to recover the trajectories of objects in each video frame.

(5) Task 5: crowd counting challenge. The task aims to to count persons in each video frame.

Download
Note that the bounding box annotations of test-dev are avalialbe. Researchers can use test-dev to publish papers. testset-challenge is used for VisDrone2020 Challenge and the annotations is unavailable.

Task 1: Object Detection in Images
VisDrone-DET dataset

trainset (1.44 GB): BaiduYun | GoogleDrive

valset (0.07 GB): BaiduYun | GoogleDrive

testset-dev (0.28 GB): BaiduYun | GoogleDrive (GT avalialbe)

testset-challenge (0.28 GB): BaiduYun | GoogleDrive

VisDrone-DET toolkit:

Matlab beta
Task 2: Object Detection in Videos
VisDrone-VID dataset

trainset (7.53 GB): BaiduYun | GoogleDrive

valset (1.49 GB): BaiduYun | GoogleDrive

testset-dev (2.14 GB): BaiduYun | GoogleDrive(GT avalialbe)

testset-challenge (2.70 GB): BaiduYun | GoogleDrive

VisDrone-VID toolkit:

Matlab beta
Task 3: Single-Object Tracking
VisDrone-SOT dataset

trainset_part1 (7.78 GB): BaiduYun | GoogleDrive

trainset_part2 (12.59 GB): BaiduYun | GoogleDrive

valset (1.29 GB): BaiduYun | GoogleDrive

testset-dev (11.27 GB): BaiduYun | GoogleDrive(GT avalialbe)

testset-challenge_part1 (17.40 GB): BaiduYun | GoogleDrive

testset-challenge_part2 (17.31 GB): BaiduYun | GoogleDrive

testset-challenge_initialization(12 KB): BaiduYun | GoogleDrive

VisDrone-SOT toolkit:

Matlab beta
Task 4: Multi-Object Tracking
VisDrone-MOT dataset

trainset (7.53 GB): BaiduYun | GoogleDrive

valset (1.48 GB): BaiduYun | GoogleDrive

testset-dev (2.14 GB): BaiduYun | GoogleDrive(GT avalialbe)

testset-challenge (2.70 GB): BaiduYun | GoogleDrive

VisDrone-MOT toolkit:

Matlab beta
Task 5: Crowd Counting
ECCV2020 Challenge DroneCrowd (1.03 GB): BaiduYun(code: h0j8)| GoogleDrive

Citation
@article{zhu2021detection,
  title={Detection and tracking meet drones challenge},
  author={Zhu, Pengfei and Wen, Longyin and Du, Dawei and Bian, Xiao and Fan, Heng and Hu, Qinghua and Ling, Haibin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={11},
  pages={7380--7399},
  year={2021},
  publisher={IEEE}
}
About
The dataset for drone based detection and tracking is released, including both image/video, and annotations.

Resources
 Readme
 Activity
Stars
 1.8k stars
Watchers
 18 watching
Forks
 198 forks
Report repository
Releases
No releases published
Packages
No packages published
Footer
¬© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Community
Docs
Contact
Manage cookies
Do not share my personal information
Transcript


Search in video
Introduction to VisDrone Dataset: Get an overview of the VisDrone dataset and its applications in drone-based imagery analysis.
0:00
so in this video here we're going to see
0:01
how we can train a custom Yol 8 model on
0:03
the V drone data set so this is a data
0:06
set containing images from drones so
0:08
this is a really cool use case for a lot
0:10
of computer vision applications and also
0:12
projects out there we're going to use
0:14
the altic Hop I'm going to show you how
0:16
we can use the data set train the model
0:18
in just a few clicks with altic hop and
0:20
then we're going to see how we can run
0:21
live inference with the model that we
0:23
have trained on drone images and videos
0:27
so let's just jump straight into the
0:28
altic documentation we can go in and see
Ultralytics VisDrone Documentation Walkthrough: Navigate through the Ultralytics documentation to efficiently work with the VisDrone dataset.
0:30
we have this withdrawn data set I'm
0:32
going to walk you through the data set
0:34
structure Ser some image examples and
0:36
then we will jump in and train the model
0:38
directly so right now if you go inside
0:41
the data sets tab up at the top inside
0:43
the TIC documentation we can then see
0:45
the data set structure all the different
VisDrone Dataset Structure: Understand the layout and organization of the dataset.
0:47
task and so on we have which it can be
0:49
used for update taking and images videos
0:52
single object tracking and also
0:54
multiobject tracking and crowd counting
0:56
so this is really useful if we have a
0:59
drone which we basically want to do
1:00
monitoring with could be traffic
1:02
intersections different places where we
1:05
want to do crowd counting or basically
1:06
just crowd management if we scroll a bit
1:09
further down we can then see the classes
Dataset YAML and Classes Overview: Learn about the YAML configuration and class labels used in the dataset.
1:11
that we have in our data set inside the
1:13
data yl file so we have pedestrian
1:16
people bicycle car van truck tricycle
1:19
awning tricycle boss and also a motor so
1:23
these are really useful if you want to
1:24
do traffic monitoring on drone videos so
1:28
we can use this directly after box and
1:30
also if you want to train it locally you
How to Train Ultralytics YOLO models on VisDrone Dataset Locally: Discover how to set up and train models on your local machine.
1:32
can just specify the withdrawn data set
1:34
yl file and it's going to download it
1:36
automatically so you can train it with
1:38
just a few lines of code on your own
1:40
local environment in a Google Cod
1:42
notebook or wherever so this is one
1:45
sample from the data set which is
1:46
already annotated we have pedestrians
VisDrone Dataset Samples: Explore sample images and annotations to better understand the dataset's contents.
1:49
people and so on walking around and we
1:51
just have this drone shot so it both
1:53
works on videos but also images so this
1:56
is pretty cool let's just jump straight
1:57
into the AL L hop and take a look at how
1:59
we can train a custom Yol 8 model so we
Ultralytics HUB Walkthrough and Usage: Learn how to use the Ultralytics HUB for managing, fine-tuning, and deploying your models with ease.
2:03
just jump straight into the allytics Hop
2:05
let's now go down into our data sets so
2:08
we can just upload our data set in here
2:10
set up a Yol 8 model that we can train
2:12
directly we already have tons of videos
2:14
covering all of it but this is a very
2:16
cool use case so we have this wi drone
2:18
data set 8,600 images with 10 classes we
2:22
already went through those we see some
2:25
of the thumbnails here so this is really
2:26
good also if you want to do parking
Upcoming Video, Parking Management Overview with VisDrone Dataset: Get a sneak peek into our next video, where we'll tackle parking management using the VisDrone dataset.
2:28
management and so on in one upcoming
2:30
videos we're going to create a project
2:31
where we can do parking management so we
2:33
can basically just track how many
2:35
available parking spots are there and so
2:37
on we can dra it specific bounding boxes
2:39
and also polygon zones where those
2:42
parking spots are then we can check if
2:44
they're occupied or not with this V
2:47
drone model as well so we're going to
2:49
train the model in this video here we're
2:50
going to see some inference results and
2:51
then we're going to use it to create a
2:53
real world application and projects with
2:56
Al litics as well for pugin management
2:59
so here we can just see some thumbnails
3:01
with all the individual annotations and
3:03
images let's now just go up and hit
3:05
train model we can specify which of the
Train Ultralytics YOLOv8 model on VisDrone Dataset using HUB with Cloud Training Feature: Follow a step-by-step guide to training models using the HUB cloud training feature in the Ultralytics HUB.
3:07
models we want to use right now let's
3:09
just go with the small model we can set
3:11
some Advanced model configurations all
3:13
the high parameters here and so on we
3:15
have covered that in separate videos so
3:17
definitely check those out so now we
3:19
just hit continue we can train on Al
3:21
litics hop account choose the available
3:24
instance and also for how many Epoch
3:26
right now let's just go with 50 Epoch
3:27
because we have a lot of images in our
3:30
data set once we're ready to train we
3:32
can just hit start training and it's
3:34
going to take care of all of it it's
3:35
going to set up an instance here you can
3:37
see all the results live all the metrics
3:40
mean positions losses and so on live
3:42
while your model is training once it's
3:44
done training we can just download the
3:46
model to our local environment and run
3:48
inference with the model so now model is
Export the Trained Model from Ultralytics HUB to PyTorch Format: Learn how to export your trained models for further use.
3:51
done training we can then go inside our
3:53
deploy tab if you scroll a bit further
3:55
down we can then see the different
3:56
export formats right now we can just
3:58
download the pytorch version and use
4:00
that directly in our own applications
4:01
and projects so we're just going to
4:03
download here and then we can use it in
4:05
our own custom python script so I've
Raw Videos we will use for inference with VisDrone fine-tuned Model: Preview the raw video data we will process using our fine-tuned model.
4:07
just opened up my code editor these are
4:09
the videos that we're going to run the
4:10
model through so we just train the model
4:12
now now we're going to have these drone
4:14
video footage that we're going to
4:16
process so right now I've just opened up
4:18
this python script here we can take it
Python code for Inferencing using Ultralytics YOLOv8 Fine-Tuned Model: Get hands-on with Python code examples for running inferences on the fine-tuned model.
4:20
directly from the AL ltic documentation
4:22
as well but the only thing that we need
4:24
to do is just to import yellow from
4:26
alter litic first of all make sure that
4:28
you have Pip installed alter litic as
4:30
well create an instance with our model
4:32
that we have trained so this is the
4:33
allytics with drone Yol 8. pytorch
4:37
model so just delete the comments here
4:39
and then we can run inference with it
4:41
directly down here at the bottom but
4:43
first of all we need to specify the
4:45
source so the source will be our videos
4:48
so we just need to specify the path you
4:49
can also specify a stream here and so on
4:52
but I have a folder called videos and
4:54
then we can just start with a drone. MP4
4:57
just test that out we want to show the
4:58
results so we said show equal to true
5:02
and we also want to save our
5:04
results all of these arguments here can
5:06
be found under Al L documentation so you
5:08
can see all of them which can be set
5:11
directly here this result is just going
5:13
to return all the results with the
5:14
bounding boxes classes confidence scores
5:17
and so on so right now let's open a new
5:21
terminal there we go I'm going to
5:23
activate my cond
5:26
environment and we can just run our
5:28
python script Wiis drone there we go we
5:31
should open up the video capture with
5:33
our Drone footage and see the results so
Final Visuals with VisDrone fine-tuned model: See the final output visuals showcasing the capabilities of the fine-tuned model.
5:36
I'm just running this on my MacBook
5:37
right now but we can see that these are
5:38
some pretty cool results if you're just
5:41
using the Yol V 8 model out of the box
5:43
and won't be able to detect these cars
5:44
with as high accuracy so here we can
5:47
pretty much see that we're detecting
5:48
everything sometimes it's swapping a bit
5:50
between car and Van but that's also
5:53
pretty good we can see some trucks
5:54
coming in here now which is also very
5:57
cool now we can go and do traffic
5:59
analysis based on how many cars are
6:00
moving in One Direction the other
6:02
direction and so on for this traffic
6:04
intersection just to test it out let's
6:06
go and test out the Yol 8 small model
6:09
just a pre-train one which is not
6:11
trained on this visual drone data set
6:13
just to see a comparison so here we just
6:15
have a pre-ra model it is not able to
6:17
detect these cars it is just they are
6:19
basically just like too small to be able
6:21
to detect anything on so this is the
Why the Model Fine-tuning Required in Different Use Cases?: Understand the importance and benefits of fine-tuning models for specific applications.
6:23
reason why we need to go in and find two
6:25
our models on our own custom data set
6:27
even though we have cars pedestrians and
6:29
all of that in the pre-trained models so
6:32
let's just revert it back here again to
6:34
our V drone model and now instead of
6:37
drone we have traffic.
6:39
MP4 and we're just going to rerun it and
6:42
we should be able to see some pretty
6:44
cool result as well so here we can see
6:46
the other video running we have motor
6:47
pedestrian coming here on the sidewalk
6:49
and we also have all these cars very
6:51
high confidence score for all the cars
6:53
even though we can even see like the
6:54
front of the cars here we still have
6:56
high confidence score and detect pretty
6:58
much all of them then we can create
7:00
accounting system tracking system and so
7:02
on on top of it and basically create
7:03
these traffic Management Systems so this
7:05
is really helpful easy to use and you
7:08
just saw how easy it is to have up and
7:09
running with the Alo litic Hub it's just
7:12
a few clicks we need the data set export
7:14
the model and use the allytics framework
7:16
so this is very easy to get up and
7:18
running as I mentioned in one of the
7:20
upcoming videos we're going to create a
7:22
whole traffic management system so a
7:24
parking management system so we can see
7:26
how many parking spots are occupied how
7:28
many are available you have probably
7:30
seen that if you have been going to
7:32
malls different places and so on so
Conclusion and Summary: Recap the key insights and explore the impact of using the VisDrone dataset with Ultralytics tools in your AI projects.
7:34
check those things out and also the
7:35
videos here on the ult Le Channel and
7:38
then I'll just see you guys in one of
7:39
the upcoming videos until then Happy
7:41
learning

Platform
Solutions
Resources
Open Source
Enterprise
Pricing

Search or jump to...
Sign in
Sign up
ultralytics
/
ultralytics
Public
Code
Issues
231
Pull requests
145
Discussions
Actions
Projects
1
Wiki
Security
Insights
Files
Go to file
.github
docker
docs
examples
tests
ultralytics
assets
cfg
datasets
Argoverse.yaml
DOTAv1.5.yaml
DOTAv1.yaml
GlobalWheat2020.yaml
HomeObjects-3K.yaml
ImageNet.yaml
Objects365.yaml
SKU-110K.yaml
VOC.yaml
VisDrone.yaml
african-wildlife.yaml
brain-tumor.yaml
carparts-seg.yaml
coco-pose.yaml
coco.yaml
coco128-seg.yaml
coco128.yaml
coco8-grayscale.yaml
coco8-multispectral.yaml
coco8-pose.yaml
coco8-seg.yaml
coco8.yaml
construction-ppe.yaml
crack-seg.yaml
dog-pose.yaml
dota8-multispectral.yaml
dota8.yaml
hand-keypoints.yaml
lvis.yaml
medical-pills.yaml
open-images-v7.yaml
package-seg.yaml
signature.yaml
tiger-pose.yaml
xView.yaml
models
trackers
__init__.py
default.yaml
data
engine
hub
models
nn
solutions
trackers
utils
__init__.py
py.typed
.dockerignore
.gitignore
CITATION.cff
CONTRIBUTING.md
LICENSE
README.md
README.zh-CN.md
mkdocs.yml
pyproject.toml
ultralytics/ultralytics/cfg/datasets
/VisDrone.yaml
glenn-jocher
glenn-jocher
Use ASSETS_URL in dataset YAMLs (#22361)
41859a5
 ¬∑ 
last week
ultralytics/ultralytics/cfg/datasets
/VisDrone.yaml

Code

Blame
87 lines (72 loc) ¬∑ 3.32 KB
# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

# VisDrone2019-DET dataset https://github.com/VisDrone/VisDrone-Dataset by Tianjin University
# Documentation: https://docs.ultralytics.com/datasets/detect/visdrone/
# Example usage: yolo train data=VisDrone.yaml
# parent
# ‚îú‚îÄ‚îÄ ultralytics
# ‚îî‚îÄ‚îÄ datasets
#     ‚îî‚îÄ‚îÄ VisDrone ‚Üê downloads here (2.3 GB)

# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: VisDrone # dataset root dir
train: images/train # train images (relative to 'path') 6471 images
val: images/val # val images (relative to 'path') 548 images
test: images/test # test-dev images (optional) 1610 images

# Classes
names:
  0: pedestrian
  1: people
  2: bicycle
  3: car
  4: van
  5: truck
  6: tricycle
  7: awning-tricycle
  8: bus
  9: motor

# Download script/URL (optional) ---------------------------------------------------------------------------------------
download: |
  import os
  from pathlib import Path
  import shutil

  from ultralytics.utils.downloads import download
  from ultralytics.utils import ASSETS_URL, TQDM


  def visdrone2yolo(dir, split, source_name=None):
      """Convert VisDrone annotations to YOLO format with images/{split} and labels/{split} structure."""
      from PIL import Image

      source_dir = dir / (source_name or f"VisDrone2019-DET-{split}")
      images_dir = dir / "images" / split
      labels_dir = dir / "labels" / split
      labels_dir.mkdir(parents=True, exist_ok=True)

      # Move images to new structure
      if (source_images_dir := source_dir / "images").exists():
          images_dir.mkdir(parents=True, exist_ok=True)
          for img in source_images_dir.glob("*.jpg"):
              img.rename(images_dir / img.name)

      for f in TQDM((source_dir / "annotations").glob("*.txt"), desc=f"Converting {split}"):
          img_size = Image.open(images_dir / f.with_suffix(".jpg").name).size
          dw, dh = 1.0 / img_size[0], 1.0 / img_size[1]
          lines = []

          with open(f, encoding="utf-8") as file:
              for row in [x.split(",") for x in file.read().strip().splitlines()]:
                  if row[4] != "0":  # Skip ignored regions
                      x, y, w, h = map(int, row[:4])
                      cls = int(row[5]) - 1
                      # Convert to YOLO format
                      x_center, y_center = (x + w / 2) * dw, (y + h / 2) * dh
                      w_norm, h_norm = w * dw, h * dh
                      lines.append(f"{cls} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\n")

          (labels_dir / f.name).write_text("".join(lines), encoding="utf-8")


  # Download (ignores test-challenge split)
  dir = Path(yaml["path"])  # dataset root dir
  urls = [
      f"{ASSETS_URL}/VisDrone2019-DET-train.zip",
      f"{ASSETS_URL}/VisDrone2019-DET-val.zip",
      f"{ASSETS_URL}/VisDrone2019-DET-test-dev.zip",
      # f"{ASSETS_URL}/VisDrone2019-DET-test-challenge.zip",
  ]
  download(urls, dir=dir, threads=4)

  # Convert
  splits = {"VisDrone2019-DET-train": "train", "VisDrone2019-DET-val": "val", "VisDrone2019-DET-test-dev": "test"}
  for folder, split in splits.items():
      visdrone2yolo(dir, split, folder)  # convert VisDrone annotations to YOLO labels
      shutil.rmtree(dir / folder)  # cleanup original directory

Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
The Role of Epochs in Model Training
Epochs vs. Iterations vs. Batches
Determining the Right Number of Epochs
Real-World Examples
Glossary
Epoch
Learn about epochs in machine learning‚Äîhow they impact model training, prevent overfitting, and optimize performance with Ultralytics YOLO.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
In machine learning (ML), an epoch represents one complete pass of the entire training dataset through the learning algorithm. It is a fundamental concept in the iterative process of training neural networks (NN), where models learn by repeatedly seeing examples from the data. The number of epochs is a key parameter that determines how many times the model will learn from the full set of training information, directly influencing the final performance and quality of the model.

The Role of Epochs in Model Training
The primary goal of model training is to enable a model to learn patterns from data. This is achieved by adjusting the model's internal parameters, known as model weights, to minimize a loss function, which quantifies the error between the model's predictions and the actual ground truth. During a single epoch, the model processes every data sample, and an optimization algorithm like Stochastic Gradient Descent (SGD) updates these weights.

Training a model for multiple epochs allows it to iteratively refine its parameters. With each pass, the model should, in theory, become better at its task, whether it's image classification or object detection. This process is managed using popular deep learning frameworks such as PyTorch or TensorFlow.

Epochs vs. Iterations vs. Batches
While related, these terms describe different aspects of the training process and are often confused.

Epoch: One complete cycle where the model has seen the entire training dataset.
Batch Size: The number of training samples used in a single iteration. Due to memory constraints, it's often impractical to process the entire dataset at once.
Iteration: A single update of the model's weights. An iteration involves processing one batch of data and performing a forward and backward pass (backpropagation).
For example, if a dataset has 10,000 images and the batch size is 100, one epoch will consist of 100 iterations (10,000 images / 100 images per batch).

Determining the Right Number of Epochs
Choosing the correct number of epochs is a critical part of hyperparameter tuning. It involves finding a balance to avoid two common problems:

Underfitting: This occurs when the model is not trained for enough epochs. It fails to learn the underlying patterns in the data and performs poorly on both training and test data.
Overfitting: This happens when the model is trained for too many epochs. It starts to "memorize" the training data, including its noise, and loses its ability to generalize to new, unseen data. While it may have excellent accuracy on the training set, its performance on the validation data will be poor.
A common technique to combat overfitting is early stopping, where training is halted once the model's performance on a validation set ceases to improve. Progress can be monitored using tools like TensorBoard or through platforms like Ultralytics HUB, which helps visualize training metrics over epochs.

Real-World Examples
The concept of epochs is universal in deep learning applications.

Autonomous Driving: An object detection model for an autonomous vehicle is trained on a massive dataset like Argoverse. The model, such as Ultralytics YOLO11, might be trained for 50-100 epochs. After each epoch, its performance on a validation set is measured using metrics like mean Average Precision (mAP). Engineers will select the model from the epoch that offers the best balance of speed and accuracy before deployment.

Medical Image Analysis: A model for tumor detection in brain scans is trained on a specialized medical imaging dataset. Given that such datasets can be small, the model might be trained for several hundred epochs. To prevent overfitting, techniques like data augmentation are used, and the validation loss is closely monitored after each epoch. This ensures the final model generalizes well to scans from new patients. Following established model training tips is crucial for success in such critical applications.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
The Importance of High-Quality Training Data
Real-World Examples
Training Data vs. Validation and Test Data
Glossary
Training Data
Discover the importance of training data in AI. Learn how quality datasets power accurate, robust machine learning models for real-world tasks.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
Training data is the foundational dataset used to teach a machine learning (ML) model how to make accurate predictions or decisions. In supervised learning, this data consists of input samples paired with corresponding correct outputs, often called labels or annotations. The model iteratively learns from these examples, adjusting its internal model weights to minimize the difference between its predictions and the actual labels. The quality, quantity, and diversity of the training data are the most critical factors influencing a model's performance and its ability to generalize to new, unseen data.

The Importance of High-Quality Training Data
The principle of "garbage in, garbage out" is especially true for training ML models. High-quality data is essential for building robust and reliable systems. Key characteristics include:

Relevance: The data must accurately reflect the problem the model is intended to solve.
Diversity: It should cover a wide range of scenarios, edge cases, and variations that the model will encounter in the real world to avoid overfitting.
Accurate Labeling: The annotations must be correct and consistent. The process of data labeling is often the most time-consuming part of a computer vision project.
Sufficient Volume: A large amount of data is typically needed for the model to learn meaningful patterns. Techniques like data augmentation can help expand the dataset artificially.
Low Bias: The data should be balanced and representative to prevent dataset bias, which can lead to unfair or incorrect model behavior. Understanding algorithmic bias is a key aspect of responsible AI development.
Platforms like Ultralytics HUB provide tools to manage datasets throughout the model development lifecycle, while open-source tools like CVAT are popular for annotation tasks.

Real-World Examples
Autonomous Vehicles: To train an object detection model for autonomous vehicles, developers use vast amounts of training data from cameras and sensors. This data consists of images and videos where every frame is meticulously labeled. Pedestrians, cyclists, other cars, and traffic signs are enclosed in bounding boxes. By training on datasets like Argoverse or nuScenes, the vehicle's AI learns to perceive and navigate its environment safely.
Medical Image Analysis: In healthcare, training data for medical image analysis might consist of thousands of MRI or CT scans. Radiologists annotate these images to highlight tumors, fractures, or other pathologies. An ML model, such as one built with Ultralytics YOLO, can be trained on a brain tumor dataset to learn to identify these anomalies, acting as a powerful tool to assist doctors in making faster and more accurate diagnoses. Resources like The Cancer Imaging Archive (TCIA) provide public access to such data for research.
Training Data vs. Validation and Test Data
In a typical ML project, data is split into three distinct sets:

Training Data: The largest portion, used directly to train the model by adjusting its parameters. Effective training often involves careful consideration of tips for model training.
Validation Data: A separate subset used periodically during training to evaluate the model's performance on data it hasn't explicitly learned from. This helps in tuning hyperparameters (e.g., learning rate, batch size) via processes like Hyperparameter Optimization (Wikipedia) and provides an early warning against overfitting. The validation mode is used for this evaluation.
Test Data: An independent dataset, unseen during training and validation, used only after the model is fully trained. It provides the final, unbiased assessment of the model's generalization ability and expected performance in the real world. Rigorous model testing is crucial before deployment.
Maintaining a strict separation between these datasets is essential for developing reliable models. State-of-the-art models are often pre-trained on large benchmark datasets like COCO or ImageNet, which serve as extensive training data. You can find more datasets on platforms like Google Dataset Search and Kaggle Datasets.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar

logo
Home
nuPlan
nuScenes
nuImages
nuReality
Tasks
About

Motional logonuScenes
Overview
The nuScenes dataset (pronounced /nuÀêsiÀênz/) is a public large-scale dataset for autonomous driving developed by the team at Motional (formerly nuTonomy). Motional is making driverless vehicles a safe, reliable, and accessible reality. By releasing a subset of our data to the public, Motional aims to support public research into computer vision and autonomous driving.

For this purpose we collected 1000 driving scenes in Boston and Singapore, two cities that are known for their dense traffic and highly challenging driving situations. The scenes of 20 second length are manually selected to show a diverse and interesting set of driving maneuvers, traffic situations and unexpected behaviors. The rich complexity of nuScenes will encourage development of methods that enable safe driving in urban areas with dozens of objects per scene. Gathering data on different continents further allows us to study the generalization of computer vision algorithms across different locations, weather conditions, vehicle types, vegetation, road markings and left versus right hand traffic.

To facilitate common computer vision tasks, such as object detection and tracking, we annotate 23 object classes with accurate 3D bounding boxes at 2Hz over the entire dataset. Additionally we annotate object-level attributes such as visibility, activity and pose.

In March 2019, we released the full nuScenes dataset with all 1,000 scenes. The full dataset includes approximately 1.4M camera images, 390k LIDAR sweeps, 1.4M RADAR sweeps and 1.4M object bounding boxes in 40k keyframes. Additional features (map layers, raw sensor data, etc.) will follow soon. We are also organizing the nuScenes 3D detection challenge as part of the Workshop on Autonomous Driving at CVPR 2019.

The nuScenes dataset is inspired by the pioneering KITTI dataset. nuScenes is the first large-scale dataset to provide data from the entire sensor suite of an autonomous vehicle (6 cameras, 1 LIDAR, 5 RADAR, GPS, IMU). Compared to KITTI, nuScenes includes 7x more object annotations.

Whereas most of the previously released datasets focus on camera-based object detection (Cityscapes, Mapillary Vistas, Apolloscapes, Berkeley Deep Drive), the goal of nuScenes is to look at the entire sensor suite.

In July 2020, we released nuScenes-lidarseg. In nuScenes-lidarseg, we annotate each lidar point from a keyframe in nuScenes with one of 32 possible semantic labels (i.e. lidar semantic segmentation). As a result, nuScenes-lidarseg contains 1.4 billion annotated points across 40,000 pointclouds and 1000 scenes (850 scenes for training and validation, and 150 scenes for testing).

The nuScenes dataset is available as free to use strictly for non-commercial purposes. Non-commercial means not primarily intended for or directed towards commercial advantage or monetary compensation. Examples of non-commercial use include but are not limited to personal use, educational use, such as in schools, academies, universities etc., and some research use. If you intend to use the nuScenes dataset for commercial purposes, we encourage you to contact us for commercial licensing options by sending an e-mail to nuScenes@motional.com.

We hope that this dataset will allow researchers across the world to develop safe autonomous driving technology.

Please use the following citation when referencing nuScenes:
@article{nuscenes2019,
 title={nuScenes: A multimodal dataset for autonomous driving},
  author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and 
          Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and 
          Giancarlo Baldan and Oscar Beijbom}, 
  journal={arXiv preprint arXiv:1903.11027},
  year={2019}
}

Show more ‚Üì
Data Collection
Scene planning
For the nuScenes dataset we collect approximately 15h of driving data in Boston and Singapore. For the full nuScenes dataset, we publish data from Boston Seaport and Singapore‚Äôs One North, Queenstown and Holland Village districts. Driving routes are carefully chosen to capture challenging scenarios. We aim for a diverse set of locations, times and weather conditions. To balance the class frequency distribution, we include more scenes with rare classes (such as bicycles). Using these criteria, we manually select 1000 scenes of 20s duration each. These scenes are carefully annotated using human experts. The annotator instructions can be found in the devkit repository.

boston
Boston Seaport
queenstown
Singapore Queenstown
singapore
Singapore One North
holland
Singapore Holland Village
Car setup
We use two Renault Zoe cars with an identical sensor layout to drive in Boston and Singapore. The data was gathered from a research platform and is not indicative of the setup used in Motional products. Please refer to the above figure for the placement of the sensors. We release data from the following sensors:

data
1x spinning LIDAR (Velodyne HDL32E):

20Hz capture frequency
32 beams, 1080 (+-10) points per ring
32 channels
360¬∞ Horizontal FOV, +10¬∞ to -30¬∞ Vertical FOV, uniform azimuth angles
80m-100m Range, Usable returns up to 70 meters, ¬± 2 cm accuracy
Up to ~1.39 Million Points per Second
5x long range RADAR sensor (Continental ARS 408-21):

13Hz capture frequency
77GHz
Independently measures distance and velocity in one cycle using Frequency Modulated Continuous Wave
Up to 250m distance
Velocity accuracy of ¬±0.1 km/h
6x camera (Basler acA1600-60gc):

12Hz capture frequency
Evetar Lens N118B05518W F1.8 f5.5mm 1/1.8"
1/1.8'' CMOS sensor of 1600x1200 resolution
Bayer8 format for 1 byte per pixel encoding
1600x900 ROI is cropped from the original resolution to reduce processing and transmission bandwidth
Auto exposure with exposure time limited to the maximum of 20 ms
Images are unpacked to BGR format and compressed to JPEG
See camera orientation and overlap in the figure below.
1x IMU & GPS (Advanced Navigation Spatial):

Position accuracy of 20mm
Heading accuracy of 0.2¬∞ with GNSS
Roll & pitch accuracy of 0.1¬∞
Localization takes into account IMU, GPS and HD lidar maps (see our paper for more details)
camera
Sensor calibration
To achieve a high quality multi-sensor dataset, it is essential to calibrate the extrinsics and intrinsics of every sensor. We express extrinsic coordinates relative to the ego frame, i.e. the midpoint of the rear vehicle axle. The most relevant steps are described below:

LIDAR extrinsics:

We use a laser liner to accurately measure the relative location of the LIDAR to the ego frame.

Camera extrinsics:

We place a cube-shaped calibration target in front of the camera and LIDAR sensors. The calibration target consists of three orthogonal planes with known patterns. After detecting the patterns we compute the transformation matrix from camera to LIDAR by aligning the planes of the calibration target. Given the LIDAR to ego frame transformation computed above, we can then compute the camera to ego frame transformation and the resulting extrinsic parameters.

RADAR extrinsics

We mount the radar in a horizontal position. Then we collect radar measurements by driving in an urban environment. After filtering radar returns for moving objects, we calibrate the yaw angle using a brute force approach to minimize the compensated range rates for static objects.

Camera intrinsic calibration

We use a calibration target board with a known set of patterns to infer the intrinsic and distortion parameters of the camera.

Sensor synchronization
In order to achieve good cross-modality data alignment between the LIDAR and the cameras, the exposure of a camera is triggered when the top LIDAR sweeps across the center of the camera‚Äôs FOV. The timestamp of the image is the exposure trigger time; and the timestamp of the LIDAR scan is the time when the full rotation of the current LIDAR frame is achieved. Given that the camera‚Äôs exposure time is nearly instantaneous, this method generally yields good data alignment. Note that the cameras run at 12Hz while the LIDAR runs at 20Hz. The 12 camera exposures are spread as evenly as possible across the 20 LIDAR scans, so not all LIDAR scans have a corresponding camera frame. Reducing the frame rate of the cameras to 12Hz helps to reduce the compute, bandwidth and storage requirement of the perception system.

Privacy protection
It is our priority to protect the privacy of third parties. For this purpose we use state-of-the-art object detection techniques to detect license plates and faces. We aim for a high recall and remove false positives that do not overlap with the reprojections of the known person and car boxes. Eventually we use the output of the object detectors to blur faces and license plates in the images of nuScenes.

Show more ‚Üì
Data format
This document describes the database schema used in nuScenes. All annotations and meta data (including calibration, maps, vehicle coordinates etc.) are covered in a relational database. The database tables are listed below. Every row can be identified by its unique primary key token. Foreign keys such as sample_token may be used to link to the token of the table sample. Please refer to the tutorial for an introduction to the most important database tables.



attribute
An attribute is a property of an instance that can change while the category remains the same. Example: a vehicle being parked/stopped/moving, and whether or not a bicycle has a rider.

attribute {
   "token":                   <str> -- Unique record identifier.
   "name":                    <str> -- Attribute name.
   "description":             <str> -- Attribute description.
}
calibrated_sensor
Definition of a particular sensor (lidar/radar/camera) as calibrated on a particular vehicle. All extrinsic parameters are given with respect to the ego vehicle body frame. All camera images come undistorted and rectified.

calibrated_sensor {
   "token":                   <str> -- Unique record identifier.
   "sensor_token":            <str> -- Foreign key pointing to the sensor type.
   "translation":             <float> [3] -- Coordinate system origin in meters: x, y, z.
   "rotation":                <float> [4] -- Coordinate system orientation as quaternion: w, x, y, z.
   "camera_intrinsic":        <float> [3, 3] -- Intrinsic camera calibration. Empty for sensors that are not cameras.
}
category
Taxonomy of object categories (e.g. vehicle, human). Subcategories are delineated by a period (e.g. human.pedestrian.adult).

category {
   "token":                   <str> -- Unique record identifier.
   "name":                    <str> -- Category name. Subcategories indicated by period.
   "description":             <str> -- Category description.
   "index":                   <int> -- The index of the label used for efficiency reasons in the .bin label files of nuScenes-lidarseg. This field did not exist previously.
}
ego_pose
Ego vehicle pose at a particular timestamp. Given with respect to global coordinate system of the log's map. The ego_pose is the output of a lidar map-based localization algorithm described in our paper. The localization is 2-dimensional in the x-y plane.

ego_pose {
   "token":                   <str> -- Unique record identifier.
   "translation":             <float> [3] -- Coordinate system origin in meters: x, y, z. Note that z is always 0.
   "rotation":                <float> [4] -- Coordinate system orientation as quaternion: w, x, y, z.
   "timestamp":               <int> -- Unix time stamp.
}
instance
An object instance, e.g. particular vehicle. This table is an enumeration of all object instances we observed. Note that instances are not tracked across scenes.

instance {
   "token":                   <str> -- Unique record identifier.
   "category_token":          <str> -- Foreign key pointing to the object category.
   "nbr_annotations":         <int> -- Number of annotations of this instance.
   "first_annotation_token":  <str> -- Foreign key. Points to the first annotation of this instance.
   "last_annotation_token":   <str> -- Foreign key. Points to the last annotation of this instance.
}
lidarseg
Mapping between nuScenes-lidarseg annotations and sample_datas corresponding to the lidar pointcloud associated with a keyframe.

lidarseg {
   "token":                   <str> -- Unique record identifier.
   "filename":                <str> -- The name of the .bin files containing the nuScenes-lidarseg labels. These are numpy arrays of uint8 stored in binary format using numpy.
   "sample_data_token":       <str> -- Foreign key. Sample_data corresponding to the annotated lidar pointcloud with is_key_frame=True. 
}
log
Information about the log from which the data was extracted.

log {
   "token":                   <str> -- Unique record identifier.
   "logfile":                 <str> -- Log file name.
   "vehicle":                 <str> -- Vehicle name.
   "date_captured":           <str> -- Date (YYYY-MM-DD).
   "location":                <str> -- Area where log was captured, e.g. singapore-onenorth.
}
map
Map data that is stored as binary semantic masks from a top-down view.

map {
   "token":                   <str> -- Unique record identifier.
   "log_tokens":              <str> [n] -- Foreign keys.
   "category":                <str> -- Map category, currently only semantic_prior for drivable surface and sidewalk.
   "filename":                <str> -- Relative path to the file with the map mask.
}
sample
A sample is an annotated keyframe at 2 Hz. The data is collected at (approximately) the same timestamp as part of a single LIDAR sweep.

sample {
   "token":                   <str> -- Unique record identifier.
   "timestamp":               <int> -- Unix time stamp.
   "scene_token":             <str> -- Foreign key pointing to the scene.
   "next":                    <str> -- Foreign key. Sample that follows this in time. Empty if end of scene.
   "prev":                    <str> -- Foreign key. Sample that precedes this in time. Empty if start of scene.
}
sample_annotation
A bounding box defining the position of an object seen in a sample. All location data is given with respect to the global coordinate system.

sample_annotation {
   "token":                   <str> -- Unique record identifier.
   "sample_token":            <str> -- Foreign key. NOTE: this points to a sample NOT a sample_data since annotations are done on the sample level taking all relevant sample_data into account.
   "instance_token":          <str> -- Foreign key. Which object instance is this annotating. An instance can have multiple annotations over time.
   "attribute_tokens":        <str> [n] -- Foreign keys. List of attributes for this annotation. Attributes can change over time, so they belong here, not in the instance table.
   "visibility_token":        <str> -- Foreign key. Visibility may also change over time. If no visibility is annotated, the token is an empty string.
   "translation":             <float> [3] -- Bounding box location in meters as center_x, center_y, center_z.
   "size":                    <float> [3] -- Bounding box size in meters as width, length, height.
   "rotation":                <float> [4] -- Bounding box orientation as quaternion: w, x, y, z.
   "num_lidar_pts":           <int> -- Number of lidar points in this box. Points are counted during the lidar sweep identified with this sample.
   "num_radar_pts":           <int> -- Number of radar points in this box. Points are counted during the radar sweep identified with this sample. This number is summed across all radar sensors without any invalid point filtering.
   "next":                    <str> -- Foreign key. Sample annotation from the same object instance that follows this in time. Empty if this is the last annotation for this object.
   "prev":                    <str> -- Foreign key. Sample annotation from the same object instance that precedes this in time. Empty if this is the first annotation for this object.
}
sample_data
A sensor data e.g. image, point cloud or radar return. For sample_data with is_key_frame=True, the time-stamps should be very close to the sample it points to. For non key-frames the sample_data points to the sample that follows closest in time.

sample_data {
   "token":                   <str> -- Unique record identifier.
   "sample_token":            <str> -- Foreign key. Sample to which this sample_data is associated.
   "ego_pose_token":          <str> -- Foreign key.
   "calibrated_sensor_token": <str> -- Foreign key.
   "filename":                <str> -- Relative path to data-blob on disk.
   "fileformat":              <str> -- Data file format.
   "width":                   <int> -- If the sample data is an image, this is the image width in pixels.
   "height":                  <int> -- If the sample data is an image, this is the image height in pixels.
   "timestamp":               <int> -- Unix time stamp.
   "is_key_frame":            <bool> -- True if sample_data is part of key_frame, else False.
   "next":                    <str> -- Foreign key. Sample data from the same sensor that follows this in time. Empty if end of scene.
   "prev":                    <str> -- Foreign key. Sample data from the same sensor that precedes this in time. Empty if start of scene.
}
scene
A scene is a 20s long sequence of consecutive frames extracted from a log. Multiple scenes can come from the same log. Note that object identities (instance tokens) are not preserved across scenes.

scene {
   "token":                   <str> -- Unique record identifier.
   "name":                    <str> -- Short string identifier.
   "description":             <str> -- Longer description of the scene.
   "log_token":               <str> -- Foreign key. Points to log from where the data was extracted.
   "nbr_samples":             <int> -- Number of samples in this scene.
   "first_sample_token":      <str> -- Foreign key. Points to the first sample in scene.
   "last_sample_token":       <str> -- Foreign key. Points to the last sample in scene.
}
sensor
A specific sensor type.

sensor {
   "token":                   <str> -- Unique record identifier.
   "channel":                 <str> -- Sensor channel name.
   "modality":                <str> {camera, lidar, radar} -- Sensor modality. Supports category(ies) in brackets.
}
visibility
The visibility of an instance is the fraction of annotation visible in all 6 images. Binned into 4 bins 0-40%, 40-60%, 60-80% and 80-100%.

visibility {
   "token":                   <str> -- Unique record identifier.
   "level":                   <str> -- Visibility level.
   "description":             <str> -- Description of visibility level.
}
Show more ‚Üì
Data annotation
After collecting the driving data, we sample well synchronized keyframes (image, LIDAR, RADAR) at 2Hz and send them to our annotation partner Scale for annotation. Using expert annotators and multiple validation steps, we achieve highly accurate annotations. All objects in the nuScenes dataset come with a semantic category, as well as a a 3D bounding box and attributes for each frame they occur in. Compared to 2D bounding boxes, this allows us to accurately infer an object‚Äôs position and orientation in space.

We provide ground truth labels for 23 object classes. For a detailed definition of every class and example images, please see the annotator instructions. For the full nuScenes dataset we provide annotations for the following categories (excl. test set):

For nuScenes-lidarseg, we annotate every point in the lidar pointcloud with a semantic label. In addition to the 23 foreground classes (things) from nuScenes, we have included 9 background classes (stuff). For a detailed definition of every class and example images, please see the annotator instructions for nuScenes and nuScenes-lidarseg. We provide annotations for the following categories (excl. test set):

Category  nuScenes cuboids  Cuboid ratio  Lidarseg points Point ratio
animal  787 0.07% 5,385 0.01%
human.pedestrian.adult  208,240 17.86%  2,156,470 2.73%
human.pedestrian.child  2,066 0.18% 9,655 0.01%
human.pedestrian.construction_worker  9,161 0.79% 139,443 0.18%
human.pedestrian.personal_mobility  395 0.03% 8,723 0.01%
human.pedestrian.police_officer 727 0.06% 9,159 0.01%
human.pedestrian.stroller 1,072 0.09% 8,809 0.01%
human.pedestrian.wheelchair 503 0.04% 12,168  0.02%
movable_object.barrier  152,087 13.04%  9,305,106 11.79%
movable_object.debris 3,016 0.26% 66,861  0.08%
movable_object.pushable_pullable  24,605  2.11% 718,641 0.91%
movable_object.trafficcone  97,959  8.40% 736,239 0.93%
static_object.bicycle_rack *  2,713 0.23% 163,126 0.21%
vehicle.bicycle 11,859  1.02% 141,351 0.18%
vehicle.bus.bendy 1,820 0.16% 357,463 0.45%
vehicle.bus.rigid 14,501  1.24% 4,247,297 5.38%
vehicle.car 493,322 42.30%  38,104,219  48.27%
vehicle.construction  14,671  1.26% 1,514,414 1.92%
vehicle.emergency.ambulance 49  0.00% 2,218 0.00%
vehicle.emergency.police  638 0.05% 59,590  0.08%
vehicle.motorcycle  12,617  1.08% 427,391 0.54%
vehicle.trailer 24,860  2.13% 4,907,511 6.22%
vehicle.truck 88,519  7.59% 15,841,384  20.07%
Total 1,166,187 100.00% 78,942,623  100.00%
flat.driveable_surface  - - 316,958,899 28.64%
flat.other  - - 8,559,216 0.77%
flat.sidewalk - - 70,197,461  6.34%
flat.terrain  - - 70,289,730  6.35%
static.manmade  - - 178,178,063 16.10%
static.other  - - 817,150 0.07%
static.vegetation - - 122,581,273 11.08%
vehicle.ego - - 337,070,621 30.46%
noise - - 2,061,156 0.19%
Total - - 1,106,713,569 100.00%
* Note that the static_object.bicycle_rack category can include bicycles that are not annotated individually. We use it to ignore large groups of shared bicycles during training to avoid biasing our object detector towards these less interesting bicycles.

Furthermore certain classes in nuScenes have special attributes:

Attribute Annotations
vehicle.moving  149,203
vehicle.stopped 65,975
vehicle.parked  420,226
cycle.with_rider  7,331
cycle.without_rider 17,345
pedestrian.sitting_lying_down 13,939
pedestrian.standing 46,530
pedestrian.moving 157,444
Total 877,993
Show more ‚Üì
Tutorials
We provide a number of tutorials for nuScenes as interactive Jupyter Notebooks in the devkit. The tutorials are shown here as static pages for users that do not want to download the dataset. These tutorials cover the basic usage of nuScenes, nuScenes-lidarseg, the map and CAN bus expansions, as well as the prediction challenge. Use the dropdown menu below to select the tutorial you want to view.
Alternatively, you can run the tutorials interactively on Colab:  Open In Colab

Select tutorial
Show more ‚Üì
Explore
On this page we provide tools to preview some of the scenes of the nuScenes dataset without having to download the entire dataset.

Usage
Below you see the LIDAR point cloud of an example scene in nuScenes. To navigate use the SW keys to move backward and forward, AD keys to move left and right, QE keys to move up and down, the arrow keys for rotation and +- to move forward and backward in time between keyframes. You can also use the slider at the bottom to jump to different keyframes. Loading a scene may take between 5-20s. To link to a particular scene or frame use the link https://www.nuscenes.org/nuscenes?sceneId=scene-0011&frame=0, where scene-0011 identifies the scene and 0 is the first frame. Your browser must support WebGL. Please contact nuScenes@motional.com if you are experiencing technical problems. Note that the tool may crash on some versions of Chrome.

View
lidar
Scenes
scene-0011


Show more ‚Üì
Panoptic
In the first nuScenes release, bounding boxes or cuboids are used to represent 3D objects. While useful in many cases, cuboids lack the ability to capture fine shape details of articulated objects. Panoptic nuScenes has higher levels of granularity by containing annotations for every single lidar point in the 40,000 keyframes of the nuScenes dataset. An astonishing 1,400,000,000 lidar points are each annotated with a semantic label, an instance label, and a track label.

The semantic label can be one of 32 classes. In addition to the 23 foreground classes (things) from nuScenes, we have included 9 background classes (stuff). For a detailed definition of every class and example images, please see the annotator instructions here and here.

The taxonomy of Panoptic nuScenes is compatible with the rest of nuScenes and nuImages, thus enabling a wide range of research across multiple sensor modalities. This is a major step forward for industry and academia alike, as it allows researchers to study and quantify novel problems such as lidar point cloud segmentation, foreground extraction, sensor calibration and mapping using point-level semantics. In the future, we plan to organize various public challenges around these tasks.

Panoptic nuScenes is standing on the shoulders of giants. The academic SemanticKITTI dataset annotates the famous KITTI dataset with lidar segmentation labels for 28 classes. KITTI primarily consists of suburban streets with low traffic density and less challenging traffic situations. Its annotations only cover the front camera, rather than the entire 360 degree view. Furthermore it does not contain radar and is strictly for non-commercial use. nuScenes set out to improve on these aspects, featuring dense data from urban and suburban scenes in Singapore and Boston. It is a multimodal dataset that covers the entire 360 degree view and can be licensed by commercial entities. Following the launch of Panoptic nuScenes (of which the lidar semantic segmentation annotations were initially released as nuScenes-lidarseg in October 2019), we have seen a number of other lidar segmentation datasets emerge, such as Hesai's Pandaset and we are looking forward to more companies sharing their data with the community.

Just like nuScenes, the Panoptic nuScenes annotations are available as free to use strictly for non-commercial purposes. Non-commercial means not primarily intended for or directed towards commercial advantage or monetary compensation. Examples of non-commercial use include but are not limited to personal use, educational use, such as in schools, academies, universities etc., and some research use. If you intend to use the nuScenes dataset for commercial purposes, we encourage you to contact us for commercial licensing options by sending an email to nuScenes@motional.com.

We hope that this dataset will allow researchers across the world to go even further in the quest to develop safe autonomous driving technology.

chart
Show more ‚Üì
Motional logoDownloads
Here we list the different releases of the nuScenes datasets. You can be a registered user to view and download them. 

Please register and login to download the nuScenes dataset free of charge for non-commercial use, so that we can better track its usage.

This dataset is also available at Registry of Open Data on AWS.

Sign Up
Login
Code
The devkit for the nuScenes dataset can be found here on our GitHub repo.

Copyright ¬© 2020 nuScenes
All Rights Reserved
Contact Us : nuScenes@motional.com
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
The Role of Validation Data
Validation Data vs. Training and Test Data
Real-World Examples
Cross-Validation
Glossary
Validation Data
Optimize machine learning models with validation data to prevent overfitting, tune hyperparameters, and ensure robust, real-world performance.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
Validation data is a sample of data held back from the training process that is used to provide an unbiased evaluation of a model's fit while tuning its hyperparameters. The primary role of the validation set is to guide the development of a machine learning (ML) model by offering a frequent, independent assessment of its performance. This feedback loop is essential for building models that not only perform well on the data they have seen but also generalize effectively to new, unseen data, a concept central to creating robust Artificial Intelligence (AI) systems.

The Role of Validation Data
The main purpose of validation data is to prevent overfitting. Overfitting occurs when a model learns the training data too well, capturing noise and details that do not apply to new data, thereby hurting its performance. By testing the model against the validation set at regular intervals (e.g., after each epoch), developers can monitor its generalization error. If performance on the training data continues to improve while performance on the validation data stagnates or degrades, it's a clear sign of overfitting.

This evaluation process is crucial for hyperparameter tuning. Hyperparameters are configuration settings external to the model, such as the learning rate or batch size, which are not learned from the data. The validation set allows for experimenting with different hyperparameter combinations to find the set that yields the best performance. This iterative process is a core part of model selection and optimization.

Validation Data vs. Training and Test Data
In a typical ML project, the dataset is split into three subsets, and understanding their distinct roles is fundamental. A common approach to data splitting is to allocate 70% for training, 15% for validation, and 15% for testing.

Training Data: This is the largest portion of the data, used to teach the model. The model iteratively learns patterns, features, and relationships from this dataset by adjusting its internal model weights.
Validation Data: This separate subset is used to provide an unbiased evaluation during the training process. It helps tune hyperparameters and make key decisions, such as when to implement early stopping to prevent overfitting. In the Ultralytics ecosystem, this evaluation is handled in the validation mode.
Test Data: This dataset is held out until the model is fully trained and tuned. It is used only once to provide a final, unbiased assessment of the model's performance. The test set's performance indicates how the model is expected to perform in a real-world deployment scenario.
Maintaining a strict separation, especially between the validation and test sets, is critical for accurately assessing a model's capabilities and avoiding the bias-variance tradeoff.

Real-World Examples
Computer Vision Object Detection: When training an Ultralytics YOLO model for detecting objects in images (e.g., using the VisDrone dataset), a portion of the labeled images is set aside as validation data. During training, the model's mAP (mean Average Precision) is calculated on this validation set after each epoch. This validation mAP helps decide when to stop training or which set of data augmentation techniques works best, before a final performance check on the test set. Effective model evaluation strategies rely heavily on this split.
Natural Language Processing Text Classification: In developing a model to classify customer reviews as positive or negative (sentiment analysis), a validation set is used to choose the optimal architecture (e.g., LSTM vs. Transformer) or tune hyperparameters like dropout rates. The model achieving the highest F1-score or accuracy on the validation set would be selected for final testing. Resources like Hugging Face Datasets often provide datasets pre-split for this purpose.
Cross-Validation
When the amount of available data is limited, a technique called Cross-Validation (specifically K-Fold Cross-Validation) is often employed. Here, the training data is split into 'K' subsets (folds). The model is trained K times, each time using K-1 folds for training and the remaining fold as the validation set. The performance is then averaged across all K runs. This provides a more robust estimate of model performance and makes better use of limited data, as explained in resources like the scikit-learn documentation and the Ultralytics K-Fold Cross-Validation guide.

In summary, validation data is a cornerstone of building reliable and high-performing AI models with frameworks like PyTorch and TensorFlow. It enables effective hyperparameter tuning, model selection, and overfitting prevention, ensuring that models generalize well beyond the data they were trained on. Platforms like Ultralytics HUB offer integrated tools for managing these datasets effectively.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar

Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
Hyperparameter Tuning vs. Related Concepts
Common Tuning Methods and Hyperparameters
Real-World Applications
Hyperparameter Tuning with Ultralytics
Glossary
Hyperparameter Tuning
Master hyperparameter tuning to optimize ML models like Ultralytics YOLO. Boost accuracy, speed, and performance with expert techniques.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Hyperparameter tuning is the process of finding the optimal configuration settings for a Machine Learning (ML) model. These settings, known as hyperparameters, are external to the model and cannot be learned directly from the data during the training process. Instead, they are set before training begins and control how the training process itself behaves. Effectively tuning these hyperparameters is a critical step in maximizing model performance and ensuring it generalizes well to new, unseen data. Without proper tuning, even the most advanced model architecture can underperform.

Hyperparameter Tuning vs. Related Concepts
It's important to differentiate hyperparameter tuning from other key concepts in ML:

Optimization Algorithm: An optimization algorithm, like Adam or Stochastic Gradient Descent (SGD), is the engine that adjusts the model's internal parameters (weights and biases) during training to minimize the loss function. Hyperparameter tuning, in contrast, involves selecting the best external settings, which can even include the choice of the optimization algorithm itself.
Neural Architecture Search (NAS): While hyperparameter tuning optimizes the settings for a given model structure, NAS automates the design of the model architecture itself, such as determining the number and type of layers. Both are forms of Automated Machine Learning (AutoML) and are often used together to build the best possible model.
Model Parameters: These are the internal variables of a model, such as the weights and biases in a neural network, that are learned from the training data through backpropagation. Hyperparameters are the higher-level settings that govern how these parameters are learned.
Common Tuning Methods and Hyperparameters
Practitioners use several strategies to find the best hyperparameter values. Common methods include Grid Search, which exhaustively tries every combination of specified values, Random Search, which samples combinations randomly, and more advanced methods like Bayesian Optimization and Evolutionary Algorithms.

Some of the most frequently tuned hyperparameters include:

Learning Rate: Controls how much the model's weights are adjusted with respect to the loss gradient.
Batch Size: The number of training examples utilized in one iteration.
Number of Epochs: The number of times the entire training dataset is passed through the model.
Data Augmentation Intensity: The degree of transformations applied to the training data, such as rotation, scaling, or color shifts. The Albumentations library is a popular tool for this.
Real-World Applications
Hyperparameter tuning is applied across various domains to achieve peak performance:

Medical Image Analysis: When training a model for tumor detection, tuning hyperparameters like the learning rate schedule, data augmentation settings, and loss function weights is crucial for achieving high detection accuracy on specific medical datasets. This is vital for reliable AI in Healthcare solutions and is a subject of ongoing research.
Autonomous Vehicles: Object detection models in self-driving cars require careful tuning. Optimizing hyperparameters such as input image resolution, Non-Maximum Suppression (NMS) thresholds, and anchor box configurations ensures the system can reliably detect pedestrians and obstacles with low latency for safe navigation. This tuning is critical for companies like Waymo and contributes to robust AI in Automotive solutions.
Hyperparameter Tuning with Ultralytics
Ultralytics provides tools to simplify hyperparameter tuning for Ultralytics YOLO models. The Ultralytics Tuner class, documented in the Hyperparameter Tuning guide, automates the process using evolutionary algorithms. Integration with platforms like Ray Tune offers further capabilities for distributed and advanced search strategies, helping users optimize their models efficiently for specific datasets (like COCO) and tasks. Users can leverage platforms like Ultralytics HUB for streamlined experiment tracking and management, which is often a key part of following best practices for model training. Popular open-source libraries like Optuna and Hyperopt are also widely used in the ML community for this purpose.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
The Importance of an Optimal Learning Rate
Learning Rate Schedulers
Learning Rate vs. Related Concepts
Real-World Applications
Glossary
Learning Rate
Master the art of setting optimal learning rates in AI! Learn how this crucial hyperparameter impacts model training and performance.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
The learning rate is a critical hyperparameter in the training of neural networks and other machine learning models. It controls the size of the adjustments made to the model's internal parameters, or weights, during each step of the training process. Essentially, it determines how quickly the model learns from the data. The optimization algorithm uses the learning rate to scale the gradient of the loss function, guiding the model toward a set of optimal weights that minimizes error.

The Importance of an Optimal Learning Rate
Choosing an appropriate learning rate is fundamental to successful model training. The value has a significant impact on both the speed of convergence and the final performance of the model.

Learning Rate Too High: If the learning rate is set too high, the model's weight updates can be too large. This may cause the training process to become unstable, with the loss fluctuating wildly and failing to decrease. In the worst case, the algorithm might continuously "overshoot" the optimal solution in the loss landscape, leading to divergence where the model's performance gets progressively worse.
Learning Rate Too Low: A learning rate that is too small will result in extremely slow training, as the model takes tiny steps toward the solution. This increases the computational cost and time required. Furthermore, a very low learning rate can cause the training process to get stuck in a poor local minimum, preventing the model from finding a more optimal set of weights and leading to underfitting.
Finding the right balance is key to training an effective model efficiently. A well-chosen learning rate allows the model to converge smoothly and quickly to a good solution.

Learning Rate Schedulers
Instead of using a single, fixed learning rate throughout training, it is often beneficial to vary it dynamically. This is achieved using learning rate schedulers. A common strategy is to start with a relatively high learning rate to make rapid progress early in the training process and then gradually decrease it. This allows the model to make finer adjustments as it gets closer to a solution, helping it settle into a deep and stable minimum in the loss landscape. Popular scheduling techniques include step decay, exponential decay, and more advanced methods like cyclical learning rates, which can help escape saddle points and poor local minima. Frameworks like PyTorch provide extensive options for scheduling.

Learning Rate vs. Related Concepts
It's helpful to differentiate the learning rate from other related terms:

Optimization Algorithm: The optimization algorithm, such as Adam or Stochastic Gradient Descent (SGD), is the mechanism that applies the updates to the model's weights. The learning rate is a parameter that this algorithm uses to determine the magnitude of those updates. While adaptive optimizers like Adam adjust the step size for each parameter individually, they still rely on a base learning rate.
Hyperparameter Tuning: The learning rate is one of the most important settings configured before training begins, making its selection a central part of hyperparameter tuning. This process involves finding the best combination of external parameters (like learning rate, batch size, etc.) to maximize model performance. Tools like the Ultralytics Tuner class and frameworks like Ray Tune can automate this search.
Batch Size: The learning rate and batch size are closely related. Training with a larger batch size often allows for the use of a higher learning rate, as the gradient estimate is more stable. The interplay between these two hyperparameters is a key consideration during model optimization, as documented in various research studies.
Real-World Applications
Selecting an appropriate learning rate is critical across various AI applications, directly influencing model accuracy and usability:

Medical Image Analysis: In tasks like tumor detection in medical imaging using models trained on datasets such as the CheXpert dataset, tuning the learning rate is crucial. A well-chosen learning rate ensures the model learns subtle features indicative of tumors without becoming unstable or failing to converge, directly impacting diagnostic accuracy. This is a key aspect of developing reliable AI in healthcare solutions.
Autonomous Vehicles: For object detection systems in self-driving cars, the learning rate affects how quickly and reliably the model learns to identify pedestrians, cyclists, and other vehicles from sensor data (e.g., from the nuScenes dataset). An optimal learning rate helps achieve the high real-time inference performance and reliability needed for safe navigation, a core challenge in AI in Automotive.
Finding the right learning rate is often an iterative process, guided by best practices for model training and empirical results. Platforms like Ultralytics HUB can help manage these experiments, ensuring the AI model learns effectively and achieves its performance goals.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar



WikipediaThe Free Encyclopedia
Search Wikipedia
Search
Donate
Create account
Log in


Photograph a historic site, help Wikipedia, and win a prize. Participate in the world's largest photography competition this month!
Learn more

Contents hide
(Top)
Approaches

Grid search
Random search
Bayesian optimization
Gradient-based optimization
Evolutionary optimization
Population-based
Early stopping-based
Others
Issues with hyperparameter optimization
See also
References
Hyperparameter optimization

Article
Talk
Read
Edit
View history

Tools
Appearance hide
Text

Small

Standard

Large
Width

Standard

Wide
Color (beta)

Automatic

Light

Dark
From Wikipedia, the free encyclopedia
In machine learning, hyperparameter optimization[1] or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts.[2][3]

Hyperparameter optimization determines the set of hyperparameters that yields an optimal model which minimizes a predefined loss function on a given data set.[4] The objective function takes a set of hyperparameters and returns the associated loss.[4] Cross-validation is often used to estimate this generalization performance, and therefore choose the set of values for hyperparameters that maximize it.[5]

Approaches

Grid search across different values of two hyperparameters. For each hyperparameter, 10 different values are considered, so a total of 100 different combinations are evaluated and compared. Blue contours indicate regions with strong results, whereas red ones show regions with poor results.
Grid search
The traditional method for hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set[6] or evaluation on a hold-out validation set.[7]

Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.

For example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant C and a kernel hyperparameter Œ≥. Both parameters are continuous, so to perform grid search, one selects a finite set of "reasonable" values for each, say

C
‚àà
{
10
,
100
,
1000
}
{\displaystyle C\in \{10,100,1000\}}
Œ≥
‚àà
{
0.1
,
0.2
,
0.5
,
1.0
}
{\displaystyle \gamma \in \{0.1,0.2,0.5,1.0\}}
Grid search then trains an SVM with each pair (C, Œ≥) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.

Grid search suffers from the curse of dimensionality, but is often embarrassingly parallel because the hyperparameter settings it evaluates are typically independent of each other.[5]


Random search across different combinations of values for two hyperparameters. In this example, 100 different random choices are evaluated. The green bars show that more individual values for each hyperparameter are considered compared to a grid search.
Random search
Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. A benefit over grid search is that random search can explore many more values than grid search could for continuous hyperparameters. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm.[5] In this case, the optimization problem is said to have a low intrinsic dimensionality.[8] Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample. Despite its simplicity, random search remains one of the important base-lines against which to compare the performance of new hyperparameter optimization methods.


Methods such as Bayesian optimization smartly explore the space of potential choices of hyperparameters by deciding which combination to explore next based on previous observations.
Bayesian optimization
Main article: Bayesian optimization
Bayesian optimization is a global optimization method for noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown[9][10][11][12][13] to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.

Gradient-based optimization
For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks.[14] Since then, these methods have been extended to other models such as support vector machines[15] or logistic regression.[16]

A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation.[17][18][19][20] A more recent work along this direction uses the implicit function theorem to calculate hypergradients and proposes a stable approximation of the inverse Hessian. The method scales to millions of hyperparameters and requires constant memory.[21]

In a different approach,[22] a hypernetwork is trained to approximate the best response function. One of the advantages of this method is that it can handle discrete hyperparameters as well. Self-tuning networks[23] offer a memory efficient version of this approach by choosing a compact representation for the hypernetwork. More recently, Œî-STN[24] has improved this method further by a slight reparameterization of the hypernetwork which speeds up training. Œî-STN also yields a better approximation of the best-response Jacobian by linearizing the network in the weights, hence removing unnecessary nonlinear effects of large changes in the weights.

Apart from hypernetwork approaches, gradient-based methods can be used to optimize discrete hyperparameters also by adopting a continuous relaxation of the parameters.[25] Such methods have been extensively used for the optimization of architecture hyperparameters in neural architecture search.

Evolutionary optimization
Main article: Evolutionary algorithm
Evolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm.[10] Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution:

Create an initial population of random solutions (i.e., randomly generate tuples of hyperparameters, typically 100+)
Evaluate the hyperparameter tuples and acquire their fitness function (e.g., 10-fold cross-validation accuracy of the machine learning algorithm with those hyperparameters)
Rank the hyperparameter tuples by their relative fitness
Replace the worst-performing hyperparameter tuples with new ones generated via crossover and mutation
Repeat steps 2-4 until satisfactory algorithm performance is reached or is no longer improving.
Evolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms,[10] automated machine learning, typical neural network [26] and deep neural network architecture search,[27][28] as well as training of the weights in deep neural networks.[29]

Population-based
Population Based Training (PBT) learns both hyperparameter values and network weights. Multiple learning processes operate independently, using different hyperparameters. As with evolutionary methods, poorly performing models are iteratively replaced with models that adopt modified hyperparameter values and weights based on the better performers. This replacement model warm starting is the primary differentiator between PBT and other evolutionary methods. PBT thus allows the hyperparameters to evolve and eliminates the need for manual hypertuning. The process makes no assumptions regarding model architecture, loss functions or training procedures.

PBT and its variants are adaptive methods: they update hyperparameters during the training of the models. On the contrary, non-adaptive methods have the sub-optimal strategy to assign a constant set of hyperparameters for the whole training.[30]

Early stopping-based

Successive halving for eight arbitrary hyperparameter configurations. The approach starts with eight models with different configurations and consecutively applies successive halving until only one model remains.
A class of early stopping-based hyperparameter optimization algorithms is purpose built for large search spaces of continuous and discrete hyperparameters, particularly when the computational cost to evaluate the performance of a set of hyperparameters is high. Irace implements the iterated racing algorithm, that focuses the search around the most promising configurations, using statistical tests to discard the ones that perform poorly.[31][32] Another early stopping hyperparameter optimization algorithm is successive halving (SHA),[33] which begins as a random search but periodically prunes low-performing models, thereby focusing computational resources on more promising models. Asynchronous successive halving (ASHA)[34] further improves upon SHA's resource utilization profile by removing the need to synchronously evaluate and prune low-performing models. Hyperband[35] is a higher level early stopping-based algorithm that invokes SHA or ASHA multiple times with varying levels of pruning aggressiveness, in order to be more widely applicable and with fewer required inputs.

Others
RBF[36] and spectral[37] approaches have also been developed.

Issues with hyperparameter optimization
When hyperparameter optimization is done, the set of hyperparameters are often fitted on a training set and selected based on the generalization performance, or score, of a validation set. However, this procedure is at risk of overfitting the hyperparameters to the validation set. Therefore, the generalization performance score of the validation set (which can be several sets in the case of a cross-validation procedure) cannot be used to simultaneously estimate the generalization performance of the final model. In order to do so, the generalization performance has to be evaluated on a set independent (which has no intersection) of the set (or sets) used for the optimization of the hyperparameters, otherwise the performance might give a value which is too optimistic (too large). This can be done on a second test set, or through an outer cross-validation procedure called nested cross-validation, which allows an unbiased estimation of the generalization performance of the model, taking into account the bias due to the hyperparameter optimization.

See also
Automated machine learning
Neural architecture search
Meta-optimization
Model selection
Self-tuning
XGBoost
Optuna
References
 Matthias Feurer and Frank Hutter. Hyperparameter optimization. In: AutoML: Methods, Systems, Challenges, pages 3‚Äì38.
 Yang, Li (2020). "On hyperparameter optimization of machine learning algorithms: Theory and practice". Neurocomputing. 415: 295‚Äì316. arXiv:2007.15745. doi:10.1016/j.neucom.2020.07.061.
 Franceschi L, Donini M, Perrone V, Klein A, Archambeau C, Seeger M, Pontil M, Frasconi P (2024). "Hyperparameter Optimization in Machine Learning". arXiv:2410.22854 [stat.ML].
 Claesen, Marc; Bart De Moor (2015). "Hyperparameter Search in Machine Learning". arXiv:1502.02127 [cs.LG].
 Bergstra, James; Bengio, Yoshua (2012). "Random Search for Hyper-Parameter Optimization" (PDF). Journal of Machine Learning Research. 13: 281‚Äì305.
 Chin-Wei Hsu, Chih-Chung Chang and Chih-Jen Lin (2010). A practical guide to support vector classification. Technical Report, National Taiwan University.
 Chicco D (December 2017). "Ten quick tips for machine learning in computational biology". BioData Mining. 10 (35) 35. doi:10.1186/s13040-017-0155-3. PMC 5721660. PMID 29234465.
 Ziyu, Wang; Frank, Hutter; Masrour, Zoghi; David, Matheson; Nando, de Feitas (2016). "Bayesian Optimization in a Billion Dimensions via Random Embeddings". Journal of Artificial Intelligence Research. 55: 361‚Äì387. arXiv:1301.1942. doi:10.1613/jair.4806. S2CID 279236.
 Hutter, Frank; Hoos, Holger; Leyton-Brown, Kevin (2011), "Sequential Model-Based Optimization for General Algorithm Configuration", Learning and Intelligent Optimization (PDF), Lecture Notes in Computer Science, vol. 6683, pp. 507‚Äì523, CiteSeerX 10.1.1.307.8813, doi:10.1007/978-3-642-25566-3_40, ISBN 978-3-642-25565-6, S2CID 6944647
 Bergstra, James; Bardenet, Remi; Bengio, Yoshua; Kegl, Balazs (2011), "Algorithms for hyper-parameter optimization" (PDF), Advances in Neural Information Processing Systems
 Snoek, Jasper; Larochelle, Hugo; Adams, Ryan (2012). "Practical Bayesian Optimization of Machine Learning Algorithms" (PDF). Advances in Neural Information Processing Systems. arXiv:1206.2944. Bibcode:2012arXiv1206.2944S.
 Thornton, Chris; Hutter, Frank; Hoos, Holger; Leyton-Brown, Kevin (2013). "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms" (PDF). Knowledge Discovery and Data Mining. arXiv:1208.3719. Bibcode:2012arXiv1208.3719T.
 Kernc (2024), SAMBO: Sequential And Model-Based Optimization: Efficient global optimization in Python, doi:10.5281/zenodo.14461363, retrieved 2025-01-30
 Larsen, Jan; Hansen, Lars Kai; Svarer, Claus; Ohlsson, M (1996). "Design and regularization of neural networks: The optimal use of a validation set" (PDF). Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop. pp. 62‚Äì71. CiteSeerX 10.1.1.415.3266. doi:10.1109/NNSP.1996.548336. ISBN 0-7803-3550-3. S2CID 238874.
 Olivier Chapelle; Vladimir Vapnik; Olivier Bousquet; Sayan Mukherjee (2002). "Choosing multiple parameters for support vector machines" (PDF). Machine Learning. 46 (1‚Äì3): 131‚Äì159. doi:10.1023/a:1012450327387.
 Chuong B; Chuan-Sheng Foo; Andrew Y Ng (2008). "Efficient multiple hyperparameter learning for log-linear models" (PDF). Advances in Neural Information Processing Systems. 20.
 Domke, Justin (2012). "Generic Methods for Optimization-Based Modeling" (PDF). Aistats. 22. Archived from the original (PDF) on 2014-01-24. Retrieved 2017-12-09.
 Maclaurin, Dougal; Duvenaud, David; Adams, Ryan P. (2015). "Gradient-based Hyperparameter Optimization through Reversible Learning". arXiv:1502.03492 [stat.ML].
 Franceschi, Luca; Donini, Michele; Frasconi, Paolo; Pontil, Massimiliano (2017). "Forward and Reverse Gradient-Based Hyperparameter Optimization" (PDF). Proceedings of the 34th International Conference on Machine Learning. arXiv:1703.01785. Bibcode:2017arXiv170301785F.
 Shaban, Amirreza; Cheng, Ching-An; Hatch, Nathan; Boots, Byron (2018). "Truncated Back-propagation for Bilevel Optimization". arXiv:1810.10667 [cs.LG].
 Lorraine, Jonathan; Vicol, Paul; Duvenaud, David (2019). "Optimizing Millions of Hyperparameters by Implicit Differentiation". arXiv:1911.02590 [cs.LG].
 Lorraine, Jonathan; Duvenaud, David (2018). "Stochastic Hyperparameter Optimization through Hypernetworks". arXiv:1802.09419 [cs.LG].
 MacKay, Matthew; Vicol, Paul; Lorraine, Jon; Duvenaud, David; Grosse, Roger (2019). "Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions". arXiv:1903.03088 [cs.LG].
 Bae, Juhan; Grosse, Roger (2020). "Delta-STN: Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians". arXiv:2010.13514 [cs.LG].
 Liu, Hanxiao; Simonyan, Karen; Yang, Yiming (2018). "DARTS: Differentiable Architecture Search". arXiv:1806.09055 [cs.LG].
 Kousiouris G, Cuccinotta T, Varvarigou T (2011). "The effects of scheduling, workload type and consolidation scenarios on virtual machine performance and their prediction through optimized artificial neural networks". Journal of Systems and Software. 84 (8): 1270‚Äì1291. doi:10.1016/j.jss.2011.04.013. hdl:11382/361472.
 Miikkulainen R, Liang J, Meyerson E, Rawal A, Fink D, Francon O, Raju B, Shahrzad H, Navruzyan A, Duffy N, Hodjat B (2017). "Evolving Deep Neural Networks". arXiv:1703.00548 [cs.NE].
 Jaderberg M, Dalibard V, Osindero S, Czarnecki WM, Donahue J, Razavi A, Vinyals O, Green T, Dunning I, Simonyan K, Fernando C, Kavukcuoglu K (2017). "Population Based Training of Neural Networks". arXiv:1711.09846 [cs.LG].
 Such FP, Madhavan V, Conti E, Lehman J, Stanley KO, Clune J (2017). "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning". arXiv:1712.06567 [cs.NE].
 Li, Ang; Spyra, Ola; Perel, Sagi; Dalibard, Valentin; Jaderberg, Max; Gu, Chenjie; Budden, David; Harley, Tim; Gupta, Pramod (2019-02-05). "A Generalized Framework for Population Based Training". arXiv:1902.01894 [cs.AI].
 L√≥pez-Ib√°√±ez, Manuel; Dubois-Lacoste, J√©r√©mie; P√©rez C√°ceres, Leslie; St√ºtzle, Thomas; Birattari, Mauro (2016). "The irace package: Iterated Racing for Automatic Algorithm Configuration". Operations Research Perspective. 3 (3): 43‚Äì58. doi:10.1016/j.orp.2016.09.002. hdl:10419/178265.
 Birattari, Mauro; St√ºtzle, Thomas; Paquete, Luis; Varrentrapp, Klaus (2002). "A Racing Algorithm for Configuring Metaheuristics". Gecco 2002: 11‚Äì18.
 Jamieson, Kevin; Talwalkar, Ameet (2015-02-27). "Non-stochastic Best Arm Identification and Hyperparameter Optimization". arXiv:1502.07943 [cs.LG].
 Li, Liam; Jamieson, Kevin; Rostamizadeh, Afshin; Gonina, Ekaterina; Hardt, Moritz; Recht, Benjamin; Talwalkar, Ameet (2020-03-16). "A System for Massively Parallel Hyperparameter Tuning". arXiv:1810.05934v5 [cs.LG].
 Li, Lisha; Jamieson, Kevin; DeSalvo, Giulia; Rostamizadeh, Afshin; Talwalkar, Ameet (2020-03-16). "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization". Journal of Machine Learning Research. 18: 1‚Äì52. arXiv:1603.06560.
 Diaz, Gonzalo; Fokoue, Achille; Nannicini, Giacomo; Samulowitz, Horst (2017). "An effective algorithm for hyperparameter optimization of neural networks". arXiv:1705.08520 [cs.AI].
 Hazan, Elad; Klivans, Adam; Yuan, Yang (2017). "Hyperparameter Optimization: A Spectral Approach". arXiv:1706.00764 [cs.LG].
vte
Differentiable computing
General 
Differentiable programmingInformation geometryStatistical manifoldAutomatic differentiationNeuromorphic computingPattern recognitionRicci calculusComputational learning theoryInductive bias
Hardware  
IPUTPUVPUMemristorSpiNNaker
Software libraries  
TensorFlowPyTorchKerasscikit-learnTheanoJAXFlux.jlMindSpore
 Portals Computer programmingTechnology
Categories: Machine learningMathematical optimizationModel selection
This page was last edited on 5 September 2025, at 06:52 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia¬Æ is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view
Wikimedia Foundation
Powered by MediaWiki

Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
The Role of Test Data in the ML Lifecycle
Real-World Applications
Glossary
Test Data
Discover the importance of test data in AI, its role in evaluating model performance, detecting overfitting, and ensuring real-world reliability.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
In machine learning, Test Data is a separate, independent portion of a dataset that is used for the final evaluation of a model after it has been fully trained and tuned. This dataset acts as a "final exam" for the model, providing an unbiased assessment of its performance on new, unseen data. The core principle is that the model should never learn from or be influenced by the test data during its development. This strict separation ensures that the performance metrics calculated on the test set, such as accuracy or mean Average Precision (mAP), are a true reflection of the model's ability to generalize to real-world scenarios. Rigorous model testing is a critical step before model deployment.

The Role of Test Data in the ML Lifecycle
In a typical Machine Learning (ML) project, data is carefully partitioned to serve different purposes. Understanding the distinction between these partitions is fundamental.

Training Data: This is the largest subset of the data, used to teach the model. The model iteratively learns patterns, features, and relationships by adjusting its internal weights based on the examples in the training set. Effective model creation relies on high-quality training data and following best practices like the ones in this model training tips guide.
Validation Data: This is a separate dataset used during the training process. Its purpose is to provide feedback on the model's performance on unseen data, which helps in hyperparameter tuning (e.g., adjusting the learning rate) and preventing overfitting. It's like a practice test that helps guide the learning strategy. The evaluation is often performed using a dedicated validation mode.
Test Data: This dataset is kept completely isolated until all training and validation are finished. It is used only once to provide a final, unbiased report on the model's performance. Using the test data to make any further adjustments to the model would invalidate the results, a mistake sometimes referred to as "data leakage" or "teaching to the test." This final evaluation is essential for understanding how a model, like an Ultralytics YOLO model, will perform after deployment. Tools like Ultralytics HUB can help manage these datasets throughout the project lifecycle.
While a Benchmark Dataset can serve as a test set, its primary role is to act as a public standard for comparing different models, often used in academic challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). You can see examples of this in model comparison pages.

Real-World Applications
AI in Automotive: A developer creates an object detection model for an autonomous vehicle using thousands of hours of driving footage for training and validation. Before deploying this model into a fleet, it is evaluated against a test dataset. This test set would include challenging, previously unseen scenarios such as driving at night in heavy rain, navigating through a snowstorm, or detecting pedestrians partially obscured by other objects. The model‚Äôs performance on this test set, often using data from benchmarks like nuScenes, determines whether it meets the stringent safety and reliability standards required for AI in automotive applications.
Medical Image Analysis: A computer vision (CV) model is trained to detect signs of pneumonia from chest X-ray images sourced from one hospital. To ensure it is clinically useful, the model must be tested on a dataset of images from a different hospital system. This test data would include images captured with different equipment, from a diverse patient population, and interpreted by different radiologists. Evaluating the model's performance on this external test set is crucial for gaining regulatory approval, such as from the FDA, and confirming its utility for AI in healthcare. This process helps ensure the model avoids dataset bias and performs reliably in new clinical settings.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar



WikipediaThe Free Encyclopedia
Search Wikipedia
Search
Donate
Create account
Log in


Photograph a historic site, help Wikipedia, and win a prize. Participate in the world's largest photography competition this month!
Learn more

Contents hide
(Top)
Overview
Research on generalization
Implications
Fear generalization
Generalization in machine learning
See also
References
Generalization (learning)

Article
Talk
Read
Edit
View history

Tools
Appearance hide
Text

Small

Standard

Large
Width

Standard

Wide
Color (beta)

Automatic

Light

Dark
From Wikipedia, the free encyclopedia
Generalization is the concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar.[1] The learner uses generalized patterns, principles, and other similarities between past experiences and novel experiences to more efficiently navigate the world.[2] For example, if a person has learned in the past that every time they eat an apple, their throat becomes itchy and swollen, they might assume they are allergic to all fruit. When this person is offered a banana to eat, they reject it upon assuming they are also allergic to it through generalizing that all fruits cause the same reaction. Although this generalization about being allergic to all fruit based on experiences with one fruit could be correct in some cases, it may not be correct in all. Both positive and negative effects have been shown in education through learned generalization and its contrasting notion of discrimination learning.

Overview
Generalization is understood to be directly tied to the transfer of knowledge across multiple situations.[3] The knowledge to be transferred is often referred to as abstractions, because the learner abstracts a rule or pattern of characteristics from previous experiences with similar stimuli.[2] Generalization allows humans and animals to recognize the similarities in knowledge acquired in one circumstance, allowing for transfer of knowledge onto new situations. This idea rivals the theory of situated cognition, instead stating that one can apply past knowledge to learning in new situations and environments.

Generalization can be supported and partly explained by the connectionism approach.[4] Just as artificial intelligences learn to distinguish between different categories by applying past learning to novel situations, humans and animals generalize previously learned properties and patterns onto new situations, thus connecting the novel experience to past experiences that are similar in one or more ways. This creates a pattern of connections that allows the learner to classify and make assumptions about the novel stimulus, such as when previous experience with seeing a canary allows the learner to predict what other birds will be like. This categorization is a foundational aspect of generalizing.

Research on generalization
In scientific studies looking at generalization, a generalization gradient is often used. This tool is used to measure how often and how much animals or humans respond to certain stimuli, depending on whether the stimuli are perceived to be similar or different. The curvilinear shape of the gradient is achieved by placing the perceived similarity of a stimulus on the x-axis and the strength of the response on the y-axis.[5] For example, when measuring responses to color, it is expected that subjects will respond to colors that are similar to each other, like shades of pink after being exposed to red, as opposed to a non-similar shade of blue.[6] The gradient is relatively predictable, with the response to similar stimuli being slightly less strong than the response to the conditioned stimulus, then steadily declining as the presented stimuli become increasingly dissimilar.[7]

Several studies have suggested that generalization is a fundamental and naturally occurring learning process for humans. Nine-month-old infants require very few (sometimes only 3) experiences with a category before learning to generalize.[2] In fact, infants generalize so well during early stages of development (such as learning to recognize specific sounds as language) that it can be hard for them to discriminate between variations of the generalized stimuli at later stages of development (such as failing to distinguish between the subtly different sounds of similar phonemes).[2] One potential explanation for why children are such efficient learners is that they operate in accordance with the goal of making their world more predictable, therefore encouraging them to hold strongly to generalizations that effectively help them to navigate their environment.[2]

Some evidence suggests that children are born with innate processes for accurately generalizing things. For example, children tend to generalize based on taxonomic rather than thematic similarities (an experience with one ball leads to the child identifying other ball-shaped objects as ‚Äúball‚Äù rather than labeling a bat as ‚Äúball‚Äù because a bat is used to hit a ball).[2]

Wakefield, Hall, James, and Goldin (2018) found that children are more flexible in generalizing new verbs when they are taught the verb by observing gestures as opposed to being taught by performing the action themselves.[8] When helping a child learn a new word, providing more examples of the word increases the child's capacity to generalize the word to different contexts and situations. Furthermore, writing interventions for grade-school students yield better results when the intervention actively targets generalization as an outcome.[9]

Generalization has been shown to be refined and/or stabilized after sleep.[10]

Implications
Without the ability to generalize, it would likely be very difficult to navigate the world in a useful way.[2] For example, generalization is an important part of how humans learn to trust unfamiliar people[11] and a necessary element in language acquisition.[12]

For a person who lacked the capacity to generalize from one experience to the next, every instance of a dog would be completely separated from other instances of dogs, so prior experience would do nothing to help the person know how to interact with this seemingly new stimulus.[2] In fact, even if the person experienced the very same dog multiple times, he or she would have no way of knowing what to expect and each instance would be as if the individual were encountering a dog for the first time. Therefore, generalization is a valuable and integral part of learning and everyday life.

Generalization is shown to have implications on the use of the spacing effect in educational settings.[13] In the past, it was thought that the information forgotten between periods of learning when implementing spaced presentation inhibited generalization. In more recent years, this forgetting has been seen as promoting generalization through repetition of information during each occasion of spaced learning. The effects of gaining long-term generalization knowledge through spaced learning can be compared with that of massed learning (lengthy and all at once; for example, cramming the night before an exam)[14] in which a person only gains short-term knowledge, decreasing the likelihood of establishing generalization.

Generalization is also considered to be an important factor in procedural memory, such as the near-automatic memory processes necessary for driving a car.[2] Without being able to generalize from previous experiences driving, a person would essentially need to relearn how to drive every time he or she encountered a new street. People who are diagnosed with NVLD - non verbal learning disorder - are known to sometimes have difficulty applying learned concept to new situations.

Not all of generalization's effects are beneficial, however. An important part of learning is knowing when not to generalize, which is called discrimination learning. Were it not for discrimination learning, humans and animals would struggle to respond correctly to different situations.[15] For example, a dog may be trained to come to its owner when it hears a whistle. If the dog generalizes this training, it may not discriminate between the sound of the whistle and other stimuli, so it would come running to its owner when it hears any high-pitched noise.

Fear generalization
A specific type of generalization, fear generalization, occurs when a person associates fears learned in the past through classical conditioning to similar situations, events, people, and objects in their present. This is important for the survival of the organism; humans and animals need to be able to assess aversive situations and respond appropriately based on generalizations made from past experiences.[16]

When fear generalization becomes maladaptive it is connected to many anxiety disorders.[17] This maladaptation is often referred to as the overgeneralization of fear and can also lead to the development of posttraumatic stress disorder.[18] Overgeneralization is hypothetically attributed to ‚Äúdysregulation of prefrontal-amygdalo-hippocampal circuitry‚Äù (Banich, et al., 2010, p. 21).[2]

One of the earliest studies about fear generalization in humans was conducted by Watson and Raynor (1920): the Little Albert experiment. In their study, an infant known as Little Albert was exposed to various kinds of animals, none of which elicited a fear response from Little Albert. However, after 7 pairings of a white rat and the sound of a hammer clanging against a steel bar (which did elicit a fear response), the 11-month old child began to cry and try to get away from the white rat even without the loud noise. Months later, additional trials showed that Little Albert had generalized his fear response to things that were similar to the white rat, including a dog, a rabbit, and a fur coat.[2]

Brain regions involved in fear generalization include the amygdala and the hippocampus.[2] The hippocampus seems to be more involved in the development of context fear generalization (developing a generalized fear for a specific environment) than stimulus fear generalization (such as Little Albert's acquisition of a fear response to white, furry objects). The amygdala, which is associated with all types of emotional responses, is fundamental in developing a classically conditioned fear response to either a stimulus or the context in which it is found.[2]

Generalization in machine learning
This section is an excerpt from Machine learning ¬ß Generalization.[edit]
Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.
See also
Chunking (psychology)
References
 Gluck, Mark A.; Mercado, Eduardo; Myers, Catherine E. (2011). Learning and memory : from brain to behavior (2nd ed.). New York: Worth Publishers. p. 209. ISBN 9781429240147.
 Banich, M. T., Dukes, P., & Caccamise, D. (2010). Generalization of knowledge: Multidisciplinary perspectives. Psychology Press.
 Walker, J. E.; Shea, T. M.; Bauer, A.M. "Generalization and the Effects of Consequences | Education.com". www.education.com. Retrieved 5 May 2016.
 Goldstein, B. E. (2015). Cognitive psychology: connecting mind research and everyday experience. W. Ross MacDonald School Resource Services Library.
 Boddez, Y., Bennett, M. P., van Esch, S., & Beckers, T. (2017). Bending rules: the shape of the perceptual generalisation gradient is sensitive to inference rules. Cognition & Emotion, 31(7), 1444‚Äì1452. https://doi.org/10.1080/02699931.2016.1230541
 Gluck, Mark A.; Mercado, Eduardo; Myers, Catherine E. (2014). Learning and memory : from brain to behavior (2nd ed.). New York: Worth Publishers. p. 212. ISBN 9781429240147.
 The Editors of Encyclopaedia Britannica. (2019, September 30). Generalization. Retrieved from https://www.britannica.com/topic/generalization
 Wakefield, E. M., Hall, C., James, K. H., & Goldin, M. S. (2018). Gesture for generalization: gesture facilitates flexible learning of words for actions on objects. Developmental Science, 21(5), 1. https://doi.org/10.1111/desc.12656
 Hier, B. O., Eckert, T. L., Viney, E. A., & Meisinger, E. (2019). Generalization and maintenance effects of writing fluency intervention strategies for elementary-age students: A randomized controlled trial. School Psychology Review, 48(4), 377. https://doi.org/10.17105/SPR-2017-0123.V48-4
 Fenn, KM; Nusbaum, HC; Margoliash, D (2003). "Consolidation during sleep of perceptual learning of spoken language". Nature. 425 (6958): 614‚Äì6. doi:10.1038/nature01951. PMID 14534586. S2CID 904751.
 FeldmanHall, O., Dunsmoor, J. E., Tompary, A., Hunter, L. E., Todorov, A., & Phelps, E. A. (2018). Stimulus generalization as a mechanism for learning to trust. Proceedings of the National Academy of Sciences, 115(7). doi:10.1073/pnas.1715227115
 Arnon, I.; Clark, E. V., eds. (2011). Experience, variation and generalization: Learning a first language. doi:10.1111/j.1540-4781.2012.01368.x. ISBN 978-9027234773. S2CID 60234960.
 Vlach, Haley A. (September 2014). "The Spacing Effect in Children's Generalization of Knowledge: Allowing Children Time to Forget Promotes Their Ability to Learn". Child Development Perspectives. 8 (3): 163‚Äì168. doi:10.1111/cdep.12079.
 Gluck, Mark A.; Mercado, Eduardo; Myers, Catherine E. (2014). Learning and memory : from brain to behavior (2nd ed.). New York: Worth Publishers. p. 312. ISBN 9781429240147.
 Cherry, K. (2019, November 26). How stimulus generalization influences learning. Retrieved from https://www.verywellmind.com/what-is-stimulus-generalization-2795885
 Asok, A., Kandel, E. R., & Rayman, J. B. (2019). The neurobiology of fear generalization. Frontiers in Behavioral Neuroscience, 12. doi: 10.3389/fnbeh.2018.00329
 Dymond, Simon; Dunsmoor, Joseph E.; Vervliet, Bram; Roche, Bryan; Hermans, Dirk (September 2015). "Fear Generalization in Humans: Systematic Review and Implications for Anxiety Disorder Research" (PDF). Behavior Therapy. 46 (5): 561‚Äì582. doi:10.1016/j.beth.2014.10.001. PMID 26459838.
 Besnard, Antoine; Sahay, Amar (12 June 2015). "Adult Hippocampal Neurogenesis, Fear Generalization, and Stress". Neuropsychopharmacology. 41 (1): 24‚Äì44. doi:10.1038/npp.2015.167. PMC 4677119. PMID 26068726.
Category: Learning theory (education)
This page was last edited on 12 August 2025, at 16:36 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia¬Æ is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
Privacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view
Wikimedia Foundation
Powered by MediaWiki

Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
A Guide on Model Testing




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Guides
YOLO Common Issues
YOLO Performance Metrics
YOLO Thread-Safe Inference
YOLO Data Augmentation
Model Deployment Options
Model YAML Configuration Guide
K-Fold Cross Validation
Hyperparameter Tuning
SAHI Tiled Inference
AzureML Quickstart
Conda Quickstart
Docker Quickstart
Raspberry Pi
NVIDIA Jetson
DeepStream on NVIDIA Jetson
Triton Inference Server
Isolating Segmentation Objects
Edge TPU on Raspberry Pi
Viewing Inference Images in a Terminal
OpenVINO Latency vs Throughput modes
ROS Quickstart
Steps of a Computer Vision Project
Defining A Computer Vision Project's Goals
Data Collection and Annotation
Preprocessing Annotated Data
Tips for Model Training
Insights on Model Evaluation and Fine-Tuning
A Guide on Model Testing
Best Practices for Model Deployment
Maintaining Your Computer Vision Model
Deploying YOLO on Vertex AI in Docker container
Explorer
Explorer API
Explorer Dashboard Demo
VOC Exploration Example
YOLOv5
Quickstart
Environments
Tutorials
Table of contents
Introduction
Model Testing Vs. Model Evaluation
Preparing for Model Testing
Testing Your Computer Vision Model
Testing Your YOLO11 Model
Using YOLO11 to Predict on Multiple Test Images
Difference Between Validation and Prediction Modes
Running YOLO11 Predictions Without Custom Training
Overfitting and Underfitting in Machine Learning
Overfitting
Signs of Overfitting
Underfitting
Signs of Underfitting
Balancing Overfitting and Underfitting
Data Leakage in Computer Vision and How to Avoid It
Why Data Leakage Happens
Detecting Data Leakage
Avoiding Data Leakage
What Comes After Model Testing
Join the AI Conversation
Community Resources
Official Documentation
In Summary
FAQ
What are the key differences between model evaluation and model testing in computer vision?
How can I test my Ultralytics YOLO11 model on multiple images?
What should I do if my computer vision model shows signs of overfitting or underfitting?
How can I detect and avoid data leakage in computer vision?
What steps should I take after testing my computer vision model?
How do I run YOLO11 predictions without custom training?
A Guide on Model Testing
Introduction
After training and evaluating your model, it's time to test it. Model testing involves assessing how well it performs in real-world scenarios. Testing considers factors like accuracy, reliability, fairness, and how easy it is to understand the model's decisions. The goal is to make sure the model performs as intended, delivers the expected results, and fits into the overall objective of your application or project.



Watch: How to Test Machine Learning Models | Avoid Data Leakage in Computer Vision üöÄ

Model testing is quite similar to model evaluation, but they are two distinct steps in a computer vision project. Model evaluation involves metrics and plots to assess the model's accuracy. On the other hand, model testing checks if the model's learned behavior is the same as expectations. In this guide, we'll explore strategies for testing your computer vision models.

Model Testing Vs. Model Evaluation
First, let's understand the difference between model evaluation and testing with an example.

Suppose you have trained a computer vision model to recognize cats and dogs, and you want to deploy this model at a pet store to monitor the animals. During the model evaluation phase, you use a labeled dataset to calculate metrics like accuracy, precision, recall, and F1 score. For instance, the model might have an accuracy of 98% in distinguishing between cats and dogs in a given dataset.

After evaluation, you test the model using images from a pet store to see how well it identifies cats and dogs in more varied and realistic conditions. You check if it can correctly label cats and dogs when they are moving, in different lighting conditions, or partially obscured by objects like toys or furniture. Model testing checks that the model behaves as expected outside the controlled evaluation environment.

Preparing for Model Testing
Computer vision models learn from datasets by detecting patterns, making predictions, and evaluating their performance. These datasets are usually divided into training and testing sets to simulate real-world conditions. Training data teaches the model while testing data verifies its accuracy.

Here are two points to keep in mind before testing your model:

Realistic Representation: The previously unseen testing data should be similar to the data that the model will have to handle when deployed. This helps get a realistic understanding of the model's capabilities.
Sufficient Size: The size of the testing dataset needs to be large enough to provide reliable insights into how well the model performs.
Testing Your Computer Vision Model
Here are the key steps to take to test your computer vision model and understand its performance.

Run Predictions: Use the model to make predictions on the test dataset.
Compare Predictions: Check how well the model's predictions match the actual labels (ground truth).
Calculate Performance Metrics: Compute metrics like accuracy, precision, recall, and F1 score to understand the model's strengths and weaknesses. Testing focuses on how these metrics reflect real-world performance.
Visualize Results: Create visual aids like confusion matrices and ROC curves. These help you spot specific areas where the model might not be performing well in practical applications.
Next, the testing results can be analyzed:

Misclassified Images: Identify and review images that the model misclassified to understand where it is going wrong.
Error Analysis: Perform a thorough error analysis to understand the types of errors (e.g., false positives vs. false negatives) and their potential causes.
Bias and Fairness: Check for any biases in the model's predictions. Ensure that the model performs equally well across different subsets of the data, especially if it includes sensitive attributes like race, gender, or age.
Testing Your YOLO11 Model
To test your YOLO11 model, you can use the validation mode. It's a straightforward way to understand the model's strengths and areas that need improvement. Also, you'll need to format your test dataset correctly for YOLO11. For more details on how to use the validation mode, check out the Model Validation docs page.

Using YOLO11 to Predict on Multiple Test Images
If you want to test your trained YOLO11 model on multiple images stored in a folder, you can easily do so in one go. Instead of using the validation mode, which is typically used to evaluate model performance on a validation set and provide detailed metrics, you might just want to see predictions on all images in your test set. For this, you can use the prediction mode.

Difference Between Validation and Prediction Modes
Validation Mode: Used to evaluate the model's performance by comparing predictions against known labels (ground truth). It provides detailed metrics such as accuracy, precision, recall, and F1 score.
Prediction Mode: Used to run the model on new, unseen data to generate predictions. It does not provide detailed performance metrics but allows you to see how the model performs on real-world images.
Running YOLO11 Predictions Without Custom Training
If you are interested in testing the basic YOLO11 model to understand whether it can be used for your application without custom training, you can use the prediction mode. While the model is pre-trained on datasets like COCO, running predictions on your own dataset can give you a quick sense of how well it might perform in your specific context.

Overfitting and Underfitting in Machine Learning
When testing a machine learning model, especially in computer vision, it's important to watch out for overfitting and underfitting. These issues can significantly affect how well your model works with new data.

Overfitting
Overfitting happens when your model learns the training data too well, including the noise and details that don't generalize to new data. In computer vision, this means your model might do great with training images but struggle with new ones.

Signs of Overfitting
High Training Accuracy, Low Validation Accuracy: If your model performs very well on training data but poorly on validation or test data, it's likely overfitting.
Visual Inspection: Sometimes, you can see overfitting if your model is too sensitive to minor changes or irrelevant details in images.
Underfitting
Underfitting occurs when your model can't capture the underlying patterns in the data. In computer vision, an underfitted model might not even recognize objects correctly in the training images.

Signs of Underfitting
Low Training Accuracy: If your model can't achieve high accuracy on the training set, it might be underfitting.
Visual Mis-classification: Consistent failure to recognize obvious features or objects suggests underfitting.
Balancing Overfitting and Underfitting
The key is to find a balance between overfitting and underfitting. Ideally, a model should perform well on both training and validation datasets. Regularly monitoring your model's performance through metrics and visual inspections, along with applying the right strategies, can help you achieve the best results.

Overfitting and Underfitting Overview

Data Leakage in Computer Vision and How to Avoid It
While testing your model, something important to keep in mind is data leakage. Data leakage happens when information from outside the training dataset accidentally gets used to train the model. The model may seem very accurate during training, but it won't perform well on new, unseen data when data leakage occurs.

Why Data Leakage Happens
Data leakage can be tricky to spot and often comes from hidden biases in the training data. Here are some common ways it can happen in computer vision:

Camera Bias: Different angles, lighting, shadows, and camera movements can introduce unwanted patterns.
Overlay Bias: Logos, timestamps, or other overlays in images can mislead the model.
Font and Object Bias: Specific fonts or objects that frequently appear in certain classes can skew the model's learning.
Spatial Bias: Imbalances in foreground-background, bounding box distributions, and object locations can affect training.
Label and Domain Bias: Incorrect labels or shifts in data types can lead to leakage.
Detecting Data Leakage
To find data leakage, you can:

Check Performance: If the model's results are surprisingly good, it might be leaking.
Look at Feature Importance: If one feature is much more important than others, it could indicate leakage.
Visual Inspection: Double-check that the model's decisions make sense intuitively.
Verify Data Separation: Make sure data was divided correctly before any processing.
Avoiding Data Leakage
To prevent data leakage, use a diverse dataset with images or videos from different cameras and environments. Carefully review your data and check that there are no hidden biases, such as all positive samples being taken at a specific time of day. Avoiding data leakage will help make your computer vision models more reliable and effective in real-world situations.

What Comes After Model Testing
After testing your model, the next steps depend on the results. If your model performs well, you can deploy it into a real-world environment. If the results aren't satisfactory, you'll need to make improvements. This might involve analyzing errors, gathering more data, improving data quality, adjusting hyperparameters, and retraining the model.

Join the AI Conversation
Becoming part of a community of computer vision enthusiasts can aid in solving problems and learning more efficiently. Here are some ways to connect, seek help, and share your thoughts.

Community Resources
GitHub Issues: Explore the YOLO11 GitHub repository and use the Issues tab to ask questions, report bugs, and suggest new features. The community and maintainers are very active and ready to help.
Ultralytics Discord Server: Join the Ultralytics Discord server to chat with other users and developers, get support, and share your experiences.
Official Documentation
Ultralytics YOLO11 Documentation: Check out the official YOLO11 documentation for detailed guides and helpful tips on various computer vision projects.
These resources will help you navigate challenges and remain updated on the latest trends and practices within the computer vision community.

In Summary
Building trustworthy computer vision models relies on rigorous model testing. By testing the model with previously unseen data, we can analyze it and spot weaknesses like overfitting and data leakage. Addressing these issues before deployment helps the model perform well in real-world applications. It's important to remember that model testing is just as crucial as model evaluation in guaranteeing the model's long-term success and effectiveness.

FAQ
What are the key differences between model evaluation and model testing in computer vision?
Model evaluation and model testing are distinct steps in a computer vision project. Model evaluation involves using a labeled dataset to compute metrics such as accuracy, precision, recall, and F1 score, providing insights into the model's performance with a controlled dataset. Model testing, on the other hand, assesses the model's performance in real-world scenarios by applying it to new, unseen data, ensuring the model's learned behavior aligns with expectations outside the evaluation environment. For a detailed guide, refer to the steps in a computer vision project.

How can I test my Ultralytics YOLO11 model on multiple images?
To test your Ultralytics YOLO11 model on multiple images, you can use the prediction mode. This mode allows you to run the model on new, unseen data to generate predictions without providing detailed metrics. This is ideal for real-world performance testing on larger image sets stored in a folder. For evaluating performance metrics, use the validation mode instead.

What should I do if my computer vision model shows signs of overfitting or underfitting?
To address overfitting:

Regularization techniques like dropout.
Increase the size of the training dataset.
Simplify the model architecture.
To address underfitting:

Use a more complex model.
Provide more relevant features.
Increase training iterations or epochs.
Review misclassified images, perform thorough error analysis, and regularly track performance metrics to maintain a balance. For more information on these concepts, explore our section on Overfitting and Underfitting.

How can I detect and avoid data leakage in computer vision?
To detect data leakage:

Verify that the testing performance is not unusually high.
Check feature importance for unexpected insights.
Intuitively review model decisions.
Ensure correct data division before processing.
To avoid data leakage:

Use diverse datasets with various environments.
Carefully review data for hidden biases.
Ensure no overlapping information between training and testing sets.
For detailed strategies on preventing data leakage, refer to our section on Data Leakage in Computer Vision.

What steps should I take after testing my computer vision model?
Post-testing, if the model performance meets the project goals, proceed with deployment. If the results are unsatisfactory, consider:

Error analysis.
Gathering more diverse and high-quality data.
Hyperparameter tuning.
Retraining the model.
Gain insights from the Model Testing Vs. Model Evaluation section to refine and enhance model effectiveness in real-world applications.

How do I run YOLO11 predictions without custom training?
You can run predictions using the pre-trained YOLO11 model on your dataset to see if it suits your application needs. Utilize the prediction mode to get a quick sense of performance results without diving into custom training.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 9 months ago
glenn-jocher
RizwanMunawar
UltralyticsAssistant
abirami-vina

Tweet

Share

Comments
 Back to top
Previous
Insights on Model Evaluation and Fine-Tuning
Next
Best Practices for Model Deployment
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Transcript


Search in video
Introduction to Model Testing: Overview of the model testing process and its significance in machine learning workflows.
0:00
so in this video here we're going to
0:01
talk about model testing how we can do
0:03
it and also the step by step why you
0:05
should do it and also why it's important
0:07
thing to do before we put our models
0:09
into production so we make sure that we
0:10
have the best model running out there
0:13
solving problems so first of all after
How to Evaluate the Machine Learning Model after Training: Overview of evaluation methods for Ultralytics YOLO11 Models.
0:16
training we want to evaluate our model
0:18
and so on we have different data sets so
0:19
we have like the split so we have a
0:21
train validation and also our test set
0:24
we basically have our training set which
0:25
we're going to train our model on we
0:26
have our validation set evaluate do
0:29
model evaluation we f- tune our high
0:31
parameters and all that make sure we
0:33
have the best model once we have our
0:35
best model evaluated a bunch of
0:36
different High parameters we're testing
0:38
out different model variations we can
0:40
then take our test set go in and do
0:42
model testing on top of that to make
0:43
sure that it generalizes well that we
0:45
can actually like put our model into
0:47
production so this is the difference
Difference between Model Validation and Model Testing: Understanding the roles of validation and testing in model development.
0:49
between model evaluation and also model
0:52
testing when we're doing model
0:53
evaluation we go in and look at
0:55
different metrics the mean eror
0:56
positions and so on all the F1 scores
0:59
prision call and so on but when we want
1:01
to do model testing after the evaluation
1:04
phase then we want to go in and apply it
1:05
on act like use cases or like act like
1:08
images so say that we have a pet store
1:10
we want to classify or predict cats and
1:12
dogs then we train a model on that we
1:14
want to do model testing could be that
1:15
we want to deploy this model into a
1:17
specific store then we take some images
1:19
from that store test that on our model
1:22
to see if it generalizes well if we're
1:24
actually like able to do predictions
1:25
outside our controlled training and
1:27
validation environment so this is pretty
1:29
pretty much the difference between model
1:31
testing and evaluation and this is
1:33
really important to know so when we want
1:35
to do preparation for model testing
Preparation for Model Testing and How to Test the Models
1:37
first of all we need to make sure that
1:38
we have a sufficient size data set for
1:40
our testing because again if we don't
1:42
have enough examples like images and so
1:44
on for our testing we might not be able
1:46
to figure out all the edge cases or
1:48
different environments that it doesn't
1:50
work in could be the lighting conditions
1:52
could be the specific environments could
1:54
be the different types of cats and dogs
1:56
and so on for a specific pet store so it
1:59
really depends and that is why we need a
2:01
sufficient size data set we also need to
2:03
have a realistic representation because
2:06
if we train our images on one specific
2:08
store and we want to apply it to other
2:10
stores it might not generalize well so
2:12
we need to make sure that we have
2:14
realistic representation off the data
2:16
set that we act like one before we put a
2:18
model into production this is the last
2:20
thing that we want is to have a model
2:21
that works good in our own development
2:24
environment and then once we put it out
2:25
in the whole world it doesn't really
2:27
work because we don't have a realistic
2:29
repi
2:30
once we did our model both model
2:32
evaluation and also model testing so
2:35
when we test our computer vision models
2:36
run predictions compare the predictions
2:38
with each other make sure that it act
2:40
like performs as you want and also
2:42
compare with the ground trth data you
2:44
can set up and do evaluation again when
2:46
you do this you can just use it in
2:48
validation mode you can do evaluation
2:50
with allytics then we want to go and
2:52
calculate the performance metric so the
2:54
accuracy precision recall if one score
2:56
and so on for our tested and also
2:59
visualize our result result like one of
3:00
the best ways to make sure and also test
3:02
your AI machine learning model is I like
3:04
to run predictions on it visualize the
3:07
results be able to see it with your own
3:09
eyes does the model perform good because
3:11
again could be that the scores are
3:13
perfectly fine they're performing good
3:15
and so on but you miss some Ed cases
3:17
could be edge cases that your
3:18
performance metrics doesn't take into
3:20
account and you will only be able to see
3:22
those visually we need to go and make
3:23
sure that we analyze like misclassified
3:25
images error analysis and also bias and
3:28
fairness so it could be that there's any
3:30
bias in model predictions for a specific
3:32
type of cat or specific type of dog so
3:34
that's really important that we test
3:36
that case as well so when you're testing
3:38
your model with Alo ltic if you're using
3:40
Yol V 10 model or whatever YOLO model
3:43
supported with Alo litics you can
3:45
basically just run it in validation mode
3:46
on your specific data set you will get
3:49
pretty much just like a whole overview
3:50
over how your model is performing and
3:52
also how it does on the validation set
3:54
but also just take your model throw it
3:56
through some images and see how it works
3:58
you can either use it for like a fine
4:00
tune model or also a pre-end model and
4:02
we need to do model testing in both
4:04
cases one of the things when we actually
Overfitting and Underfitting in Machine Learning: Insights into detecting, addressing, and balancing these critical issues.
4:07
do custom fine tuning and so on on our
4:09
own specific data set we need to make
4:11
sure that we're not underfitting or
4:12
overfitting because if we take a base
4:14
model fine tuning on our own data set
4:16
and we just have a very specific data
4:18
set we might train it and overfit to
4:21
that specific data set so our model
4:23
doesn't really generalize well which
4:24
means that if you're only training on
4:26
one specific pet store it might not be
4:28
able to generalize well and and apply to
4:30
multiple different pet stores to prevent
4:32
that we can either like train it for
4:34
less or we can have multiple pet stores
4:36
combine some images from pretty much
4:38
multiple pet stores in the same data set
4:40
and it will start to generalize well so
4:43
if you just have one specific example
4:45
it's not going to generalize but once
4:46
you start to get two three four five
4:49
different examples from different
4:50
environments the model will start to
4:52
learn from that and generalize we don't
4:54
want under fitting as well because then
4:56
our model is basically not learning
4:58
enough so this is very important as well
Data Leakage in Computer Vision and How to Avoid It: Strategies to prevent data leakage and ensure fair evaluation results.
5:00
could be data leakage once you put your
5:02
model out there the data might be
5:04
different you might have like data drift
5:06
as well so it might not be the exact
5:08
same data the exact same environment
5:10
lighting conditions and so on once you
5:12
want to run your models in production
5:14
compared to in your testing environment
5:16
so this is one of the most frequent
5:17
problems when you actually like have a
5:19
good model you can see it in the juv
5:21
notebook it's very good but you put it
5:23
out in the real world and it doesn't
5:24
really work that good could also be over
5:26
time that you have data change or like
5:28
data shift so we need to make sure that
5:30
we're testing for that as well so after
5:33
we're done with our model testing we can
5:34
take our model put it into production
5:37
make sure that we actually like optimize
5:38
it for a specific framework and also the
5:41
specific Hardware that we are running
5:42
our models on so this is all for model
5:45
testing make sure that you do it it is
5:47
really important if you're not doing it
5:49
know each individual step and so on and
5:51
also why it's so important you run into
5:53
a lot of problems once you want to take
5:55
your models and apply them into the real
Conclusion and Summary: Key takeaways, practical tips, and next steps for deploying models.
5:58
world so hope you learned Le on
6:00
throughout this video here definitely
6:01
check out the other videos that we have
6:02
on the channel pretty much covering
6:04
everything that you need in the computer
6:06
vision Pipeline and then I'll just see
6:08
you guys in one of the upcoming videos
6:10
until then Happy learning
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
The Importance of Benchmarking
Real-World Examples
Benchmark vs. Other Datasets
Glossary
Benchmark Dataset
Discover how benchmark datasets drive AI innovation by enabling fair model evaluation, reproducibility, and progress in machine learning.

Flexible enterprise licensing solution to power your innovation
Get started
Train YOLO models with Ultralytics HUB
A benchmark dataset is a standardized, high-quality dataset used in machine learning (ML) to evaluate and compare the performance of different algorithms and models in a fair, reproducible manner. These datasets are carefully curated and widely accepted by the research community, serving as a common ground for measuring progress in specific tasks like object detection or image classification. By testing models against the same data and evaluation metrics, researchers and developers can objectively determine which approaches are more effective, faster, or more efficient. The use of benchmarks is fundamental to advancing the state of the art in artificial intelligence (AI).

The Importance of Benchmarking
In the rapidly evolving field of computer vision (CV), benchmark datasets are indispensable. They provide a stable baseline for assessing model improvements and innovations. Without them, it would be difficult to know if a new model architecture or training technique truly represents an advancement or if its performance is simply due to being tested on a different, potentially easier, dataset. Public leaderboards, often associated with challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), use these datasets to foster healthy competition and transparently track progress. This process encourages the development of more robust and generalizable models, which is crucial for real-world model deployment.

Real-World Examples
Comparing Object Detection Models: When Ultralytics develops a new model like YOLO11, its performance is rigorously tested on standard benchmark datasets such as COCO. The results, measured by metrics like mean Average Precision (mAP), are compared against previous versions (YOLOv8, YOLOv10) and other state-of-the-art models. These model comparisons help users choose the best model for their needs. Platforms like Ultralytics HUB allow users to train models and benchmark them on custom data.
Advancing Autonomous Driving: Companies developing technology for autonomous vehicles rely heavily on benchmarks like Argoverse or nuScenes. These datasets contain complex urban driving scenarios with detailed annotations for cars, pedestrians, and cyclists. By evaluating their perception models on these benchmarks, companies can measure improvements in detection accuracy, tracking reliability, and overall system robustness, which is critical for ensuring safety in AI for self-driving cars.
Benchmark vs. Other Datasets
It's important to distinguish benchmark datasets from other data splits used in the ML lifecycle:

Training Data: Used to teach the model by adjusting its parameters based on input examples and their corresponding labels. This is typically the largest portion of the data. Techniques like data augmentation are often applied here.
Validation Data: Used during training to tune model hyperparameters (like learning rate or architecture choices) and provide an unbiased estimate of model skill. It helps prevent overfitting to the training data.
Test Data: Used after the model is fully trained to provide a final, unbiased evaluation of its performance on unseen data.
While a benchmark dataset often serves as a standardized test set, its primary purpose is broader: to provide a common standard for comparison across the entire research community. Many benchmark datasets are listed and tracked on platforms like Papers with Code, which hosts leaderboards for various ML tasks. Other notable datasets include Open Images V7 from Google and the Pascal VOC challenge. Access to such high-quality computer vision datasets is essential for anyone building reliable AI systems.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
COCO




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Datasets
Detection
Argoverse
COCO
COCO8
COCO8-Grayscale
COCO8-Multispectral
COCO128
LVIS
GlobalWheat2020
Objects365
OpenImagesV7
SKU-110K
HomeObjects-3K
Construction-PPE
VisDrone
VOC
xView
RF100
Brain-tumor
African-wildlife
Signature
Medical-pills
Segmentation
COCO
COCO8-seg
COCO128-seg
Crack-seg
Carparts-seg
Package-seg
Pose
COCO
COCO8-pose
Tiger-pose
Hand-keypoints
Dog-pose
Classification
Caltech 101
Caltech 256
CIFAR-10
CIFAR-100
Fashion-MNIST
ImageNet
ImageNet-10
Imagenette
Imagewoof
MNIST
Oriented Bounding Boxes (OBB)
DOTAv2
DOTA8
Multi-Object Tracking
Table of contents
COCO Pretrained Models
Key Features
Dataset Structure
Applications
Dataset YAML
Usage
Sample Images and Annotations
Citations and Acknowledgments
FAQ
What is the COCO dataset and why is it important for computer vision?
How can I train a YOLO model using the COCO dataset?
What are the key features of the COCO dataset?
Where can I find pretrained YOLO11 models trained on the COCO dataset?
How is the COCO dataset structured and how do I use it?
COCO Dataset
The COCO (Common Objects in Context) dataset is a large-scale object detection, segmentation, and captioning dataset. It is designed to encourage research on a wide variety of object categories and is commonly used for benchmarking computer vision models. It is an essential dataset for researchers and developers working on object detection, segmentation, and pose estimation tasks.



Watch: Ultralytics COCO Dataset Overview

COCO Pretrained Models
Model size
(pixels)  mAPval
50-95 Speed
CPU ONNX
(ms)  Speed
T4 TensorRT10
(ms)  params
(M) FLOPs
(B)
YOLO11n 640 39.5  56.1 ¬± 0.8  1.5 ¬± 0.0 2.6 6.5
YOLO11s 640 47.0  90.0 ¬± 1.2  2.5 ¬± 0.0 9.4 21.5
YOLO11m 640 51.5  183.2 ¬± 2.0 4.7 ¬± 0.1 20.1  68.0
YOLO11l 640 53.4  238.6 ¬± 1.4 6.2 ¬± 0.1 25.3  86.9
YOLO11x 640 54.7  462.8 ¬± 6.7 11.3 ¬± 0.2  56.9  194.9
Key Features
COCO contains 330K images, with 200K images having annotations for object detection, segmentation, and captioning tasks.
The dataset comprises 80 object categories, including common objects like cars, bicycles, and animals, as well as more specific categories such as umbrellas, handbags, and sports equipment.
Annotations include object bounding boxes, segmentation masks, and captions for each image.
COCO provides standardized evaluation metrics like mean Average Precision (mAP) for object detection, and mean Average Recall (mAR) for segmentation tasks, making it suitable for comparing model performance.
Dataset Structure
The COCO dataset is split into three subsets:

Train2017: This subset contains 118K images for training object detection, segmentation, and captioning models.
Val2017: This subset has 5K images used for validation purposes during model training.
Test2017: This subset consists of 20K images used for testing and benchmarking the trained models. Ground truth annotations for this subset are not publicly available, and the results are submitted to the COCO evaluation server for performance evaluation.
Applications
The COCO dataset is widely used for training and evaluating deep learning models in object detection (such as Ultralytics YOLO, Faster R-CNN, and SSD), instance segmentation (such as Mask R-CNN), and keypoint detection (such as OpenPose). The dataset's diverse set of object categories, large number of annotated images, and standardized evaluation metrics make it an essential resource for computer vision researchers and practitioners.

Dataset YAML
A YAML (Yet Another Markup Language) file is used to define the dataset configuration. It contains information about the dataset's paths, classes, and other relevant information. In the case of the COCO dataset, the coco.yaml file is maintained at https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml.

ultralytics/cfg/datasets/coco.yaml


# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

# COCO 2017 dataset https://cocodataset.org by Microsoft
# Documentation: https://docs.ultralytics.com/datasets/detect/coco/
# Example usage: yolo train data=coco.yaml
# parent
# ‚îú‚îÄ‚îÄ ultralytics
# ‚îî‚îÄ‚îÄ datasets
#     ‚îî‚îÄ‚îÄ coco ‚Üê downloads here (20.1 GB)

# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: coco # dataset root dir
train: train2017.txt # train images (relative to 'path') 118287 images
val: val2017.txt # val images (relative to 'path') 5000 images
test: test-dev2017.txt # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794

# Classes
names:
  0: person
  1: bicycle
  2: car
  3: motorcycle
  4: airplane
  5: bus
  6: train
  7: truck
  8: boat
  9: traffic light
  10: fire hydrant
  11: stop sign
  12: parking meter
  13: bench
  14: bird
  15: cat
  16: dog
  17: horse
  18: sheep
  19: cow
  20: elephant
  21: bear
  22: zebra
  23: giraffe
  24: backpack
  25: umbrella
  26: handbag
  27: tie
  28: suitcase
  29: frisbee
  30: skis
  31: snowboard
  32: sports ball
  33: kite
  34: baseball bat
  35: baseball glove
  36: skateboard
  37: surfboard
  38: tennis racket
  39: bottle
  40: wine glass
  41: cup
  42: fork
  43: knife
  44: spoon
  45: bowl
  46: banana
  47: apple
  48: sandwich
  49: orange
  50: broccoli
  51: carrot
  52: hot dog
  53: pizza
  54: donut
  55: cake
  56: chair
  57: couch
  58: potted plant
  59: bed
  60: dining table
  61: toilet
  62: tv
  63: laptop
  64: mouse
  65: remote
  66: keyboard
  67: cell phone
  68: microwave
  69: oven
  70: toaster
  71: sink
  72: refrigerator
  73: book
  74: clock
  75: vase
  76: scissors
  77: teddy bear
  78: hair drier
  79: toothbrush

# Download script/URL (optional)
download: |
  from pathlib import Path

  from ultralytics.utils import ASSETS_URL
  from ultralytics.utils.downloads import download

  # Download labels
  segments = True  # segment or box labels
  dir = Path(yaml["path"])  # dataset root dir
  urls = [ASSETS_URL + ("/coco2017labels-segments.zip" if segments else "/coco2017labels.zip")]  # labels
  download(urls, dir=dir.parent)
  # Download data
  urls = [
      "http://images.cocodataset.org/zips/train2017.zip",  # 19G, 118k images
      "http://images.cocodataset.org/zips/val2017.zip",  # 1G, 5k images
      "http://images.cocodataset.org/zips/test2017.zip",  # 7G, 41k images (optional)
  ]
  download(urls, dir=dir / "images", threads=3)
Usage
To train a YOLO11n model on the COCO dataset for 100 epochs with an image size of 640, you can use the following code snippets. For a comprehensive list of available arguments, refer to the model Training page.

Train Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data="coco.yaml", epochs=100, imgsz=640)

Sample Images and Annotations
The COCO dataset contains a diverse set of images with various object categories and complex scenes. Here are some examples of images from the dataset, along with their corresponding annotations:

Dataset sample image

Mosaiced Image: This image demonstrates a training batch composed of mosaiced dataset images. Mosaicing is a technique used during training that combines multiple images into a single image to increase the variety of objects and scenes within each training batch. This helps improve the model's ability to generalize to different object sizes, aspect ratios, and contexts.
The example showcases the variety and complexity of the images in the COCO dataset and the benefits of using mosaicing during the training process.

Citations and Acknowledgments
If you use the COCO dataset in your research or development work, please cite the following paper:


BibTeX

@misc{lin2015microsoft,
      title={Microsoft COCO: Common Objects in Context},
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll√°r},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

We would like to acknowledge the COCO Consortium for creating and maintaining this valuable resource for the computer vision community. For more information about the COCO dataset and its creators, visit the COCO dataset website.

FAQ
What is the COCO dataset and why is it important for computer vision?
The COCO dataset (Common Objects in Context) is a large-scale dataset used for object detection, segmentation, and captioning. It contains 330K images with detailed annotations for 80 object categories, making it essential for benchmarking and training computer vision models. Researchers use COCO due to its diverse categories and standardized evaluation metrics like mean Average Precision (mAP).

How can I train a YOLO model using the COCO dataset?
To train a YOLO11 model using the COCO dataset, you can use the following code snippets:

Train Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data="coco.yaml", epochs=100, imgsz=640)

Refer to the Training page for more details on available arguments.

What are the key features of the COCO dataset?
The COCO dataset includes:

330K images, with 200K annotated for object detection, segmentation, and captioning.
80 object categories ranging from common items like cars and animals to specific ones like handbags and sports equipment.
Standardized evaluation metrics for object detection (mAP) and segmentation (mean Average Recall, mAR).
Mosaicing technique in training batches to enhance model generalization across various object sizes and contexts.
Where can I find pretrained YOLO11 models trained on the COCO dataset?
Pretrained YOLO11 models on the COCO dataset can be downloaded from the links provided in the documentation. Examples include:

YOLO11n
YOLO11s
YOLO11m
YOLO11l
YOLO11x
These models vary in size, mAP, and inference speed, providing options for different performance and resource requirements.

How is the COCO dataset structured and how do I use it?
The COCO dataset is split into three subsets:

Train2017: 118K images for training.
Val2017: 5K images for validation during training.
Test2017: 20K images for benchmarking trained models. Results need to be submitted to the COCO evaluation server for performance evaluation.
The dataset's YAML configuration file is available at coco.yaml, which defines paths, classes, and dataset details.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 7 months ago
glenn-jocher
RizwanMunawar
jk4e
ambitious-octopus
UltralyticsAssistant
MatthewNoyce
Laughing-q

Tweet

Share

Comments
 Back to top
Previous
Argoverse
Next
COCO8
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Transcript


Search in video
Introduction to Ultralytics COCO Dataset
0:00
hey guys wel to new video in this video
0:01
here we're going to take a look at the
0:02
Coco data set and how we can use it with
0:04
alal lytics so first of all we're going
0:06
to talk about like what is the Coco data
0:08
set we're going to go through a couple
0:09
of examples so this is the data set that
0:11
most optic detection and also
0:12
segmentation models are pre-trained on
0:14
also the YOLO V5 and you eight models if
0:17
you're using that directly those
0:18
pre-trained models are pre-trained on
0:20
the coko data set and it contains 80
0:22
classes in this video here we're going
0:23
to take a look at how we can use it with
0:25
Al lytics the key features the data
0:27
structure and also the individual
0:28
classes which are in inside the Coco
0:31
data set so if we just go straight into
0:32
the allytics documentation we can go up
Exploring Ultralytics Documentation
0:34
here at the top and we have a data set
0:37
tab if you go over here to the left we
0:38
can see all the different data sets
0:40
directly integrated inside Alo litics
0:42
and you can pull all the data set you
0:43
can use it on your own try to train the
0:45
Y V8 and you models on it directly with
0:48
just specifying the yaml file for the
0:50
data set structure so if you go over to
0:52
the left we can see that we have an
0:53
explore API which we already covered in
0:55
another video so this is pretty cool if
0:57
you want to explore how your data set
0:59
looks like we can just see a preview
1:00
here so you can actually like just have
1:03
curious or ask an AI to show images for
1:06
specific situations could so it could be
1:08
like if you want to find cars and images
1:10
you can just search for cars and it will
1:12
filter all of those images in your data
1:14
set so that's very cool but if you go
1:16
over to the left we can see that we have
1:17
a detection tab we also have a
1:18
segmentation Tab and we also have this
1:21
post estimation classification and
1:23
rended bounding boxes here at the end
1:25
but this video here we're going to take
1:26
a look at the Coco data set it is both
1:28
used for detection and and also
1:30
segmentation and even postestimation
1:32
down here for human postestimation when
1:35
we pre-training the models so this is
1:37
widely used data set pretty much used
1:40
all over the world and it is
1:41
specifically used for Benchmark computer
1:43
vision models we can see the key
Key Features of the COCO Dataset
1:45
features here so the Coca data set it
1:47
contains 330k images with 200k images
1:51
having annotations for up detection
1:53
segmentation and also captioning task so
1:56
this is pretty cool we also have some
1:57
images just for like having a test set
1:59
and also if you just want to do like
2:01
zero shot or few shot Learning Without
2:04
labels and
2:05
annotations the data set contains 80
2:08
object categories including common
2:09
objects like cars bicycles and animals
2:11
you guys have probably seen like a ton
2:13
of weos out there running with the UL 8
2:15
models so we have 80 classes that we can
2:17
actually detect from out of the box with
2:20
these pre-trained models so the
2:21
annotations include optic bounding boxes
2:24
segmentation mask and captions for each
2:25
image as we just mentioned up above we
2:28
can also see that c provides a
2:29
standardized evaluation metric like mean
2:31
average position for object detection
2:33
mean average recall for segmentation
2:35
task which is act like pretty good for
2:37
comparing model performances across a
2:39
bunch of different models so if we take
2:41
a look at the data set structure we can
Understanding Dataset Structures
2:43
see that we have the three soft sets we
2:44
have a train validation and a test set
2:46
so for the applications it can be used
2:48
for a lot of different applications and
2:50
projects out there and you have probably
2:51
already experienced and played around
2:53
with the pre-train models with the 8
2:55
classes so again we're going to take a
2:57
look at the data set L EML file here
Overview of Dataset YAML Files
2:59
where we can see all the classes
3:00
specified so this is the data structure
3:02
that we need to have to use it with
3:04
allytics we just have to have this coco.
3:06
yaml we can specify the data set we can
3:08
have it locally as well on our computer
3:10
or we can have it down at the bottom in
3:12
our data set file and it will download
3:14
it automatically as well so this is a
3:16
script and URL here for downloading
3:18
right now this is a script but you can
3:19
also just have a URL here instead and
3:21
then it's going to download that once
3:23
you're going to use the models but I'm
3:24
going to show you that in just a second
3:26
but let's now going to take a look at
3:27
some of the classes that we can detect
3:29
by using using the Coco data set and the
3:31
pre-train models out of the box so you
3:33
guys are probably familiar with the
3:35
person bicycle cars these are more the
3:37
common classes but we're actually like
3:39
also able to detect airplanes traffic
3:41
lights boats parking meter bench bird a
3:44
bunch of different types of animals I
3:46
can just go through them here and you
3:47
guys can go through them also a lot of
3:50
different like sports balls could it be
3:51
a baseball baseball glove skateboards
3:54
Apple sandwich so just a bunch of
3:56
different common Optics out there in the
3:58
real world so this can be used for a lot
4:00
of applications and projects out of the
4:02
box so this is pretty much the only
4:04
thing that we need to be able to use
4:05
this data set with ultral litics if we
4:07
go down we also have a usage tab so this
How to Utilize Datasets Effectively
4:09
is how we can train a Yol 8 model with
4:11
the kogre data set either use the CLI
4:14
command interface or we can use a python
4:16
script we only need to set up a few
4:18
lines of code so from allytics import
4:20
Yello we create an instance of our YOLO
4:21
model depending on which of the models
4:23
that we want to use right now we can
4:25
just specify Nano but we also have like
4:27
Nano small medium large and extra large
4:29
and then then we can just directly on
4:30
that model instance call train we
4:34
specify the AML file here for our data
4:36
the number of epoch and also the image
4:37
size that we want to train our model on
4:40
so this is how you can load a pre-train
4:41
model and then you can train it on the
4:43
KOCO data set but you can also like just
4:44
initialize the weights randomly and
4:46
train fully from scratch that will take
4:48
a long time because again you know that
4:50
we have 120k images in the Coco data set
4:53
we also have this Coco 8 data set which
4:54
is a softs set of it but again this will
4:57
take a very long time to go in and train
4:59
from scratch so that's why we're often
5:01
using pre-end model and then we just
5:02
fine-tune it on our own data set because
5:05
then we act like only need a few hundred
5:07
images and we can have our own custom
5:08
applications and projects up and running
5:11
if we take a look at the sample images
5:12
and annotations we can just see we have
Examples of Sample Images and Annotations
5:14
some tennis players we have bosses
5:16
different types of animals even in
5:17
different lighting conditions here we
5:19
can just see all of these boxes we have
5:21
some augmentation in here so these are
5:23
some very complex situations very small
5:25
Optics a lot of variations in the Optics
5:28
as well because we have these 80
5:29
different classes in there so we can see
5:32
that the Coco data set also uses this
5:34
mosa image style where it's basically
Mosaic Image Augmentation Overview
5:36
just combining a bunch of these images
5:38
so here you can see we have a boss we
5:39
also have an indoor environment and this
5:41
is basically just a technique that is
5:42
used during training that combines like
5:44
multiple images into a single image
5:46
basically just to increase the variety
5:48
of objects and also different scenes
5:50
because then we can both have indoor
5:51
scenes outdoor scenes and the models
5:53
will be able to generalize way more
5:55
especially when we're pre-training our
5:57
models so before we wrap out the video
5:59
here let's just go in and open up a new
6:00
Anaconda prompt and see how we can
Training YOLOv8 Models with Ultralytics Datasets
6:02
actually use it directly from the
6:03
command line you can also do it in
6:05
Python by just copying pasting this Co
6:08
snit again we have coppit on all of the
6:10
documentations inside also lyc stocks
6:12
and then you can just copy paste it and
6:14
use it in your own applications and
6:15
projects you can also directly call
6:17
predict and then it's going to do
6:18
predictions on the pred models but we
6:20
have videos covering all of that here on
6:22
the allytics YouTube channel so right
6:25
now I'm just going to copy paste this
6:26
one in here we have yellow our task is
6:28
the tech we want to train and this is
6:30
the data set that we're going to specify
6:32
and we also specify the model number of
6:33
EPO and also the image size so just
6:36
decrease the EPO here to save some time
6:38
we should be able to hit enter and it's
6:39
going to train the model and this is how
6:41
easy it is to train a Yol V8 model on
6:44
the Cobo data set so first of all here
6:46
we can see that it's going to download
6:47
the data set it might just take some
6:49
time we can see the progress B down at
6:51
the bottom after I downloaded data set
6:53
it's going to download the model if this
6:54
is the first time that you're running it
6:56
and after we have the data set and we
6:58
have our model we can now go and train
7:00
it for the number of epoch that were
7:01
specified so after it's done training we
7:03
can go in and grab the best wage from
7:05
our runs folder and also see all the
7:07
training scrafts so we can go in and
7:08
evaluate our model that we have just
7:10
trained and use it in own applications
7:12
and projects so now we can see that we
7:14
have downloaded train validation and our
7:15
test split and it will just take a
7:17
minute or two and then it's going to
7:19
start the training so that's it for the
7:20
C data with allytics we went over the
Summary
7:23
whole structure and all the individual
7:24
classes which we're using to pre-train
7:26
our models and then we can fine-tune our
7:28
own custom models on top of that so hope
7:30
you learned T definitely go in and check
7:32
it out test it out yourself and then
7:33
I'll just see you guys in the next video
7:35
Until Then happy learning
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
The Imagenet Large Scale Visual Recognition Challenge (ILSVRC)
Real-World Applications of Imagenet
Imagenet vs. Related Concepts
Glossary
ImageNet
Discover ImageNet, the groundbreaking dataset fueling computer vision advances with 14M+ images, powering AI research, models & applications.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
ImageNet is a massive, publicly accessible dataset of over 14 million images that have been hand-annotated to indicate what objects they picture. Organized according to the WordNet hierarchy, it contains more than 20,000 categories, with a typical category, such as "balloon" or "strawberry," consisting of several hundred images. This vast and diverse collection has been instrumental in advancing the fields of computer vision (CV) and deep learning (DL), serving as a standard for training and benchmarking models.

The creation of ImageNet by researchers at Stanford University was a pivotal moment for artificial intelligence (AI). Before ImageNet, datasets were often too small to train complex neural networks (NN) effectively, leading to problems like overfitting. ImageNet provided the scale needed to train deep models, paving the way for the modern AI revolution. You can learn more by reading the original ImageNet research paper.

The Imagenet Large Scale Visual Recognition Challenge (ILSVRC)
The influence of ImageNet was amplified by the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), an annual competition held from 2010 to 2017. This challenge became a crucial benchmark for evaluating the performance of computer vision algorithms. In 2012, a convolutional neural network (CNN) named AlexNet achieved a groundbreaking victory, significantly outperforming all previous models. This success demonstrated the power of deep learning and GPU computation, sparking a wave of innovation in the field. The ILSVRC has been a key driver in the development of many modern architectures, and you can see how today's models perform on various benchmarks on sites like Papers with Code.

Real-World Applications of Imagenet
ImageNet's primary use is as a resource for pre-training models. By training a model on this vast dataset, it learns to recognize a rich set of visual features. This knowledge can then be transferred to new, more specific tasks. This technique is known as transfer learning.

Medical Imaging Analysis: A model pre-trained on ImageNet, such as an Ultralytics YOLO model, can be fine-tuned on a much smaller, specialized dataset of medical scans to detect specific conditions like tumors. The initial training on ImageNet provides a strong foundation of general visual understanding, which is crucial for achieving high accuracy in medical image analysis tasks where labeled data is scarce. This is a key application for AI in healthcare.
Retail Product Recognition: In retail, models can be adapted to identify thousands of different products on a shelf for automated inventory management. Instead of training from scratch, a model pre-trained on ImageNet can be quickly adapted to the specific products of a store. This reduces the need for massive amounts of custom training data and accelerates model deployment. Many powerful AI in retail solutions leverage this approach.
Imagenet vs. Related Concepts
It is important to differentiate ImageNet from other related terms and datasets:

ImageNet vs. CV Tasks: ImageNet itself is a dataset‚Äîa collection of labeled images. It is not a task. Instead, it is used to train and benchmark models that perform tasks like image classification, where a single label is assigned to an image. This differs from object detection, which involves locating objects with bounding boxes, or image segmentation, which classifies every pixel in an image.
ImageNet vs. COCO: While ImageNet is the gold standard for classification, other computer vision datasets are more suitable for other tasks. The COCO (Common Objects in Context) dataset, for example, is the preferred benchmark for object detection and instance segmentation. This is because COCO provides more detailed annotations, such as bounding boxes and per-pixel segmentation masks for multiple objects in each image. In contrast, most ImageNet images have only a single image-level label.
Models like YOLO11 are often pre-trained on ImageNet for their classification backbone before being trained on COCO for detection tasks. This multi-stage training process leverages the strengths of both datasets. You can see how different models compare on these benchmarks on our model comparison pages. While highly influential, it's worth noting that ImageNet has limitations, including known dataset biases that are important to consider from an AI ethics perspective.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
Train




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Tasks
Detect
Segment
Classify
Pose
OBB
Modes
Train
Val
Predict
Export
Track
Benchmark
Table of contents
Introduction
Why Choose Ultralytics YOLO for Training?
Key Features of Train Mode
Usage Examples
Multi-GPU Training
Idle GPU Training
Apple Silicon MPS Training
Resuming Interrupted Trainings
Train Settings
Augmentation Settings and Hyperparameters
Logging
Comet
ClearML
TensorBoard
FAQ
How do I train an object detection model using Ultralytics YOLO11?
What are the key features of Ultralytics YOLO11's Train mode?
How do I resume training from an interrupted session in Ultralytics YOLO11?
Can I train YOLO11 models on Apple silicon chips?
What are the common training settings, and how do I configure them?
Model Training with Ultralytics YOLO
Ultralytics YOLO ecosystem and integrations

Introduction
Training a deep learning model involves feeding it data and adjusting its parameters so that it can make accurate predictions. Train mode in Ultralytics YOLO11 is engineered for effective and efficient training of object detection models, fully utilizing modern hardware capabilities. This guide aims to cover all the details you need to get started with training your own models using YOLO11's robust set of features.



Watch: How to Train a YOLO model on Your Custom Dataset in Google Colab.

Why Choose Ultralytics YOLO for Training?
Here are some compelling reasons to opt for YOLO11's Train mode:

Efficiency: Make the most out of your hardware, whether you're on a single-GPU setup or scaling across multiple GPUs.
Versatility: Train on custom datasets in addition to readily available ones like COCO, VOC, and ImageNet.
User-Friendly: Simple yet powerful CLI and Python interfaces for a straightforward training experience.
Hyperparameter Flexibility: A broad range of customizable hyperparameters to fine-tune model performance.
Key Features of Train Mode
The following are some notable features of YOLO11's Train mode:

Automatic Dataset Download: Standard datasets like COCO, VOC, and ImageNet are downloaded automatically on first use.
Multi-GPU Support: Scale your training efforts seamlessly across multiple GPUs to expedite the process.
Hyperparameter Configuration: The option to modify hyperparameters through YAML configuration files or CLI arguments.
Visualization and Monitoring: Real-time tracking of training metrics and visualization of the learning process for better insights.
Tip

YOLO11 datasets like COCO, VOC, ImageNet and many others automatically download on first use, i.e. yolo train data=coco.yaml
Usage Examples
Train YOLO11n on the COCO8 dataset for 100 epochs at image size 640. The training device can be specified using the device argument. If no argument is passed GPU device=0 will be used if available, otherwise device='cpu' will be used. See Arguments section below for a full list of training arguments.

Windows Multi-Processing Error

On Windows, you may receive a RuntimeError when launching the training as a script. Add a if __name__ == "__main__": block before your training code to resolve it.

Single-GPU and CPU Training Example

Device is determined automatically. If a GPU is available then it will be used (default CUDA device 0), otherwise training will start on CPU.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.yaml")  # build a new model from YAML
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)
model = YOLO("yolo11n.yaml").load("yolo11n.pt")  # build from YAML and transfer weights

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

Multi-GPU Training
Multi-GPU training allows for more efficient utilization of available hardware resources by distributing the training load across multiple GPUs. This feature is available through both the Python API and the command-line interface. To enable multi-GPU training, specify the GPU device IDs you wish to use.

Multi-GPU Training Example

To train with 2 GPUs, CUDA devices 0 and 1 use the following commands. Expand to additional GPUs as required.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model with 2 GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[0, 1])

# Train the model with the two most idle GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[-1, -1])

Idle GPU Training
Idle GPU Training enables automatic selection of the least utilized GPUs in multi-GPU systems, optimizing resource usage without manual GPU selection. This feature identifies available GPUs based on utilization metrics and VRAM availability.

Idle GPU Training Example

To automatically select and use the most idle GPU(s) for training, use the -1 device parameter. This is particularly useful in shared computing environments or servers with multiple users.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolov8n.pt")  # load a pretrained model (recommended for training)

# Train using the single most idle GPU
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=-1)

# Train using the two most idle GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[-1, -1])

The auto-selection algorithm prioritizes GPUs with:

Lower current utilization percentages
Higher available memory (free VRAM)
Lower temperature and power consumption
This feature is especially valuable in shared computing environments or when running multiple training jobs across different models. It automatically adapts to changing system conditions, ensuring optimal resource allocation without manual intervention.

Apple Silicon MPS Training
With the support for Apple silicon chips integrated in the Ultralytics YOLO models, it's now possible to train your models on devices utilizing the powerful Metal Performance Shaders (MPS) framework. The MPS offers a high-performance way of executing computation and image processing tasks on Apple's custom silicon.

To enable training on Apple silicon chips, you should specify 'mps' as your device when initiating the training process. Below is an example of how you could do this in Python and via the command line:

MPS Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model with MPS
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device="mps")

While leveraging the computational power of the Apple silicon chips, this enables more efficient processing of the training tasks. For more detailed guidance and advanced configuration options, please refer to the PyTorch MPS documentation.

Resuming Interrupted Trainings
Resuming training from a previously saved state is a crucial feature when working with deep learning models. This can come in handy in various scenarios, like when the training process has been unexpectedly interrupted, or when you wish to continue training a model with new data or for more epochs.

When training is resumed, Ultralytics YOLO loads the weights from the last saved model and also restores the optimizer state, learning rate scheduler, and the epoch number. This allows you to continue the training process seamlessly from where it was left off.

You can easily resume training in Ultralytics YOLO by setting the resume argument to True when calling the train method, and specifying the path to the .pt file containing the partially trained model weights.

Below is an example of how to resume an interrupted training using Python and via the command line:

Resume Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("path/to/last.pt")  # load a partially trained model

# Resume training
results = model.train(resume=True)

By setting resume=True, the train function will continue training from where it left off, using the state stored in the 'path/to/last.pt' file. If the resume argument is omitted or set to False, the train function will start a new training session.

Remember that checkpoints are saved at the end of every epoch by default, or at fixed intervals using the save_period argument, so you must complete at least 1 epoch to resume a training run.

Train Settings
The training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model's performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.

Argument  Type  Default Description
model str None  Specifies the model file for training. Accepts a path to either a .pt pretrained model or a .yaml configuration file. Essential for defining the model structure or initializing weights.
data  str None  Path to the dataset configuration file (e.g., coco8.yaml). This file contains dataset-specific parameters, including paths to training and validation data, class names, and number of classes.
epochs  int 100 Total number of training epochs. Each epoch represents a full pass over the entire dataset. Adjusting this value can affect training duration and model performance.
time  float None  Maximum training time in hours. If set, this overrides the epochs argument, allowing training to automatically stop after the specified duration. Useful for time-constrained training scenarios.
patience  int 100 Number of epochs to wait without improvement in validation metrics before early stopping the training. Helps prevent overfitting by stopping training when performance plateaus.
batch int or float  16  Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).
imgsz int 640 Target image size for training. Images are resized to squares with sides equal to the specified value (if rect=False), preserving aspect ratio for YOLO models but not RTDETR. Affects model accuracy and computational complexity.
save  bool  True  Enables saving of training checkpoints and final model weights. Useful for resuming training or model deployment.
save_period int -1  Frequency of saving model checkpoints, specified in epochs. A value of -1 disables this feature. Useful for saving interim models during long training sessions.
cache bool  False Enables caching of dataset images in memory (True/ram), on disk (disk), or disables it (False). Improves training speed by reducing disk I/O at the cost of increased memory usage.
device  int or str or list  None  Specifies the computational device(s) for training: a single GPU (device=0), multiple GPUs (device=[0,1]), CPU (device=cpu), MPS for Apple silicon (device=mps), or auto-selection of most idle GPU (device=-1) or multiple idle GPUs (device=[-1,-1])
workers int 8 Number of worker threads for data loading (per RANK if Multi-GPU training). Influences the speed of data preprocessing and feeding into the model, especially useful in multi-GPU setups.
project str None  Name of the project directory where training outputs are saved. Allows for organized storage of different experiments.
name  str None  Name of the training run. Used for creating a subdirectory within the project folder, where training logs and outputs are stored.
exist_ok  bool  False If True, allows overwriting of an existing project/name directory. Useful for iterative experimentation without needing to manually clear previous outputs.
pretrained  bool or str True  Determines whether to start training from a pretrained model. Can be a boolean value or a string path to a specific model from which to load weights. Enhances training efficiency and model performance.
optimizer str 'auto'  Choice of optimizer for training. Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for automatic selection based on model configuration. Affects convergence speed and stability.
seed  int 0 Sets the random seed for training, ensuring reproducibility of results across runs with the same configurations.
deterministic bool  True  Forces deterministic algorithm use, ensuring reproducibility but may affect performance and speed due to the restriction on non-deterministic algorithms.
single_cls  bool  False Treats all classes in multi-class datasets as a single class during training. Useful for binary classification tasks or when focusing on object presence rather than classification.
classes list[int] None  Specifies a list of class IDs to train on. Useful for filtering out and focusing only on certain classes during training.
rect  bool  False Enables minimum padding strategy‚Äîimages in a batch are minimally padded to reach a common size, with the longest side equal to imgsz. Can improve efficiency and speed but may affect model accuracy.
multi_scale bool  False Enables multi-scale training by increasing/decreasing imgsz by up to a factor of 0.5 during training. Trains the model to be more accurate with multiple imgsz during inference.
cos_lr  bool  False Utilizes a cosine learning rate scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.
close_mosaic  int 10  Disables mosaic data augmentation in the last N epochs to stabilize training before completion. Setting to 0 disables this feature.
resume  bool  False Resumes training from the last saved checkpoint. Automatically loads model weights, optimizer state, and epoch count, continuing training seamlessly.
amp bool  True  Enables Automatic Mixed Precision (AMP) training, reducing memory usage and possibly speeding up training with minimal impact on accuracy.
fraction  float 1.0 Specifies the fraction of the dataset to use for training. Allows for training on a subset of the full dataset, useful for experiments or when resources are limited.
profile bool  False Enables profiling of ONNX and TensorRT speeds during training, useful for optimizing model deployment.
freeze  int or list None  Freezes the first N layers of the model or specified layers by index, reducing the number of trainable parameters. Useful for fine-tuning or transfer learning.
lr0 float 0.01  Initial learning rate (i.e. SGD=1E-2, Adam=1E-3). Adjusting this value is crucial for the optimization process, influencing how rapidly model weights are updated.
lrf float 0.01  Final learning rate as a fraction of the initial rate = (lr0 * lrf), used in conjunction with schedulers to adjust the learning rate over time.
momentum  float 0.937 Momentum factor for SGD or beta1 for Adam optimizers, influencing the incorporation of past gradients in the current update.
weight_decay  float 0.0005  L2 regularization term, penalizing large weights to prevent overfitting.
warmup_epochs float 3.0 Number of epochs for learning rate warmup, gradually increasing the learning rate from a low value to the initial learning rate to stabilize training early on.
warmup_momentum float 0.8 Initial momentum for warmup phase, gradually adjusting to the set momentum over the warmup period.
warmup_bias_lr  float 0.1 Learning rate for bias parameters during the warmup phase, helping stabilize model training in the initial epochs.
box float 7.5 Weight of the box loss component in the loss function, influencing how much emphasis is placed on accurately predicting bounding box coordinates.
cls float 0.5 Weight of the classification loss in the total loss function, affecting the importance of correct class prediction relative to other components.
dfl float 1.5 Weight of the distribution focal loss, used in certain YOLO versions for fine-grained classification.
pose  float 12.0  Weight of the pose loss in models trained for pose estimation, influencing the emphasis on accurately predicting pose keypoints.
kobj  float 2.0 Weight of the keypoint objectness loss in pose estimation models, balancing detection confidence with pose accuracy.
nbs int 64  Nominal batch size for normalization of loss.
overlap_mask  bool  True  Determines whether object masks should be merged into a single mask for training, or kept separate for each object. In case of overlap, the smaller mask is overlaid on top of the larger mask during merge.
mask_ratio  int 4 Downsample ratio for segmentation masks, affecting the resolution of masks used during training.
dropout float 0.0 Dropout rate for regularization in classification tasks, preventing overfitting by randomly omitting units during training.
val bool  True  Enables validation during training, allowing for periodic evaluation of model performance on a separate dataset.
plots bool  False Generates and saves plots of training and validation metrics, as well as prediction examples, providing visual insights into model performance and learning progression.
compile bool or str False Enables PyTorch 2.x torch.compile graph compilation with backend='inductor'. Accepts True ‚Üí "default", False ‚Üí disables, or a string mode such as "default", "reduce-overhead", "max-autotune-no-cudagraphs". Falls back to eager with a warning if unsupported.
Note on Batch-size Settings

The batch argument can be configured in three ways:

Fixed Batch Size: Set an integer value (e.g., batch=16), specifying the number of images per batch directly.
Auto Mode (60% GPU Memory): Use batch=-1 to automatically adjust batch size for approximately 60% CUDA memory utilization.
Auto Mode with Utilization Fraction: Set a fraction value (e.g., batch=0.70) to adjust batch size based on the specified fraction of GPU memory usage.
Augmentation Settings and Hyperparameters
Augmentation techniques are essential for improving the robustness and performance of YOLO models by introducing variability into the training data, helping the model generalize better to unseen data. The following table outlines the purpose and effect of each augmentation argument:

Argument  Type  Default Supported Tasks Range Description
hsv_h float 0.015 detect, segment, pose, obb, classify  0.0 - 1.0 Adjusts the hue of the image by a fraction of the color wheel, introducing color variability. Helps the model generalize across different lighting conditions.
hsv_s float 0.7 detect, segment, pose, obb, classify  0.0 - 1.0 Alters the saturation of the image by a fraction, affecting the intensity of colors. Useful for simulating different environmental conditions.
hsv_v float 0.4 detect, segment, pose, obb, classify  0.0 - 1.0 Modifies the value (brightness) of the image by a fraction, helping the model to perform well under various lighting conditions.
degrees float 0.0 detect, segment, pose, obb  0.0 - 180 Rotates the image randomly within the specified degree range, improving the model's ability to recognize objects at various orientations.
translate float 0.1 detect, segment, pose, obb  0.0 - 1.0 Translates the image horizontally and vertically by a fraction of the image size, aiding in learning to detect partially visible objects.
scale float 0.5 detect, segment, pose, obb, classify  >=0.0 Scales the image by a gain factor, simulating objects at different distances from the camera.
shear float 0.0 detect, segment, pose, obb  -180 - +180 Shears the image by a specified degree, mimicking the effect of objects being viewed from different angles.
perspective float 0.0 detect, segment, pose, obb  0.0 - 0.001 Applies a random perspective transformation to the image, enhancing the model's ability to understand objects in 3D space.
flipud  float 0.0 detect, segment, pose, obb, classify  0.0 - 1.0 Flips the image upside down with the specified probability, increasing the data variability without affecting the object's characteristics.
fliplr  float 0.5 detect, segment, pose, obb, classify  0.0 - 1.0 Flips the image left to right with the specified probability, useful for learning symmetrical objects and increasing dataset diversity.
bgr float 0.0 detect, segment, pose, obb  0.0 - 1.0 Flips the image channels from RGB to BGR with the specified probability, useful for increasing robustness to incorrect channel ordering.
mosaic  float 1.0 detect, segment, pose, obb  0.0 - 1.0 Combines four training images into one, simulating different scene compositions and object interactions. Highly effective for complex scene understanding.
mixup float 0.0 detect, segment, pose, obb  0.0 - 1.0 Blends two images and their labels, creating a composite image. Enhances the model's ability to generalize by introducing label noise and visual variability.
cutmix  float 0.0 detect, segment, pose, obb  0.0 - 1.0 Combines portions of two images, creating a partial blend while maintaining distinct regions. Enhances model robustness by creating occlusion scenarios.
copy_paste  float 0.0 segment 0.0 - 1.0 Copies and pastes objects across images to increase object instances.
copy_paste_mode str flip  segment - Specifies the copy-paste strategy to use. Options include 'flip' and 'mixup'.
auto_augment  str randaugment classify  - Applies a predefined augmentation policy ('randaugment', 'autoaugment', or 'augmix') to enhance model performance through visual diversity.
erasing float 0.4 classify  0.0 - 0.9 Randomly erases regions of the image during training to encourage the model to focus on less obvious features.
These settings can be adjusted to meet the specific requirements of the dataset and task at hand. Experimenting with different values can help find the optimal augmentation strategy that leads to the best model performance.

Info

For more information about training augmentation operations, see the reference section.

Logging
In training a YOLO11 model, you might find it valuable to keep track of the model's performance over time. This is where logging comes into play. Ultralytics YOLO provides support for three types of loggers - Comet, ClearML, and TensorBoard.

To use a logger, select it from the dropdown menu in the code snippet above and run it. The chosen logger will be installed and initialized.

Comet
Comet is a platform that allows data scientists and developers to track, compare, explain and optimize experiments and models. It provides functionalities such as real-time metrics, code diffs, and hyperparameters tracking.

To use Comet:

Example


Python

# pip install comet_ml
import comet_ml

comet_ml.init()

Remember to sign in to your Comet account on their website and get your API key. You will need to add this to your environment variables or your script to log your experiments.

ClearML
ClearML is an open-source platform that automates tracking of experiments and helps with efficient sharing of resources. It is designed to help teams manage, execute, and reproduce their ML work more efficiently.

To use ClearML:

Example


Python

# pip install clearml
import clearml

clearml.browser_login()

After running this script, you will need to sign in to your ClearML account on the browser and authenticate your session.

TensorBoard
TensorBoard is a visualization toolkit for TensorFlow. It allows you to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.

To use TensorBoard in Google Colab:

Example


CLI

load_ext tensorboard
tensorboard --logdir ultralytics/runs # replace with 'runs' directory

To use TensorBoard locally run the below command and view results at http://localhost:6006/.

Example


CLI

tensorboard --logdir ultralytics/runs # replace with 'runs' directory

This will load TensorBoard and direct it to the directory where your training logs are saved.

After setting up your logger, you can then proceed with your model training. All training metrics will be automatically logged in your chosen platform, and you can access these logs to monitor your model's performance over time, compare different models, and identify areas for improvement.

FAQ
How do I train an object detection model using Ultralytics YOLO11?
To train an object detection model using Ultralytics YOLO11, you can either use the Python API or the CLI. Below is an example for both:

Single-GPU and CPU Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

For more details, refer to the Train Settings section.

What are the key features of Ultralytics YOLO11's Train mode?
The key features of Ultralytics YOLO11's Train mode include:

Automatic Dataset Download: Automatically downloads standard datasets like COCO, VOC, and ImageNet.
Multi-GPU Support: Scale training across multiple GPUs for faster processing.
Hyperparameter Configuration: Customize hyperparameters through YAML files or CLI arguments.
Visualization and Monitoring: Real-time tracking of training metrics for better insights.
These features make training efficient and customizable to your needs. For more details, see the Key Features of Train Mode section.

How do I resume training from an interrupted session in Ultralytics YOLO11?
To resume training from an interrupted session, set the resume argument to True and specify the path to the last saved checkpoint.

Resume Training Example


Python
CLI

from ultralytics import YOLO

# Load the partially trained model
model = YOLO("path/to/last.pt")

# Resume training
results = model.train(resume=True)

Check the section on Resuming Interrupted Trainings for more information.

Can I train YOLO11 models on Apple silicon chips?
Yes, Ultralytics YOLO11 supports training on Apple silicon chips utilizing the Metal Performance Shaders (MPS) framework. Specify 'mps' as your training device.

MPS Training Example


Python
CLI

from ultralytics import YOLO

# Load a pretrained model
model = YOLO("yolo11n.pt")

# Train the model on Apple silicon chip (M1/M2/M3/M4)
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device="mps")

For more details, refer to the Apple Silicon MPS Training section.

What are the common training settings, and how do I configure them?
Ultralytics YOLO11 allows you to configure a variety of training settings such as batch size, learning rate, epochs, and more through arguments. Here's a brief overview:

Argument  Default Description
model None  Path to the model file for training.
data  None  Path to the dataset configuration file (e.g., coco8.yaml).
epochs  100 Total number of training epochs.
batch 16  Batch size, adjustable as integer or auto mode.
imgsz 640 Target image size for training.
device  None  Computational device(s) for training like cpu, 0, 0,1, or mps.
save  True  Enables saving of training checkpoints and final model weights.
For an in-depth guide on training settings, check the Train Settings section.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 3 months ago
glenn-jocher
Laughing-q
UltralyticsAssistant
MatthewNoyce
Y-T-G
JairajJangle
jk4e
RizwanMunawar
dependabot
fcakyon
Burhan-Q

Tweet

Share

Comments
 Back to top
Previous
Ultralytics YOLO11 Modes
Next
Val
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Transcript


Search in video
Introduction
0:00
so in this video here we're going to see how we  can train a custom object detection model with  
0:03
YOLOv8 from Ultralytics we're going to see how  we can set it up we're also going to see how you  
0:08
can label your data with a specific tool you can  lose use like whatever tools that you want how can  
0:13
we act like export it into the YOLOv8 format  import into a Google Colab notebook and then  
0:17
we're going to train our own custom optic texting  model with YOLOv8 so let's now jump straight into  
0:22
Google Colab notebook let's see how we can set  it up it will be down in description here under  
0:26
the video so you can use it on your own custom  data set so first of all here we're basically  
0:31
just going to set up our Nvidia SMI just to see  what GPU we're running on if you want to use the  
0:36
GPU make sure that you go up to runtime change  random type and make sure that right like have  
0:41
chosen a dpu as the hardware accelerator so  yeah we have done that now we should be able  
0:45
to call this Nvidia is a my command here we go  we can see the information about our GPU right  
0:50
now this is just like the standard GPU in a Google  Colab notebook again you can use the free version  
0:55
as well and you can run this fine and train  your own custom YOLOv8 model from Ultralytics  
1:01
so now we're just going to pip install Ultralytics  here so now we can see that we've installed  
1:05
Ultralytics in our environment now we can go down  and install YOLOv8 or basically just set that up  
1:10
so from Ultralytics we're going to import the YOLO  model there we go now we have imported the YOLO  
1:15
model from Ultralytics now we basically just need  our data set so I'm using like roboflow in this  
Ô∏è Setting Up Environment
1:20
video to go in and label my data set you can use  whatever tool that you want I have the data set  
1:24
here let me just go in shortly to show you guys  that so I just have a data set with a number of  
1:29
different cups then I've just labeled it here with  this annotation tool I'm basically just drawing  
1:34
boundary boxes around it so you can just drag the  tool draw a bounding box around the object and  
1:38
then choose whatever class that you want to like  have your object in after we've done that we can  
1:44
then go in and Export our data set so I have some  versions already generated we hit export data set  
1:49
and then we can select the format we're going to  go with the YOLOv8 format from Ultralytics then  
1:54
we can hit show downloadable code so we just get a  code snippet and we can just copy paste that into  
2:00
our Google colab notebook so I've already copy  pasted it here you first of all you just need  
2:04
to copy paste in your API key then when we run  this we act like just download our data set and  
2:09
then we'll have our data set in our Google colab  environment and then we can just directly train  
2:14
our yellow V8 model on that so now we can see that  our data set has been downloaded we should be able  
2:19
to go over into our folder we can see we have  this cup detection version two free we can see  
2:24
we get a test set validation set and also a train  set so yeah we just have some test images we also  
2:29
have the labels here so this will basically like  be the pounding boxes we have the label and then  
2:34
we also basically have the top left corner and the  bottom right corner so this is the YOLOv8 format  
2:38
from Ultralytics if you go down to the data here  I just want to make sure that we actually have  
2:42
the correct path because we're using this in a  Google colab environment and we actually need to  
2:47
directory for our image just so I'm doing to copy  paste that there we go and we also have our train
2:56
you probably also need to do this here because  we're using a Google home app environment you  
3:00
can also have your data set in a Google Drive  and then just connect to your Google Drive then  
3:04
we basically have the path to our test set train  set and also our validation set we're just going  
3:08
to save it and now we should be able to run it  directly in our Google collab notebook so now we  
3:13
have this yellow command we're going to set the  task to detect for optic detection then we also  
3:18
have this mode we set that equal to train because  we want to train our own custom YOLOv8 model now  
3:23
for update detection we can choose which of the  yellow models that we want to use in this example  
3:28
I'm going to go with the media model and now we  also just need to specify the location to our data  
3:32
set so that will be the path to our data dot yaml  file that I just showed you and then we also need  
3:38
to specify the number of epochs and image size  of the images that we actually train our model on  
3:44
so now we can just directly go in and train our  YOLOv8 model so now we can see the training has  
Ô∏è Labeling Dataset
3:48
started for a custom YOLOv8 model they're just  going pretty fast because we're using the GPU here  
3:52
in Google collabs now we can go into the metrics  that we get while our model is training just to  
3:57
verify that we actually learn whatever you want to  I usually look at the mean error position so that  
4:02
is probably like the most important parameter we  both have the mean air position of 0.50 and also  
4:07
0.50 to 0.95 in intervals of 0.05 and those should  act like be increasing over your number of epochs  
4:15
and ideally they should be close to one okay so a  custom uv8 model is now done training we can see  
4:20
that we end up at our mean out position of 0.50 at  around like 0.81 82 and we also have like a meter  
4:27
position of 0.50 to 95 of around 0.5 you can just  directly go in and download the weights from here  
4:34
so in the next video I'm going to show you how we  can export these weights and basically just create  
4:38
another instance in our own custom python script  with a yellow V8 model and then use the custom  
4:44
trained one instead of a pre-trained one and also  how we can run live inference with our own custom  
4:49
trained yellow V8 model so here we're basically  just going to plot the confusion Matrix we can  
4:54
see that it acts like does a pretty good job with  our detection so ideally we should have all the  
4:59
values here in the diagonal in our confusion  Matrix let's take a look at the train results  
5:04
so these are all the graphs from our training  results for the metrics so we can see that the  
5:07
mean air position 0.50 is increasing now we can  go ahead and take a look at the validation we can  
5:12
choose the best model that we have at like  train and we specify the data set location  
5:16
for our validation set and then it will just do  validation on images that the model hasn't trained  
5:22
on before still look fine on our validation  set even though that these images have never  
5:26
been seen by the model before let's now do the  prediction mode so I'm directly just going to  
5:31
have a follow running through all the images  that we have in our detect and predict folder  
5:36
there we go now we should be able to see all the  predictions so sometimes we made some predictions  
5:40
but here we see that we're basically detecting  like all the cops with the correct labels here  
5:44
we made some detections here we get some nice  predictions again pretty high confidence score  
5:48
and we're only using the medium model so this is  how you can train your own custom YOLOv8 object  
5:52
detection model so thanks for watching this  video here I hope to see in the next one it'd  
5:56
be really exciting to run it live on a webcam  I hope to see in that video guys bye for now
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How Loss Functions Work
Common Types of Loss Functions
Real-World Applications
Relationship with Other Key Concepts
Glossary
Loss Function
Discover the role of loss functions in machine learning, their types, importance, and real-world AI applications like YOLO and object detection.

Flexible enterprise licensing solution to power your innovation
Get started
Train YOLO models with Ultralytics HUB
A loss function, also known as a cost function or objective function, is a fundamental component in machine learning (ML) and deep learning (DL). It quantifies the difference‚Äîor "loss"‚Äîbetween a model's predicted output and the actual ground truth label for a given piece of data. The value calculated by the loss function serves as a measure of how poorly the model is performing. The primary goal during the model training process is to minimize this value, thereby improving the model's accuracy and performance.

How Loss Functions Work
During each iteration of training, the model processes a batch of training data and makes predictions. The loss function then compares these predictions to the true labels. A higher loss value indicates a larger discrepancy and a greater need for correction, while a lower loss value signifies that the model's predictions are closer to the actual values.

This loss value is crucial because it provides the signal needed for the model to learn. This signal is used by an optimization algorithm, such as Stochastic Gradient Descent (SGD), to adjust the model's internal parameters, or model weights. The process of backpropagation calculates the gradient of the loss function with respect to these weights, indicating the direction in which the weights should be adjusted to reduce the loss. This iterative process of calculating loss and updating weights allows the model to gradually converge towards a state where it can make highly accurate predictions.

Common Types of Loss Functions
The choice of loss function depends heavily on the specific task the model is designed to solve. Different problems require different ways of measuring error. Some common types include:

Mean Squared Error (MSE): A popular loss function for regression tasks, where the goal is to predict a continuous numerical value. It calculates the average of the squares of the differences between the predicted and actual values.
Cross-Entropy Loss: Widely used for image classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1. It is effective when training models to distinguish between multiple classes, such as classifying images in the ImageNet dataset.
Intersection over Union (IoU) Loss: Variants of IoU are essential for object detection tasks. These loss functions, such as GIoU, DIoU, and CIoU, measure the discrepancy between the predicted bounding box and the ground truth box. They are integral to training accurate object detectors like Ultralytics YOLO11.
Dice Loss: Commonly used in image segmentation, especially in medical image analysis, to measure the overlap between predicted and actual segmentation masks. It is particularly useful for handling class imbalance.
Real-World Applications
Loss functions are at the core of training virtually every deep learning model.

Autonomous Vehicles: In the development of autonomous vehicles, object detection models are trained to identify pedestrians, other cars, and traffic signs. During training, a loss function combines multiple components: one part calculates the error in classifying each object (e.g., car vs. pedestrian), while another part, often an IoU-based loss, calculates the error in localizing the object's bounding box. Minimizing this combined loss helps create robust models for safe navigation, a key component of AI in automotive solutions.
Medical Diagnosis: In AI in healthcare, models like U-Net are trained for semantic segmentation to identify tumors in medical scans. A loss function such as Dice Loss or a combination of Cross-Entropy and Dice Loss is used to compare the model's predicted tumor mask with the mask annotated by a radiologist. By minimizing this loss on a dataset of medical images, the model learns to accurately delineate pathological regions, aiding in faster and more precise diagnoses.
Relationship with Other Key Concepts
It is important to differentiate loss functions from other related concepts in ML.

Loss Function vs. Evaluation Metrics: This is a crucial distinction. Loss functions are used during training to guide the optimization process. They must be differentiable to allow for gradient-based learning. In contrast, evaluation metrics like Accuracy, Precision, Recall, and mean Average Precision (mAP) are used after training (on validation data or test data) to assess a model's real-world performance. While a lower loss generally correlates with better metric scores, they serve different purposes. You can learn more about performance metrics in our guide.
Loss Function vs. Optimization Algorithm: The loss function defines the objective‚Äîwhat needs to be minimized. The optimization algorithm, such as the Adam optimizer, defines the mechanism‚Äîhow to minimize the loss by updating model weights based on the calculated gradients and the learning rate.
Overfitting and Underfitting: Monitoring the loss on both training and validation sets is key to diagnosing these common issues. Overfitting is likely occurring if the training loss continues to decrease while the validation loss begins to rise. Underfitting is indicated by high loss values on both sets. These insights are discussed in guides like our Tips for Model Training.
Understanding loss functions is essential for anyone involved in building and training AI models. Platforms like Ultralytics HUB abstract away much of this complexity, automatically handling loss function implementation and optimization, which makes building advanced computer vision (CV) models more accessible.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
‚ú® New course! Enroll in Building Live Voice Agents with Google‚Äôs ADK


Explore Courses
AI Newsletter
‚ú® AI Dev x NYC
Community
Company
Start Learning

A Complete Guide to
Natural Language Processing
Last updated on Jan 11, 2023
Table of Contents
Introduction
What is Natural Language Processing (NLP)
Why Does Natural Language Processing (NLP) Matter?
What is Natural Language Processing (NLP) Used For?
How Does Natural Language Processing (NLP) Work?
Top Natural Language Processing (NLP) Techniques
Six Important Natural Language Processing (NLP) Models
Programming Languages, Libraries, And Frameworks For Natural Language Processing (NLP)
Controversies Surrounding Natural Language Processing (NLP)
How To Get Started In Natural Language Processing (NLP)
Conclusion
Relevant Courses
Natural Language Processing Specialization
Machine Learning Specialization
Deep Learning Specialization
Introduction
Natural Language Processing (NLP) is one of the hottest areas of artificial intelligence (AI) thanks to applications like text generators that compose coherent essays, chatbots that fool people into thinking they‚Äôre sentient, and text-to-image programs that produce photorealistic images of anything you can describe. Recent years have brought a revolution in the ability of computers to understand human languages, programming languages, and even biological and chemical sequences, such as DNA and protein structures, that resemble language. The latest AI models are unlocking these areas to analyze the meanings of input text and generate meaningful, expressive output.

What is Natural Language Processing (NLP)
Natural language processing (NLP) is the discipline of building machines that can manipulate human language ‚Äî or data that resembles human language ‚Äî in the way that it is written, spoken, and organized. It evolved from computational linguistics, which uses computer science to understand the principles of language, but rather than developing theoretical frameworks, NLP is an engineering discipline that seeks to build technology to accomplish useful tasks. NLP can be divided into two overlapping subfields: natural language understanding (NLU), which focuses on semantic analysis or determining the intended meaning of text, and natural language generation (NLG), which focuses on text generation by a machine. NLP is separate from ‚Äî but often used in conjunction with ‚Äî speech recognition, which seeks to parse spoken language into words, turning sound into text and vice versa.

Why Does Natural Language Processing (NLP) Matter?
NLP is an integral part of everyday life and becoming more so as language technology is applied to diverse fields like retailing (for instance, in customer service chatbots) and medicine (interpreting or summarizing electronic health records). Conversational agents such as Amazon‚Äôs Alexa and Apple‚Äôs Siri utilize NLP to listen to user queries and find answers. The most sophisticated such agents ‚Äî such as GPT-3, which was recently opened for commercial applications ‚Äî can generate sophisticated prose on a wide variety of topics as well as power chatbots that are capable of holding coherent conversations. Google uses NLP to improve its search engine results, and social networks like Facebook use it to detect and filter hate speech. 

NLP is growing increasingly sophisticated, yet much work remains to be done. Current systems are prone to bias and incoherence, and occasionally behave erratically. Despite the challenges, machine learning engineers have many opportunities to apply NLP in ways that are ever more central to a functioning society.

What is Natural Language Processing (NLP) Used For?
NLP is used for a wide variety of language-related tasks, including answering questions, classifying text in a variety of ways, and conversing with users. 

Here are 11 tasks that can be solved by NLP:

Sentiment analysis is the process of classifying the emotional intent of text. Generally, the input to a sentiment classification model is a piece of text, and the output is the probability that the sentiment expressed is positive, negative, or neutral. Typically, this probability is based on either hand-generated features, word n-grams, TF-IDF features, or using deep learning models to capture sequential long- and short-term dependencies. Sentiment analysis is used to classify customer reviews on various online platforms as well as for niche applications like identifying signs of mental illness in online comments.
NLP sentiment analysis illustration
Toxicity classification is a branch of sentiment analysis where the aim is not just to classify hostile intent but also to classify particular categories such as threats, insults, obscenities, and hatred towards certain identities. The input to such a model is text, and the output is generally the probability of each class of toxicity. Toxicity classification models can be used to moderate and improve online conversations by silencing offensive comments, detecting hate speech, or scanning documents for defamation. 
Machine translation automates translation between different languages. The input to such a model is text in a specified source language, and the output is the text in a specified target language. Google Translate is perhaps the most famous mainstream application. Such models are used to improve communication between people on social-media platforms such as Facebook or Skype. Effective approaches to machine translation can distinguish between words with similar meanings. Some systems also perform language identification; that is, classifying text as being in one language or another. 
Named entity recognition aims to extract entities in a piece of text into predefined categories such as personal names, organizations, locations, and quantities. The input to such a model is generally text, and the output is the various named entities along with their start and end positions. Named entity recognition is useful in applications such as summarizing news articles and combating disinformation. For example, here is what a named entity recognition model could provide: 
named entity recognition NLP
Spam detection is a prevalent binary classification problem in NLP, where the purpose is to classify emails as either spam or not. Spam detectors take as input an email text along with various other subtexts like title and sender‚Äôs name. They aim to output the probability that the mail is spam. Email providers like Gmail use such models to provide a better user experience by detecting unsolicited and unwanted emails and moving them to a designated spam folder. 
Grammatical error correction models encode grammatical rules to correct the grammar within text. This is viewed mainly as a sequence-to-sequence task, where a model is trained on an ungrammatical sentence as input and a correct sentence as output. Online grammar checkers like Grammarly and word-processing systems like Microsoft Word use such systems to provide a better writing experience to their customers. Schools also use them to grade student essays. 
Topic modeling is an unsupervised text mining task that takes a corpus of documents and discovers abstract topics within that corpus. The input to a topic model is a collection of documents, and the output is a list of topics that defines words for each topic as well as assignment proportions of each topic in a document. Latent Dirichlet Allocation (LDA), one of the most popular topic modeling techniques, tries to view a document as a collection of topics and a topic as a collection of words. Topic modeling is being used commercially to help lawyers find evidence in legal documents. 
Text generation, more formally known as natural language generation (NLG), produces text that‚Äôs similar to human-written text. Such models can be fine-tuned to produce text in different genres and formats ‚Äî including tweets, blogs, and even computer code. Text generation has been performed using Markov processes, LSTMs, BERT, GPT-2, LaMDA, and other approaches. It‚Äôs particularly useful for autocomplete and chatbots.
Autocomplete predicts what word comes next, and autocomplete systems of varying complexity are used in chat applications like WhatsApp. Google uses autocomplete to predict search queries. One of the most famous models for autocomplete is GPT-2, which has been used to write articles, song lyrics, and much more. 
Chatbots automate one side of a conversation while a human conversant generally supplies the other side. They can be divided into the following two categories:
Database query: We have a database of questions and answers, and we would like a user to query it using natural language. 
Conversation generation: These chatbots can simulate dialogue with a human partner. Some are capable of engaging in wide-ranging conversations. A high-profile example is Google‚Äôs LaMDA, which provided such human-like answers to questions that one of its developers was convinced that it had feelings.
Information retrieval finds the documents that are most relevant to a query. This is a problem every search and recommendation system faces. The goal is not to answer a particular query but to retrieve, from a collection of documents that may be numbered in the millions, a set that is most relevant to the query. Document retrieval systems mainly execute two processes: indexing and matching. In most modern systems, indexing is done by a vector space model through Two-Tower Networks, while matching is done using similarity or distance scores. Google recently integrated its search function with a multimodal information retrieval model that works with text, image, and video data.
 
information retrieval illustration
Summarization is the task of shortening text to highlight the most relevant information. Researchers at Salesforce developed a summarizer that also evaluates factual consistency to ensure that its output is accurate. Summarization is divided into two method classes:
Extractive summarization focuses on extracting the most important sentences from a long text and combining these to form a summary. Typically, extractive summarization scores each sentence in an input text and then selects several sentences to form the summary.
Abstractive summarization produces a summary by paraphrasing. This is similar to writing the abstract that includes words and sentences that are not present in the original text. Abstractive summarization is usually modeled as a sequence-to-sequence task, where the input is a long-form text and the output is a summary.
Question answering deals with answering questions posed by humans in a natural language. One of the most notable examples of question answering was Watson, which in 2011 played the television game-show Jeopardy against human champions and won by substantial margins. Generally, question-answering tasks come in two flavors:
Multiple choice: The multiple-choice question problem is composed of a question and a set of possible answers. The learning task is to pick the correct answer. 
Open domain: In open-domain question answering, the model provides answers to questions in natural language without any options provided, often by querying a large number of texts.
How Does Natural Language Processing (NLP) Work?
NLP models work by finding relationships between the constituent parts of language ‚Äî for example, the letters, words, and sentences found in a text dataset. NLP architectures use various methods for data preprocessing, feature extraction, and modeling. Some of these processes are: 

Data preprocessing: Before a model processes text for a specific task, the text often needs to be preprocessed to improve model performance or to turn words and characters into a format the model can understand. Data-centric AI is a growing movement that prioritizes data preprocessing. Various techniques may be used in this data preprocessing:
Stemming and lemmatization: Stemming is an informal process of converting words to their base forms using heuristic rules. For example, ‚Äúuniversity,‚Äù ‚Äúuniversities,‚Äù and ‚Äúuniversity‚Äôs‚Äù might all be mapped to the base univers. (One limitation in this approach is that ‚Äúuniverse‚Äù may also be mapped to univers, even though universe and university don‚Äôt have a close semantic relationship.) Lemmatization is a more formal way to find roots by analyzing a word‚Äôs morphology using vocabulary from a dictionary. Stemming and lemmatization are provided by libraries like spaCy and NLTK. 
Sentence segmentation breaks a large piece of text into linguistically meaningful sentence units. This is obvious in languages like English, where the end of a sentence is marked by a period, but it is still not trivial. A period can be used to mark an abbreviation as well as to terminate a sentence, and in this case, the period should be part of the abbreviation token itself. The process becomes even more complex in languages, such as ancient Chinese, that don‚Äôt have a delimiter that marks the end of a sentence. 
Stop word removal aims to remove the most commonly occurring words that don‚Äôt add much information to the text. For example, ‚Äúthe,‚Äù ‚Äúa,‚Äù ‚Äúan,‚Äù and so on.
Tokenization splits text into individual words and word fragments. The result generally consists of a word index and tokenized text in which words may be represented as numerical tokens for use in various deep learning methods. A method that instructs language models to ignore unimportant tokens can improve efficiency.  
tokenizers NLP illustration
Feature extraction: Most conventional machine-learning techniques work on the features ‚Äì generally numbers that describe a document in relation to the corpus that contains it ‚Äì created by either Bag-of-Words, TF-IDF, or generic feature engineering such as document length, word polarity, and metadata (for instance, if the text has associated tags or scores). More recent techniques include Word2Vec, GLoVE, and learning the features during the training process of a neural network.
Bag-of-Words: Bag-of-Words counts the number of times each word or n-gram (combination of n words) appears in a document. For example, below, the Bag-of-Words model creates a numerical representation of the dataset based on how many of each word in the word_index occur in the document. 
tokenizers bag of words nlp
TF-IDF: In Bag-of-Words, we count the occurrence of each word or n-gram in a document. In contrast, with TF-IDF, we weight each word by its importance. To evaluate a word‚Äôs significance, we consider two things:
Term Frequency: How important is the word in the document?
TF(word in a document)= Number of occurrences of that word in document / Number of words in document

Inverse Document Frequency: How important is the term in the whole corpus?
IDF(word in a corpus)=log(number of documents in the corpus / number of documents that include the word)

A word is important if it occurs many times in a document. But that creates a problem. Words like ‚Äúa‚Äù and ‚Äúthe‚Äù appear often. And as such, their TF score will always be high. We resolve this issue by using Inverse Document Frequency, which is high if the word is rare and low if the word is common across the corpus. The TF-IDF score of a term is the product of TF and IDF. 

tokenizers tf idf illustration
Word2Vec, introduced in 2013, uses a vanilla neural network to learn high-dimensional word embeddings from raw text. It comes in two variations: Skip-Gram, in which we try to predict surrounding words given a target word, and Continuous Bag-of-Words (CBOW), which tries to predict the target word from surrounding words. After discarding the final layer after training, these models take a word as input and output a word embedding that can be used as an input to many NLP tasks. Embeddings from Word2Vec capture context. If particular words appear in similar contexts, their embeddings will be similar.
GLoVE is similar to Word2Vec as it also learns word embeddings, but it does so by using matrix factorization techniques rather than neural learning. The GLoVE model builds a matrix based on the global word-to-word co-occurrence counts. 
Modeling: After data is preprocessed, it is fed into an NLP architecture that models the data to accomplish a variety of tasks.
Numerical features extracted by the techniques described above can be fed into various models depending on the task at hand. For example, for classification, the output from the TF-IDF vectorizer could be provided to logistic regression, naive Bayes, decision trees, or gradient boosted trees. Or, for named entity recognition, we can use hidden Markov models along with n-grams. 
Deep neural networks typically work without using extracted features, although we can still use TF-IDF or Bag-of-Words features as an input. 
Language Models: In very basic terms, the objective of a language model is to predict the next word when given a stream of input words. Probabilistic models that use Markov assumption are one example:
P(Wn)=P(Wn|Wn‚àí1)

Deep learning is also used to create such language models. Deep-learning models take as input a word embedding and, at each time state, return the probability distribution of the next word as the probability for every word in the dictionary. Pre-trained language models learn the structure of a particular language by processing a large corpus, such as Wikipedia. They can then be fine-tuned for a particular task. For instance, BERT has been fine-tuned for tasks ranging from fact-checking to writing headlines. 

Top Natural Language Processing (NLP) Techniques
Most of the NLP tasks discussed above can be modeled by a dozen or so general techniques. It‚Äôs helpful to think of these techniques in two categories: Traditional machine learning methods and deep learning methods. 

Traditional Machine learning NLP techniques: 

Logistic regression is a supervised classification algorithm that aims to predict the probability that an event will occur based on some input. In NLP, logistic regression models can be applied to solve problems such as sentiment analysis, spam detection, and toxicity classification.
Naive Bayes is a supervised classification algorithm that finds the conditional probability distribution P(label | text) using the following Bayes formula:
P(label | text) = P(label) x P(text|label) / P(text) 

and predicts based on which joint distribution has the highest probability. The naive assumption in the Naive Bayes model is that the individual words are independent. Thus: 

P(text|label) = P(word_1|label)*P(word_2|label)*‚Ä¶P(word_n|label)

In NLP, such statistical methods can be applied to solve problems such as spam detection or finding bugs in software code. 

Decision trees are a class of supervised classification models that split the dataset based on different features to maximize information gain in those splits.
decision tree NLP techniques
Latent Dirichlet Allocation (LDA) is used for topic modeling. LDA tries to view a document as a collection of topics and a topic as a collection of words. LDA is a statistical approach. The intuition behind it is that we can describe any topic using only a small set of words from the corpus.
Hidden Markov models: Markov models are probabilistic models that decide the next state of a system based on the current state. For example, in NLP, we might suggest the next word based on the previous word. We can model this as a Markov model where we might find the transition probabilities of going from word1 to word2, that is, P(word1|word2). Then we can use a product of these transition probabilities to find the probability of a sentence. The hidden Markov model (HMM) is a probabilistic modeling technique that introduces a hidden state to the Markov model. A hidden state is a property of the data that isn‚Äôt directly observed. HMMs are used for part-of-speech (POS) tagging where the words of a sentence are the observed states and the POS tags are the hidden states. The HMM adds a concept called emission probability; the probability of an observation given a hidden state. In the prior example, this is the probability of a word, given its POS tag. HMMs assume that this probability can be reversed: Given a sentence, we can calculate the part-of-speech tag from each word based on both how likely a word was to have a certain part-of-speech tag and the probability that a particular part-of-speech tag follows the part-of-speech tag assigned to the previous word. In practice, this is solved using the Viterbi algorithm.
hidden markov models illustration
Deep learning NLP Techniques: 

Convolutional Neural Network (CNN): The idea of using a CNN to classify text was first presented in the paper ‚ÄúConvolutional Neural Networks for Sentence Classification‚Äù by Yoon Kim. The central intuition is to see a document as an image. However, instead of pixels, the input is sentences or documents represented as a matrix of words.
convolutional neural network based text classification
Recurrent Neural Network (RNN): Many techniques for text classification that use deep learning process words in close proximity using n-grams or a window (CNNs). They can see ‚ÄúNew York‚Äù as a single instance. However, they can‚Äôt capture the context provided by a particular text sequence. They don‚Äôt learn the sequential structure of the data, where every word is dependent on the previous word or a word in the previous sentence. RNNs remember previous information using hidden states and connect it to the current task. The architectures known as Gated Recurrent Unit (GRU) and long short-term memory (LSTM) are types of RNNs designed to remember information for an extended period. Moreover, the bidirectional LSTM/GRU keeps contextual information in both directions, which is helpful in text classification. RNNs have also been used to generate mathematical proofs and translate human thoughts into words. 
recurrent neural network illustration
Autoencoders are deep learning encoder-decoders that approximate a mapping from X to X, i.e., input=output. They first compress the input features into a lower-dimensional representation (sometimes called a latent code, latent vector, or latent representation) and learn to reconstruct the input. The representation vector can be used as input to a separate model, so this technique can be used for dimensionality reduction. Among specialists in many other fields, geneticists have applied autoencoders to spot mutations associated with diseases in amino acid sequences. 
auto-encoder
Encoder-decoder sequence-to-sequence: The encoder-decoder seq2seq architecture is an adaptation to autoencoders specialized for translation, summarization, and similar tasks. The encoder encapsulates the information in a text into an encoded vector. Unlike an autoencoder, instead of reconstructing the input from the encoded vector, the decoder‚Äôs task is to generate a different desired output, like a translation or summary. 
seq2seq illustration
Transformers: The transformer, a model architecture first described in the 2017 paper ‚ÄúAttention Is All You Need‚Äù (Vaswani, Shazeer, Parmar, et al.), forgoes recurrence and instead relies entirely on a self-attention mechanism to draw global dependencies between input and output. Since this mechanism processes all words at once (instead of one at a time) that decreases training speed and inference cost compared to RNNs, especially since it is parallelizable. The transformer architecture has revolutionized NLP in recent years, leading to models including BLOOM, Jurassic-X, and Turing-NLG. It has also been successfully applied to a variety of different vision tasks, including making 3D images.
encoder-decoder transformer
Six Important Natural Language Processing (NLP) Models
Over the years, many NLP models have made waves within the AI community, and some have even made headlines in the mainstream news. The most famous of these have been chatbots and language models. Here are some of them:

Eliza was developed in the mid-1960s to try to solve the Turing Test; that is, to fool people into thinking they‚Äôre conversing with another human being rather than a machine. Eliza used pattern matching and a series of rules without encoding the context of the language.
Tay was a chatbot that Microsoft launched in 2016. It was supposed to tweet like a teen and learn from conversations with real users on Twitter. The bot adopted phrases from users who tweeted sexist and racist comments, and Microsoft deactivated it not long afterward. Tay illustrates some points made by the ‚ÄúStochastic Parrots‚Äù paper, particularly the danger of not debiasing data.
BERT and his Muppet friends: Many deep learning models for NLP are named after Muppet characters, including ELMo, BERT, Big BIRD, ERNIE, Kermit, Grover, RoBERTa, and Rosita. Most of these models are good at providing contextual embeddings and enhanced knowledge representation.
Generative Pre-Trained Transformer 3 (GPT-3) is a 175 billion parameter model that can write original prose with human-equivalent fluency in response to an input prompt. The model is based on the transformer architecture. The previous version, GPT-2, is open source. Microsoft acquired an exclusive license to access GPT-3‚Äôs underlying model from its developer OpenAI, but other users can interact with it via an application programming interface (API). Several groups including EleutherAI and Meta have released open source interpretations of GPT-3. 
Language Model for Dialogue Applications (LaMDA) is a conversational chatbot developed by Google. LaMDA is a transformer-based model trained on dialogue rather than the usual web text. The system aims to provide sensible and specific responses to conversations. Google developer Blake Lemoine came to believe that LaMDA is sentient. Lemoine had detailed conversations with AI about his rights and personhood. During one of these conversations, the AI changed Lemoine‚Äôs mind about Isaac Asimov‚Äôs third law of robotics. Lemoine claimed that LaMDA was sentient, but the idea was disputed by many observers and commentators. Subsequently, Google placed Lemoine on administrative leave for distributing proprietary information and ultimately fired him.
Mixture of Experts (MoE): While most deep learning models use the same set of parameters to process every input, MoE models aim to provide different parameters for different inputs based on efficient routing algorithms to achieve higher performance. Switch Transformer is an example of the MoE approach that aims to reduce communication and computational costs.
Programming Languages, Libraries, And Frameworks For Natural Language Processing (NLP)
Many languages and libraries support NLP. Here are a few of the most useful.

Python is the most-used programming language to tackle NLP tasks. Most libraries and frameworks for deep learning are written for Python. Here are a few that practitioners may find helpful:
Natural Language Toolkit (NLTK) is one of the first NLP libraries written in Python. It provides easy-to-use interfaces to corpora and lexical resources such as WordNet. It also provides a suite of text-processing libraries for classification, tagging, stemming, parsing, and semantic reasoning.
spaCy is one of the most versatile open source NLP libraries. It supports more than 66 languages. spaCy also provides pre-trained word vectors and implements many popular models like BERT. spaCy can be used for building production-ready systems for named entity recognition, part-of-speech tagging, dependency parsing, sentence segmentation, text classification, lemmatization, morphological analysis, entity linking, and so on.
Deep Learning libraries: Popular deep learning libraries include TensorFlow and PyTorch, which make it easier to create models with features like automatic differentiation. These libraries are the most common tools for developing NLP models.
Hugging Face offers open-source implementations and weights of over 135 state-of-the-art models. The repository enables easy customization and training of the models.
Gensim provides vector space modeling and topic modeling algorithms.
R: Many early NLP models were written in R, and R is still widely used by data scientists and statisticians. Libraries in R for NLP include TidyText, Weka, Word2Vec, SpaCyR, TensorFlow, and PyTorch.
Many other languages including JavaScript, Java, and Julia have libraries that implement NLP methods.
Controversies Surrounding Natural Language Processing (NLP)
NLP has been at the center of a number of controversies. Some are centered directly on the models and their outputs, others on second-order concerns, such as who has access to these systems, and how training them impacts the natural world. 

Stochastic parrots: A 2021 paper titled ‚ÄúOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big?‚Äù by Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell examines how language models may repeat and amplify biases found in their training data. The authors point out that huge, uncurated datasets scraped from the web are bound to include social biases and other undesirable information, and models that are trained on them will absorb these flaws. They advocate greater care in curating and documenting datasets, evaluating a model‚Äôs potential impact prior to development, and encouraging research in directions other than designing ever-larger architectures to ingest ever-larger datasets.
Coherence versus sentience: Recently, a Google engineer tasked with evaluating the LaMDA language model was so impressed by the quality of its chat output that he believed it to be sentient. The fallacy of attributing human-like intelligence to AI dates back to some of the earliest NLP experiments. 
Environmental impact: Large language models require a lot of energy during both training and inference. One study estimated that training a single large language model can emit five times as much carbon dioxide as a single automobile over its operational lifespan. Another study found that models consume even more energy during inference than training. As for solutions, researchers have proposed using cloud servers located in countries with lots of renewable energy as one way to offset this impact. 
High cost leaves out non-corporate researchers: The computational requirements needed to train or deploy large language models are too expensive for many small companies. Some experts worry that this could block many capable engineers from contributing to innovation in AI. 
Black box: When a deep learning model renders an output, it‚Äôs difficult or impossible to know why it generated that particular result. While traditional models like logistic regression enable engineers to examine the impact on the output of individual features, neural network methods in natural language processing are essentially black boxes. Such systems are said to be ‚Äúnot explainable,‚Äù since we can‚Äôt explain how they arrived at their output. An effective approach to achieve explainability is especially important in areas like banking, where regulators want to confirm that a natural language processing system doesn‚Äôt discriminate against some groups of people, and law enforcement, where models trained on historical data may perpetuate historical biases against certain groups.
‚ÄúNonsense on stilts‚Äù: Writer Gary Marcus has criticized deep learning-based NLP for generating sophisticated language that misleads users to believe that natural language algorithms understand what they are saying and mistakenly assume they are capable of more sophisticated reasoning than is currently possible.

How To Get Started In Natural Language Processing (NLP)
If you are just starting out, many excellent courses can help.

Course
Machine Learning Specialization
A foundational set of three courses that introduces beginners to the fundamentals of learning algorithms. Prerequisites include high-school math and basic programming skills
View Course
Course
Deep Learning Specialization
An intermediate set of five courses that help learners get hands-on experience building and deploying neural networks, the technology at the heart of today‚Äôs most advanced NLP and other sorts of AI models.
View Course
Course
Natural Language Processing Specialization
An intermediate set of four courses that provide learners with the theory and application behind the most relevant and widely used NLP models.
View Course
If you want to learn more about NLP, try reading research papers. Work through the papers that introduced the models and techniques described in this article. Most are easy to find on arxiv.org. You might also take a look at these resources: 

The Batch: A weekly newsletter that tells you what matters in AI. It‚Äôs the best way to keep up with developments in deep learning.
NLP News: A newsletter from Sebastian Ruder, a research scientist at Google, focused on what‚Äôs new in NLP. 
Papers with Code: A web repository of machine learning research, tasks, benchmarks, and datasets.
We highly recommend learning to implement basic algorithms (linear and logistic regression, Naive Bayes, decision trees, and vanilla neural networks) in Python. The next step is to take an open-source implementation and adapt it to a new dataset or task. 

Conclusion
NLP is one of the fast-growing research domains in AI, with applications that involve tasks including translation, summarization, text generation, and sentiment analysis. Businesses use NLP to power a growing number of applications, both internal ‚Äî like detecting insurance fraud, determining customer sentiment, and optimizing aircraft maintenance ‚Äî and customer-facing, like Google Translate. 

Aspiring NLP practitioners can begin by familiarizing themselves with foundational AI skills: performing basic mathematics, coding in Python, and using algorithms like decision trees, Naive Bayes, and logistic regression. Online courses can help you build your foundation. They can also help as you proceed into specialized topics. Specializing in NLP requires a working knowledge of things like neural networks, frameworks like PyTorch and TensorFlow, and various data preprocessing techniques. The transformer architecture, which has revolutionized the field since it was introduced in 2017, is an especially important architecture.

NLP is an exciting and rewarding discipline, and has potential to profoundly impact the world in many positive ways. Unfortunately, NLP is also the focus of several controversies, and understanding them is also part of being a responsible practitioner. For instance, researchers have found that models will parrot biased language found in their training data, whether they‚Äôre counterfactual, racist, or hateful. Moreover, sophisticated language models can be used to generate disinformation. A broader concern is that training large models produces substantial greenhouse gas emissions.

This page is only a brief overview of what NLP is all about. If you have an appetite for more, DeepLearning.AI offers courses for everyone in their NLP journey, from AI beginners and those who are ready to specialize. No matter your current level of expertise or aspirations, remember to keep learning!

Courses
The Batch
Community
Careers
About
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How Stochastic Gradient Descent Works
Sgd Vs. Other Optimizers
Real-World Applications
Glossary
Stochastic Gradient Descent (SGD)
Discover how Stochastic Gradient Descent optimizes machine learning models, enabling efficient training for large datasets and deep learning tasks.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Stochastic Gradient Descent (SGD) is a fundamental and widely used optimization algorithm in machine learning (ML). It is an iterative method used to train models by adjusting their internal parameters, such as weights and biases, to minimize a loss function. Unlike traditional Gradient Descent, which processes the entire dataset for each update, SGD updates the parameters using just a single, randomly selected training sample. This "stochastic" approach makes the training process significantly faster and more scalable, which is especially important when working with big data. The noisy updates can also help the model escape poor local minima in the error landscape and potentially find a better overall solution.

How Stochastic Gradient Descent Works
The core idea behind SGD is to approximate the true gradient of the loss function, which is calculated over the entire dataset, by using the gradient of the loss for a single sample. While this single-sample gradient is a noisy estimate, it is computationally cheap and, on average, points in the right direction. The process involves repeating a simple two-step cycle for each training sample:

Calculate the Gradient: Compute the gradient of the loss function with respect to the model's parameters for a single training example.
Update the Parameters: Adjust the parameters in the opposite direction of the gradient, scaled by a learning rate. This moves the model toward a state with lower error for that specific sample.
This cycle is repeated for many passes over the entire dataset, known as epochs, gradually improving the model's performance. The efficiency of SGD has made it a cornerstone of modern deep learning (DL), and it is supported by all major frameworks like PyTorch and TensorFlow.

Sgd Vs. Other Optimizers
SGD is one of several gradient-based optimization methods, each with its own trade-offs.

Batch Gradient Descent: This method calculates the gradient using the entire training dataset. It provides a stable and direct path to the minimum but is extremely slow and memory-intensive for large datasets, making it impractical for most modern applications.
Mini-Batch Gradient Descent: This is a compromise between Batch GD and SGD. It updates parameters using a small, random subset (a "mini-batch") of the data. It balances the stability of Batch GD with the efficiency of SGD and is the most common approach used in practice.
Adam Optimizer: Adam is an adaptive optimization algorithm that maintains a separate learning rate for each parameter and adjusts it as learning progresses. It often converges faster than SGD, but SGD can sometimes find a better minimum and offer better generalization, helping to prevent overfitting.
Real-World Applications
SGD and its variants are critical for training a wide array of AI models across different domains.

Real-time Object Detection Training: For models like Ultralytics YOLO designed for real-time inference, training needs to be efficient. SGD allows developers to train these models on large image datasets like COCO or custom datasets managed via platforms like Ultralytics HUB. The rapid updates enable faster convergence compared to Batch GD, crucial for iterating quickly during model development and hyperparameter tuning. This efficiency supports applications in fields like autonomous vehicles and robotics.
Training Large Language Models (LLMs): Training models for Natural Language Processing (NLP) often involves massive text datasets. SGD and its variants are essential for iterating through this data efficiently, allowing models such as GPT-4 or those found on Hugging Face to learn grammar, context, and semantics. The stochastic nature helps escape poor local minima in the complex loss landscape, a common challenge in training large neural networks. This process is foundational to tasks like machine translation and sentiment analysis.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
The Role of Batch Size in Model Training
Choosing the Right Batch Size
Batch Size in Training vs. Inference
Real-World Applications
Batch Size vs. Related Terms
Glossary
Batch Size
Discover the impact of batch size on deep learning. Optimize training speed, memory usage, and model performance efficiently.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Batch size is a fundamental hyperparameter in machine learning that defines the number of training samples processed before the model's internal parameters are updated. Instead of processing the entire training dataset at once, which can be computationally prohibitive, the data is divided into smaller subsets or "batches." The choice of batch size is a critical decision that directly impacts the model's learning dynamics, training speed, and final performance. It represents a trade-off between computational efficiency and the accuracy of the gradient estimate used to update the model weights.

The Role of Batch Size in Model Training
During training, a neural network (NN) learns by adjusting its weights based on the error it makes. This adjustment is guided by an optimization algorithm like gradient descent. The batch size determines how many examples the model "sees" before it calculates the gradient and performs a weight update.

Stochastic Gradient Descent (SGD): When the batch size is 1, the process is called stochastic gradient descent. The gradient is calculated for each individual sample, leading to frequent but noisy updates.
Batch Gradient Descent: When the batch size equals the total number of samples in the training dataset, it's known as batch gradient descent. This provides a very accurate gradient estimate but is computationally expensive and memory-intensive.
Mini-Batch Gradient Descent: This is the most common approach, where the batch size is set to a value between 1 and the total dataset size (e.g., 32, 64, 128). It offers a balance between the stability of batch gradient descent and the efficiency of stochastic gradient descent.
The selection of batch size influences the training process significantly. A larger batch size provides a more accurate estimate of the gradient, but the computational cost for each update is higher. Conversely, a smaller batch size leads to less accurate gradient estimates but allows for more rapid updates.

Choosing the Right Batch Size
Finding the optimal batch size is a crucial part of hyperparameter tuning and depends on the dataset, model architecture, and available hardware.

Large Batch Sizes: Processing more data at once can fully leverage the parallel processing capabilities of GPUs, leading to faster training times per epoch. However, research has shown that very large batches can sometimes lead to a "generalization gap," where the model performs well on the training data but poorly on unseen data. They also require significant memory, which can be a limiting factor.
Small Batch Sizes: These require less memory and often lead to better model generalization, as the noise in the gradient updates can help the model escape local minima and find a more robust solution. This can help prevent overfitting. The primary downside is that training is slower because weight updates are more frequent and less data is processed in parallel.
For many applications, batch sizes that are powers of two (like 32, 64, 128, 256) are recommended as they often align well with GPU memory architectures. Tools like Ultralytics HUB allow for easy experimentation with different batch sizes when training models.

Batch Size in Training vs. Inference
While batch size is a core concept in training, it also applies to inference, but with a different purpose. During inference, batching is used to process multiple inputs (e.g., images or sentences) simultaneously to maximize throughput. This is often referred to as batch inferencing.

For applications requiring immediate results, such as real-time inference in an autonomous vehicle, a batch size of 1 is used to minimize inference latency. In offline scenarios, like processing a large collection of images overnight, a larger batch size can be used to improve efficiency.

Real-World Applications
Medical Imaging Analysis: When training a YOLO11 model for tumor detection in medical images, the images are often high-resolution. Due to memory constraints on a GPU, a small batch size (e.g., 4 or 8) is typically used. This allows the model to be trained on high-detail data without exceeding available memory, ensuring stable training.
Manufacturing Quality Control: In an AI in manufacturing setting, a model might be trained to detect defects on an assembly line. With a large dataset of millions of product images, a larger batch size (e.g., 256 or 512) might be used on a powerful distributed training cluster. This speeds up the training process, allowing for faster model iteration and deployment.
Batch Size vs. Related Terms
It's important to distinguish batch size from other related concepts:

Batch Size vs. Epoch and Iteration: An iteration is one update of the model's weights. An epoch is one full pass over the entire training dataset. The number of iterations in an epoch is the total number of training samples divided by the batch size.
Batch Size vs. Batch Normalization: Batch Normalization (BatchNorm) is a technique used within a neural network layer to standardize the inputs for each mini-batch. While its effectiveness can be influenced by the batch size (it performs better with larger batches), it is a distinct layer in the model architecture, not a training loop parameter. Most modern deep learning frameworks like PyTorch and TensorFlow provide robust implementations.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How Backpropagation Works
Backpropagation vs. Related Concepts
Real-World Applications
Glossary
Backpropagation
Learn how backpropagation trains neural networks, reduces error rates, and powers AI applications like image recognition and NLP efficiently.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Backpropagation, short for "backward propagation of errors," is the fundamental algorithm used to train artificial neural networks. It works by calculating the gradient of the loss function with respect to each weight in the network, allowing the model to learn from its mistakes. This process is the cornerstone of modern deep learning, enabling models to tackle complex tasks by iteratively adjusting their internal parameters to improve performance. The development of backpropagation was a pivotal moment in the history of AI, transforming neural networks from a theoretical concept into powerful, practical tools.

How Backpropagation Works
The process of backpropagation is central to the model training loop and can be understood as a two-phase cycle that repeats for each batch of data:

Forward Pass: The training data is fed into the network. Each neuron receives inputs, processes them using its model weights and an activation function, and passes the output to the next layer. This continues until the final layer produces a prediction. The model's prediction is then compared to the ground truth (the correct labels) using a loss function, which calculates an error score quantifying how wrong the prediction was.

Backward Pass: This is where backpropagation begins. It starts at the final layer and propagates the error backward through the network, layer by layer. At each neuron, it uses calculus (specifically, the chain rule) to calculate how much that neuron's weights and biases contributed to the total error. This contribution is known as the gradient. The gradients effectively tell the model how to adjust each weight to reduce the error. An optimization algorithm then uses these gradients to update the weights.

This cycle of forward and backward passes is repeated for many epochs, allowing the model to gradually minimize its error and improve its accuracy. Frameworks like PyTorch and TensorFlow have highly optimized, automatic differentiation engines that handle the complex calculus of backpropagation behind the scenes.

Backpropagation vs. Related Concepts
It is important to distinguish backpropagation from other related concepts in machine learning:

Optimization Algorithm: Backpropagation is the method for calculating the gradients of the loss with respect to the model's parameters. An optimization algorithm, such as Stochastic Gradient Descent (SGD) or the Adam optimizer, is the mechanism that uses these gradients to update the model's weights. Think of backpropagation as providing the map, and the optimizer as driving the car.
Loss Function: A loss function measures the error between the model's predictions and the true values. Backpropagation uses this error score as the starting point to calculate the gradients. The choice of loss function is critical, but it is a separate component from the backpropagation algorithm itself.
Vanishing and Exploding Gradients: These are problems that can occur during backpropagation in deep networks. A vanishing gradient occurs when gradients become extremely small, preventing early layers from learning. Conversely, an exploding gradient happens when gradients become excessively large, leading to unstable training. Techniques like careful weight initialization, normalization, and using activation functions like ReLU are used to mitigate these issues.
Real-World Applications
Backpropagation is implicitly used whenever a deep learning model undergoes training. Here are two concrete examples:

Object Detection with Ultralytics YOLO: When training an Ultralytics YOLO model (like YOLO11) for object detection on a dataset such as COCO, backpropagation is used in each training iteration. After the model predicts bounding boxes and classes, the loss is calculated. Backpropagation computes the gradients for all weights throughout the model's backbone and detection head. An optimizer then uses these gradients to adjust the weights, improving the model's ability to accurately locate and classify objects. Users can leverage platforms like Ultralytics HUB to manage this training process, benefiting from efficient backpropagation implementations. This is crucial for applications ranging from autonomous vehicles to security systems.
Natural Language Processing Models: Large language models (LLMs) like BERT and GPT models are trained using backpropagation. For instance, in a sentiment analysis task, the model predicts the sentiment of a given text. The difference between the predicted sentiment and the actual label results in an error value. Backpropagation calculates how much each parameter in the vast network contributed to this error. Optimization algorithms then update these parameters, enabling the model to better understand linguistic nuances, context, and sentiment over the course of training. Academic research groups like the Stanford NLP group continuously explore and refine these techniques.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
Hyperparameter Tuning vs. Related Concepts
Common Tuning Methods and Hyperparameters
Real-World Applications
Hyperparameter Tuning with Ultralytics
Glossary
Hyperparameter Tuning
Master hyperparameter tuning to optimize ML models like Ultralytics YOLO. Boost accuracy, speed, and performance with expert techniques.

Flexible enterprise licensing solution to power your innovation
Get started
Train YOLO models with Ultralytics HUB
Hyperparameter tuning is the process of finding the optimal configuration settings for a Machine Learning (ML) model. These settings, known as hyperparameters, are external to the model and cannot be learned directly from the data during the training process. Instead, they are set before training begins and control how the training process itself behaves. Effectively tuning these hyperparameters is a critical step in maximizing model performance and ensuring it generalizes well to new, unseen data. Without proper tuning, even the most advanced model architecture can underperform.

Hyperparameter Tuning vs. Related Concepts
It's important to differentiate hyperparameter tuning from other key concepts in ML:

Optimization Algorithm: An optimization algorithm, like Adam or Stochastic Gradient Descent (SGD), is the engine that adjusts the model's internal parameters (weights and biases) during training to minimize the loss function. Hyperparameter tuning, in contrast, involves selecting the best external settings, which can even include the choice of the optimization algorithm itself.
Neural Architecture Search (NAS): While hyperparameter tuning optimizes the settings for a given model structure, NAS automates the design of the model architecture itself, such as determining the number and type of layers. Both are forms of Automated Machine Learning (AutoML) and are often used together to build the best possible model.
Model Parameters: These are the internal variables of a model, such as the weights and biases in a neural network, that are learned from the training data through backpropagation. Hyperparameters are the higher-level settings that govern how these parameters are learned.
Common Tuning Methods and Hyperparameters
Practitioners use several strategies to find the best hyperparameter values. Common methods include Grid Search, which exhaustively tries every combination of specified values, Random Search, which samples combinations randomly, and more advanced methods like Bayesian Optimization and Evolutionary Algorithms.

Some of the most frequently tuned hyperparameters include:

Learning Rate: Controls how much the model's weights are adjusted with respect to the loss gradient.
Batch Size: The number of training examples utilized in one iteration.
Number of Epochs: The number of times the entire training dataset is passed through the model.
Data Augmentation Intensity: The degree of transformations applied to the training data, such as rotation, scaling, or color shifts. The Albumentations library is a popular tool for this.
Real-World Applications
Hyperparameter tuning is applied across various domains to achieve peak performance:

Medical Image Analysis: When training a model for tumor detection, tuning hyperparameters like the learning rate schedule, data augmentation settings, and loss function weights is crucial for achieving high detection accuracy on specific medical datasets. This is vital for reliable AI in Healthcare solutions and is a subject of ongoing research.
Autonomous Vehicles: Object detection models in self-driving cars require careful tuning. Optimizing hyperparameters such as input image resolution, Non-Maximum Suppression (NMS) thresholds, and anchor box configurations ensures the system can reliably detect pedestrians and obstacles with low latency for safe navigation. This tuning is critical for companies like Waymo and contributes to robust AI in Automotive solutions.
Hyperparameter Tuning with Ultralytics
Ultralytics provides tools to simplify hyperparameter tuning for Ultralytics YOLO models. The Ultralytics Tuner class, documented in the Hyperparameter Tuning guide, automates the process using evolutionary algorithms. Integration with platforms like Ray Tune offers further capabilities for distributed and advanced search strategies, helping users optimize their models efficiently for specific datasets (like COCO) and tasks. Users can leverage platforms like Ultralytics HUB for streamlined experiment tracking and management, which is often a key part of following best practices for model training. Popular open-source libraries like Optuna and Hyperopt are also widely used in the ML community for this purpose.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
Underfitting Vs. Overfitting
Causes and Solutions for Underfitting
Real-World Examples of Underfitting
Glossary
Underfitting
Learn how to identify, prevent, and address underfitting in machine learning models with expert tips, strategies, and real-world examples.

Flexible enterprise licensing solution to power your innovation
Get started
Train YOLO models with Ultralytics HUB
Underfitting is a common issue in machine learning (ML) where a model is too simple to capture the underlying patterns in the training data. This simplicity prevents it from learning the relationship between the input features and the target variable, leading to poor performance on both the data it was trained on and new, unseen data. An underfit model has high bias, meaning it makes strong, often incorrect, assumptions about the data. This results in a model that fails to achieve a high level of accuracy and cannot generalize well.

Underfitting Vs. Overfitting
Underfitting and overfitting are two key challenges in ML that relate to a model's ability to generalize from training data to new data. They represent two extremes on the spectrum of model complexity.

Underfitting: The model is too simple and has high bias. It fails to learn the underlying structure of the data, resulting in a high loss function value and poor performance on both the training and validation datasets.
Overfitting: The model is too complex and has high variance. It learns the training data too well, including the noise and random fluctuations. This results in excellent performance on the training set but poor performance on unseen data, as the model has essentially memorized the training examples instead of learning general patterns.
The ultimate goal in ML is to strike a balance between these two, a concept known as the bias-variance tradeoff, to create a model that generalizes effectively to new, real-world scenarios. Analyzing learning curves is a common method for diagnosing whether a model is underfitting, overfitting, or well-fitted.

Causes and Solutions for Underfitting
Identifying and addressing underfitting is crucial for building effective models. The problem typically stems from a few common causes, each with corresponding solutions.

Model is Too Simple: Using a linear model for a complex, non-linear problem is a classic cause of underfitting.
Solution: Increase model complexity. This could involve switching to a more powerful model architecture, such as a deeper neural network or a larger pre-trained model like moving from a smaller to a larger Ultralytics YOLO model variant. You can explore various YOLO model comparisons to select a more suitable architecture.
Insufficient or Poor-Quality Features: If the input features provided to the model do not contain enough information to make accurate predictions, the model will underfit.
Solution: Improve the features through feature engineering. This might involve creating new features, using polynomial features, or applying different data preprocessing techniques to better represent the underlying patterns. Feature selection techniques can also be applied.
Insufficient Training: The model may not have been trained for enough epochs to learn the patterns in the data.
Solution: Increase the training duration. It's important to monitor validation metrics to ensure that longer training doesn't lead to overfitting. Tools like Ultralytics HUB can help you track and manage your training experiments.
Excessive Regularization: Techniques like L1 and L2 regularization or high dropout rates are used to prevent overfitting, but if they are too aggressive, they can constrain the model too much and cause underfitting.
Solution: Reduce the amount of regularization. This might mean lowering the penalty term in regularization functions or reducing the dropout rate. Following best practices for model training can help find the right balance.
Real-World Examples of Underfitting
Simple Image Classifier: Imagine training a very basic Convolutional Neural Network (CNN) with only one or two layers on a complex image classification task, such as identifying thousands of object categories in the ImageNet dataset. The model's limited capacity would prevent it from learning the intricate features needed to distinguish between so many classes, resulting in low accuracy on both training and test data. Frameworks like PyTorch and TensorFlow provide the tools to build more sophisticated architectures to overcome this.
Basic Predictive Maintenance: Consider using a simple linear regression model for predictive modeling to estimate when a machine will fail based only on its operating temperature. If machine failures are actually influenced by a complex, non-linear interplay of factors like vibration, age, and pressure, the simple linear model will underfit. It cannot capture the true complexity of the system, leading to poor predictive performance and an inability to anticipate failures accurately. A more complex model, like a gradient boosting machine or a neural network, would be more appropriate.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Skip to content
Towards Data Science
Publish AI, ML & data-science insights to a global community of data professionals.

Sign in
Submit an Article
Latest
Editor‚Äôs Picks
Deep Dives
Newsletter
Write For TDS
LinkedIn
X

Toggle Search
Understand your data with principal component analysis (PCA) and discover underlying patterns
Enhanced data exploration that goes beyond descriptives

Philipp Schmalen
Aug 16, 2020
15 min read
Share
Save time, resources and stay healthy with data exploration that goes beyond means, distributions and correlations: Leverage PCA to see through the surface of variables. It saves time and resources, because it uncovers data issues before an hour-long model training and is good for a programmer‚Äôs health, since she trades off data worries with something more enjoyable. For example, a well-proven machine learning model might fail, because of one-dimensional data with insufficient variance or other related issues. PCA offers valuable insights that make you confident about data properties and its hidden dimensions.

This article shows how to leverage PCA to understand key properties of a dataset, saving time and resources down the road which ultimately leads to a happier, more fulfilled coding life. I hope this post helps to apply PCA in a consistent way and understand its results.

Photo by Jonathan Borba
Photo by Jonathan Borba
TL;DR
PCA provides valuable insights that reach beyond descriptive statistics and help to discover underlying patterns. Two PCA metrics indicate 1. how many components capture the largest share of variance (explained variance), and 2., which features correlate with the most important components (factor loading). These metrics crosscheck previous steps in the project work flow, such as data collection which then can be adjusted. As a shortcut and ready-to-use tool, I provide the function do_pca() which conducts a PCA for a prepared dataset to inspect its results within seconds in this notebook or this script.

Data exploration as a safety net
When a project structure resembles the one below, the prepared dataset is under scrutiny in the 4. step by looking at descriptive statistics. Among the most common ones are means, distributions and correlations taken across all observations or subgroups.

Common project structure

Collection: gather, retrieve or load data
Processing: Format raw data, handle missing entries
Engineering: Construct and select features
Exploration: Inspect descriptives, properties
Modelling: Train, validate and test models
Evaluation: Inspect results, compare models
When the moment arrives of having a clean dataset after hours of work, makes many glances already towards the exciting step of applying models to the data. At this stage, around 80‚Äì90% of the project‚Äôs workload is done, if the data did not fell out of the sky, cleaned and processed. Of course, the urge is strong for modeling, but here are **** two reasons why a thorough data exploration saves time down the road:

catch coding errors ‚Üí revise feature engineering (step 3)
identify underlying properties ‚Üí rethink data collection (step 1), preprocessing (step 2) or feature engineering (step 3)
Wondering about underperforming models due to underlying data issues after a few hours into training, validating and testing is like a photographer on the set, not knowing how their models might look like. Therefore, the key message is to see data exploration as an opportunity to get to know your data, understanding its strength and weaknesses.

Descriptive statistics often reveal coding errors. However, detecting underlying issues likely requires more than that. Decomposition methods such as PCA help to identify these and enable to revise previous steps. This ensures a smooth transition to model building.

Photo by Harrison Haines from Pexels
Photo by Harrison Haines from Pexels
Look beneath the surface with PCA
Large datasets often require PCA to reduce dimensionality anyway. The method as such captures the maximum possible variance across features and projects observations onto mutually uncorrelated vectors, called components. Still, PCA serves other purposes than dimensionality reduction. It also helps to discover underlying patterns across features.

To focus on the implementation in Python instead of methodology, I will skip describing PCA in its workings. There exist many great resources about it that I refer to those instead:

Animations showing PCA in action: https://setosa.io/ev/principal-component-analysis/
PCA explained in a family conversation: https://stats.stackexchange.com/a/140579
Smith [2]. A tutorial on principal components analysis: Accessible here.
Two metrics are crucial to make sense of PCA for data exploration:

1. Explained variance measures how much a model can reflect the variance of the whole data. Principle components try to capture as much of the variance as possible and this measure shows to what extent they can do that. It helps to see Components are sorted by explained variance, with the first one scoring highest and with a total sum of up to 1 across all components.

2. Factor loading indicates how much a variable correlates with a component. Each component is made of a linear combination of variables, where some might have more weight than others. Factor loadings indicate this as correlation coefficients, ranging from -1 to 1, and make components interpretable.

The upcoming sections apply PCA to exciting data from a behavioral field experiment and guide through using these metrics to enhance data exploration.

Load data: A Randomized Educational Intervention on Grit (Alan et al., 2019)
The iris dataset served well as a canonical example of several PCA. In an effort to be diverse and using novel data from a field study, I rely on replication data from Alan et al. [1]. I hope this is appreciated. It comprises data from behavioral experiments at Turkish schools, where 10 year olds took part in a curriculum to improve a non-cognitive skill called grit which defines as perseverance to pursue a task. The authors sampled individual characteristics and conducted behavioral experiments to measure a potential treatment effect between those receiving the program ( grit == 1) and those taking part in a control treatment ( grit == 0).

The following loads the data from an URL and stores it as a pandas dataframe.

# To load data from Harvard Dataverse
import io 
import requests
# load exciting data from URL (at least something else than Iris)
url = 'https://dataverse.harvard.edu/api/access/datafile/3352340?gbrecs=false'
s = requests.get(url).content
# store as dataframe
df_raw = pd.read_csv(io.StringIO(s.decode('utf-8')), sep='t')
Photo by cottonbro from Pexels
Photo by cottonbro from Pexels
Preprocessing and feature engineering
For PCA to work, the data needs to be numeric, without missings, and standardized. I put all steps into one function ( clean_data) which returns a dataframe with standardized features. and conduct steps 1 to 3 of the project work flow (collecting, processing and engineering). To begin with, import necessary modules and packages.

import pandas as pd
import numpy as np
# sklearn module
from sklearn.decomposition import PCA
# plots
import matplotlib.pyplot as plt
import seaborn as sns
# seaborn settings
sns.set_style("whitegrid")
sns.set_context("talk")
# imports for function
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
Next, the clean_data() function is defined. It gives a shortcut to transform the raw data into a prepared dataset with (i.) selected features, (ii.) missings replaced by column means, and (iii.) standardized variables.

Note about selected features: I selected features in (iv.) according to their replication scripts, accessible on Harvard Dataverse and solely used sample 2 ("sample B" in the publicly accessible working paper). To be concise, refer to the paper for relevant descriptives (p. 30, Table 2).

Preparing the data takes one line of code (v).

def clean_data(data, select_X=None, impute=False, std=False): 
    """Returns dataframe with selected, imputed 
       and standardized features

    Input
          data: dataframe
          select_X: list of feature names to be selected (string)
          impute: If True impute np.nan with mean
          std: If True standardize data

    Return
        dataframe: data with selected, imputed 
                   and standardized features    
    """

    # (i.) select features
    if select_X is not None:
        data = data.filter(select_X, axis='columns')
        print("t>>> Selected features: {}".format(select_X))
    else:
        # store column names
        select_X = list(data.columns)

    # (ii.) impute with mean 
    if impute:
        imp = SimpleImputer()
        data = imp.fit_transform(data)
        print("t>>> Imputed missings")

    # (iii.) standardize 
    if std:
        std_scaler = StandardScaler()
        data = std_scaler.fit_transform(data)
        print("t>>> Standardized data")

    return pd.DataFrame(data, columns=select_X)
#  (iv.) select relevant features in line with Alan et al. (2019)
selected_features = ['grit', 'male', 'task_ability', 'raven', 'grit_survey1', 'belief_survey1', 'mathscore1', 'verbalscore1', 'risk', 'inconsistent']
# (v.) select features, impute missings and standardize
X_std = clean_data(df_raw, selected_features, impute=True, std=True)
Now, the data is ready for exploration.

Scree plots and factor loadings: Interpret PCA results
A PCA yields two metrics that are relevant for data exploration: Firstly, how much variance each component explains (scree plot), and secondly how much a variable correlates with a component (factor loading). The following sections provide a practical example and guide through the PCA output with a scree plot for explained variance and a heatmap on factor loadings.

Explained variance shows the number of dimensions across variables
Nowadays, data is abundant and the size of datasets continues to grow. Data scientists routinely deal with hundreds of variables. However, are these variables worth their memory? Put differently: Does a variable capture unique patterns or does it measure similar properties already reflected by other variables?

PCA might answer this through the metric of explained variance per component. It details the number of underlying dimensions on which most of the variance is observed.

The code below initializes a PCA object from sklearn and transforms the original data along the calculated components (i.). Thereafter, information on explained variance is retrieved (ii.) and printed (iii.).

# (i.) initialize and compute pca
pca = PCA()
X_pca = pca.fit_transform(X_std)
# (ii.) get basic info
n_components = len(pca.explained_variance_ratio_)
explained_variance = pca.explained_variance_ratio_
cum_explained_variance = np.cumsum(explained_variance)
idx = np.arange(n_components)+1
df_explained_variance = pd.DataFrame([explained_variance, cum_explained_variance], 
                                     index=['explained variance', 'cumulative'], 
                                     columns=idx).T
mean_explained_variance = df_explained_variance.iloc[:,0].mean() # calculate mean explained variance
# (iii.) Print explained variance as plain text
print('PCA Overview')
print('='*40)
print("Total: {} components".format(n_components))
print('-'*40)
print('Mean explained variance:', round(mean_explained_variance,3))
print('-'*40)
print(df_explained_variance.head(20))
print('-'*40)
PCA Overview 
======================================== 
Total: 10 components 
---------------------------------------- 
Mean explained variance: 0.1 
---------------------------------------- 
explained variance cumulative 
1 0.265261 0.265261 
2 0.122700 0.387962 
3 0.113990 0.501951 
4 0.099139 0.601090 
5 0.094357 0.695447 
6 0.083412 0.778859 
7 0.063117 0.841976 
8 0.056386 0.898362 
9 0.052588 0.950950 
10 0.049050 1.000000 
----------------------------------------
Interpretation: The first component makes up for around 27% of the explained variance. This is relatively low as compared to other datasets, but no matter of concern. It simply indicates that a major share (100%‚Äì27%=73%) of observations distributes across more than one dimension. Another way to approach the output is to ask: How much components are required to cover more than X% of the variance? For example, I want to reduce the data‚Äôs dimensionality and retain at least 90% variance of the original data. Then I would have to include 9 components to reach at least 90% and even have 95% of explained variance covered in this case. With an overall of 10 variables in the original dataset, the scope to reduce dimensionality is limited. Additionally, this shows that each of the 10 original variables adds somewhat unique patterns and limitedly repeats information from other variables.

Photo by Leo Woessner from Pexels
Photo by Leo Woessner from Pexels
To give another example, I list explained variance of "the" wine dataset:

PCA Overview: Wine dataset 
======================================== 
Total: 13 components 
---------------------------------------- 
Mean explained variance: 0.077 
---------------------------------------- 
explained variance cumulative 
1 0.361988 0.361988 
2 0.192075 0.554063 
3 0.111236 0.665300 
4 0.070690 0.735990 
5 0.065633 0.801623 
6 0.049358 0.850981 
7 0.042387 0.893368 
8 0.026807 0.920175 
9 0.022222 0.942397 
10 0.019300 0.961697 
11 0.017368 0.979066 
12 0.012982 0.992048 
13 0.007952 1.000000 
----------------------------------------
Here, 8 out of 13 components suffice to capture at least 90% of the original variance. Thus, there is more scope to reduce dimensionality. Furthermore, it indicates that some variables do not contribute much to variance in the data.

Instead of plain text, a scree plot visualizes explained variance across components and informs about individual and cumulative explained variance for each component. The next code chunk creates such a scree plot and includes an option to focus on the first X components to be manageable when dealing with hundreds of components for larger datasets (limit).

#limit plot to x PC
limit = int(input("Limit scree plot to nth component (0 for all) > "))
if limit > 0:
    limit_df = limit
else:
    limit_df = n_components
df_explained_variance_limited = df_explained_variance.iloc[:limit_df,:]
#make scree plot
fig, ax1 = plt.subplots(figsize=(15,6))
ax1.set_title('Explained variance across principal components', fontsize=14)
ax1.set_xlabel('Principal component', fontsize=12)
ax1.set_ylabel('Explained variance', fontsize=12)
ax2 = sns.barplot(x=idx[:limit_df], y='explained variance', data=df_explained_variance_limited, palette='summer')
ax2 = ax1.twinx()
ax2.grid(False)
ax2.set_ylabel('Cumulative', fontsize=14)
ax2 = sns.lineplot(x=idx[:limit_df]-1, y='cumulative', data=df_explained_variance_limited, color='#fc8d59')
ax1.axhline(mean_explained_variance, ls='--', color='#fc8d59') #plot mean
ax1.text(-.8, mean_explained_variance+(mean_explained_variance*.05), "average", color='#fc8d59', fontsize=14) #label y axis
max_y1 = max(df_explained_variance_limited.iloc[:,0])
max_y2 = max(df_explained_variance_limited.iloc[:,1])
ax1.set(ylim=(0, max_y1+max_y1*.1))
ax2.set(ylim=(0, max_y2+max_y2*.1))
plt.show()

A scree plot might show distinct jumps from one component to another. For example, when the first component captures disproportionately more variance than others, it could be a sign that variables inform about the same underlying factor or do not add additional dimensions, but say the same thing from a marginally different angle.

To give a direct example and to get a feeling for how distinct jumps might look like, I provide the scree plot of the Boston house prices dataset:


Two Reasons why PCA saves time down the road
Assume you have hundreds of variables, apply PCA and discover that over much of the explained variance is captured by the first few components. This might hint at a much lower number of underlying dimensions than the number of variables. Most likely, dropping some hundred variables leads to performance gains for training, validation and testing. There will be more time left to select a suitable model and refine it than to wait for the model itself to discover lack of variance behind several variables.

In addition to this, imagine that the data was constructed by oneself, e.g. through web scraping, and the scraper extracted pre-specified information from a web page. In that case, the retrieved information could be one-dimensional, when the developer of the scraper had only few relevant items in mind, but forgot to include items that shed light on further aspects of the problem setting. At this stage, it might be worthwhile to go back to the first step of the work flow and adjust data collection.

Discover underlying factors with correlations between features and components
PCA offers another valuable statistic besides explained variance: The correlation between each principle component and a variable, also called factor loading. This statistic facilitates to grasp the dimension that lies behind a component. For example, a dataset includes information about individuals such as math score, reaction time and retention span. The overarching dimension would be cognitive skills and a component that strongly correlates with these variables can be interpreted as the cognitive skill dimension. Similarly, another dimension could be non-cognitive skills and personality, when the data has features such as self-confidence, patience or conscientiousness. A component that captures this area highly correlates with those features.

The following code creates a heatmap to inspect these correlations, also called factor loading matrix.

# adjust y-axis size dynamically
size_yaxis = round(X_std.shape[1] * 0.5)
fig, ax = plt.subplots(figsize=(8,size_yaxis))
# plot the first top_pc components
top_pc = 3
sns.heatmap(df_c.iloc[:,:top_pc], annot=True, cmap="YlGnBu", ax=ax)
plt.show()

The first component strongly negatively associates with task ability, reasoning score (raven), math score, verbal score and positively links to beliefs about being gritty (_gritsurvey1). Summarizing this into a common underlying factor is subjective and requires domain knowledge. In my opinion, the first component mainly captures cognitive skills.

The second component correlates negatively with receiving the treatment (grit), gender (male) and positively relates to being inconsistent. Interpreting this dimension is less clear-cut and much more challenging. Nevertheless, it accounts for 12% of explained variance instead of 27% like the first component, which results in less interpretable dimensions as it spans slightly across several topical areas. All components that follow might be analogously difficult to interpret.

Evidence that variables capture similar dimensions could be uniformly distributed factor loadings. One example which inspired this article is on of my projects where I relied on Google Trends data and self-constructed keywords about a firm‚Äôs sustainability. A list of the 15th highest factor loadings for the first principle component revealed loadings ranging from 0.12 as the highest value to 0.11 as the lowest loading of all 15. Such a uniform distribution of factor loadings could be an issue. This especially applies when data is self-collected and someone preselected what is being considered for collection. Adjusting this selection might add dimensionality to your data which possibly improves model performance at the end.

Another reason why PCA saves time down the road
If the data was self-constructed, the factor loadings show how each feature contributes to an underlying dimension, which helps to come up with additional perspectives on data collection and what features or dimensions could add valuable variance. Rather than blind guessing which features to add, factor loadings lead to informed decisions for data collection. They may even be an inspiration in the search for more advanced features.

Conclusion
All in all, PCA is a flexible instrument in the toolbox for data exploration. Its main purpose is to reduce complexity of large datasets. But it also serves well to look beneath the surface of variables, discover latent dimensions and relate variables to these dimensions, making them interpretable. Key metrics to consider are explained variance and factor loading.

This article shows how to leverage these metrics for data exploration that goes beyond averages, distributions and correlations and build an understanding of underlying properties of the data. Identifying patterns across variables is valuable to rethink previous steps in the project workflow, such as data collection, processing or feature engineering.

Thanks for reading! I hope you find it as useful as I had fun to write this guide. I am curious of your thoughts on this matter. If you have any feedback I highly appreciate your feedback and look forward receiving your message.

Appendix
Access the Jupyter Notebook
I applied PCA to even more exemplary datasets like Boston housing market, wine and iris using do_pca(). It illustrates how PCA output looks like for small datasets. Feel free to download my notebook or script.

Note on factor analysis vs. PCA
A rule of thumb formulated here states: Use PCA if you want to reduce your correlated observed variables to a smaller set of uncorrelated variables and use factor analysis to test a model of latent factors on observed variables.

Even though this distinction is scientifically correct, it becomes less relevant in an applied context. PCA relates closely to factor analysis which often leads to similar conclusions about data properties which is what we care about. Therefore, the distinction can be relaxed for data exploration. This post gives an example in an applied context and another example with hands-on code for factor analysis is attached in the notebook.

Finally, for those interested in the differences between factor analysis and PCA refer to this post. Note, that throughout this article I never used the term latent factor to be precise.

References
[1] Alan, S., Boneva, T., & Ertac, S. (2019). Ever failed, try again, succeed better: Results from a randomized educational intervention on grit. The Quarterly Journal of Economics, 134(3), 1121‚Äì1162.

[2] Smith, L. I. (2002). A tutorial on principal components analysis.

Written By

Philipp Schmalen
See all from Philipp Schmalen
Data Exploration
Data Science Toolbox
Explained Variance
Factor Loading
Pca
Share This Article

Share on Facebook
Share on LinkedIn
Share on X
Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.

Write for TDS
Related Articles
Photo by Denise Johnson on Unsplash
Stacked Data Exploration‚Ää-‚Ääa new and advanced way to explore your data
Artificial Intelligence
Get started with limitless possibilities of stacked data exploration

Pranay Dave
April 2, 2022
6 min read
Fast and Easy Data Exploration for Machine Learning
Data Science
Open-source library Sweetviz is the way

Pau Labarta Bajo
June 15, 2022
8 min read
Visualizing Different NFL Player Styles
Data Science
Different players have different strengths and weaknesses ‚Äì is there a way to visualize them?

Blake Atkinson
August 8, 2019
14 min read
How do people move when social distancing becomes the rule
Data Visualization
Uncovering the impact of shelter-in-place requirements on mobility behavior among German and Swedish residents early‚Ä¶

Maximilian Faschan
October 23, 2021
11 min read
Unsupervised image mapping
A simple and efficient way to explore a large quantity of images

Jordan Thieyre
September 24, 2020
9 min read
Impact of the Covid-19 Pandemic and the Earthquake on Traffic Flow in the Narrow City Center
Covid-19
On how to find hidden phenomena by mining the traffic data

Leo Tisljaric, PhD
May 15, 2021
4 min read
Conceptual vs inbuilt Principal Component Analysis for Breast Cancer Diagnosis
To fit the concept of eigenvalues and eigenvectors and python inbuilt function in PCA followed‚Ä¶

Debangana Mallick
April 26, 2021
15 min read
YouTube
X
LinkedIn
Threads
Bluesky
Towards Data Science
Your home for data science and Al. The world‚Äôs leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.

¬© Insight Media Group, LLC 2025
Subscribe to Our Newsletter
Write For TDS
About
Advertise
Privacy Policy
Terms of Use
Cookies Settings
Some areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How to Identify Overfitting
Overfitting vs. Underfitting
Real-World Examples of Overfitting
How to Prevent Overfitting
Glossary
Overfitting
Learn how to identify, prevent, and address overfitting in machine learning. Discover techniques for improving model generalization and real-world performance.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
Overfitting is a fundamental concept in machine learning (ML) that occurs when a model learns the detail and noise in the training data to the extent that it negatively impacts the model's performance on new, unseen data. In essence, the model memorizes the training set instead of learning the underlying patterns. This results in a model that achieves high accuracy on the data it was trained on, but fails to generalize to real-world data, making it unreliable for practical applications. Achieving good generalization is a primary goal in AI development.

How to Identify Overfitting
Overfitting is typically identified by monitoring the model's performance on both the training dataset and a separate validation dataset during the training process. A common sign of overfitting is when the loss function value for the training set continues to decrease, while the loss for the validation set begins to increase. Similarly, if training accuracy keeps improving but validation accuracy plateaus or worsens over subsequent epochs, the model is likely overfitting. Tools like TensorBoard are excellent for visualizing these metrics and diagnosing such issues early. Platforms like Ultralytics HUB can also help track experiments and evaluate models to detect overfitting.

Overfitting vs. Underfitting
Overfitting and underfitting are two common problems in machine learning that represent a model's failure to generalize. They are essentially opposite problems.

Overfitting: The model is too complex for the data (high variance). It captures noise and random fluctuations in the training data, leading to excellent performance during training but poor performance on the test data.
Underfitting: The model is too simple to capture the underlying structure of the data (high bias). It performs poorly on both training and test data because it cannot learn the relevant patterns.
The challenge in deep learning is to find the right balance, a concept often described by the bias-variance tradeoff.

Real-World Examples of Overfitting
Autonomous Vehicle Object Detection: Imagine training an Ultralytics YOLO model for an autonomous vehicle using a dataset that only contains images from sunny, daytime conditions. The model might become highly specialized in detecting pedestrians and cars in bright light but fail dramatically at night or in rainy or foggy weather. It has overfit to the specific lighting and weather conditions of the training data. Using diverse datasets like Argoverse can help prevent this.
Medical Image Analysis: A CNN model is trained to detect tumors from MRI scans sourced from a single hospital. The model might inadvertently learn to associate specific artifacts or noise patterns from that hospital's particular MRI machine with the presence of a tumor. When tested on scans from a different hospital with a different machine, its performance could drop significantly because it has overfit to the noise of the original training set, not the actual biological markers of tumors. This is a critical issue in fields like AI in healthcare.
How to Prevent Overfitting
Several techniques can be employed to combat overfitting and build more robust models.

Get More Data: Increasing the size and diversity of the training dataset is one of the most effective ways to prevent overfitting. More data helps the model learn the true underlying patterns rather than noise. You can explore a variety of Ultralytics datasets to enhance your projects.
Data Augmentation: This involves artificially expanding the training dataset by creating modified copies of existing data. Techniques like random rotations, scaling, cropping, and color shifts are applied. Ultralytics YOLO data augmentation techniques are built-in to improve model robustness.
Simplify Model Architecture: Sometimes, a model is too complex for the given dataset. Using a simpler architecture with fewer parameters can prevent it from memorizing the data. For instance, choosing a smaller model variant like YOLOv8n vs. YOLOv8x can be beneficial for smaller datasets.
Regularization: This technique adds a penalty to the loss function based on the complexity of the model, discouraging large model weights. Common methods are L1 and L2 regularization, which you can read more about here.
Dropout: A specific form of regularization where a random fraction of neurons are ignored during each training step. This forces the network to learn redundant representations and prevents any single neuron from becoming too influential. The Dropout concept is explained in detail here.
Early Stopping: This involves monitoring the model‚Äôs performance on a validation set and stopping the training process as soon as the validation performance begins to decline, even if the training performance is still improving. You can see an explanation of early stopping in Keras for more details.
Cross-Validation: By using techniques like K-Fold cross-validation, the data is split into multiple folds, and the model is trained and validated on different subsets. This provides a more robust estimate of the model's ability to generalize.
Model Pruning: This involves removing parameters or connections from a trained network that have little impact on its performance, thus reducing complexity. Companies like Neural Magic offer tools that specialize in pruning models for efficient deployment.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
How Cnn's Work
Cnn Vs. Other Architectures
Real-World Applications
Tools And Frameworks
Glossary
Convolutional Neural Network (CNN)
Discover how Convolutional Neural Networks (CNNs) revolutionize computer vision, powering AI in healthcare, self-driving cars, and more.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
A Convolutional Neural Network (CNN) is a specialized type of neural network (NN) that is highly effective for processing data with a grid-like topology, such as images. Inspired by the human visual cortex, CNNs automatically and adaptively learn spatial hierarchies of features from input data. This makes them the foundational architecture for most modern computer vision (CV) tasks, where they have achieved state-of-the-art results in everything from image classification to object detection.

How Cnn's Work
Unlike a standard neural network where every neuron in one layer is connected to every neuron in the next, CNNs use a special mathematical operation called a convolution. This allows the network to learn features in a local receptive field, preserving the spatial relationships between pixels.

A typical CNN architecture consists of several key layers:

Convolutional Layer: This is the core building block where a filter, or kernel, slides over the input image to produce feature maps. These maps highlight patterns like edges, corners, and textures. The size of these filters and the patterns they detect are learned during model training.
Activation Layer: After each convolution, an activation function like ReLU is applied to introduce non-linearity, allowing the model to learn more complex patterns.
Pooling (Downsampling) Layer: This layer reduces the spatial dimensions (width and height) of the feature maps, which decreases the computational load and helps make the detected features more robust to changes in position and orientation. A classic paper on the topic is ImageNet Classification with Deep Convolutional Neural Networks.
Fully Connected Layer: After several convolutional and pooling layers, the high-level features are flattened and passed to a fully connected layer, which performs classification based on the learned features.
Cnn Vs. Other Architectures
While CNNs are a type of deep learning model, they differ significantly from other architectures.

Neural Networks (NNs): A standard NN treats input data as a flat vector, losing all spatial information. CNNs preserve this information, making them ideal for image analysis.
Vision Transformers (ViTs): Unlike CNNs, which have a strong inductive bias for spatial locality, ViTs treat an image as a sequence of patches and use a self-attention mechanism to learn global relationships. ViTs often require more data to train but can excel at tasks where long-range context is important. Many modern models, like RT-DETR, use a hybrid approach, combining a CNN backbone with a Transformer-based detection head.
Real-World Applications
CNNs are the driving force behind countless real-world applications:

Object Detection: Models from the Ultralytics YOLO family, such as YOLOv8 and YOLO11, utilize CNN backbones to identify and locate objects in images and videos with remarkable speed and accuracy. This technology is crucial for everything from AI in automotive systems to AI-driven inventory management.
Medical Image Analysis: In healthcare, CNNs assist radiologists by analyzing medical scans (X-rays, MRIs, CTs) to detect tumors, fractures, and other anomalies. This application helps improve diagnostic speed and consistency, as highlighted in research from institutions like the National Institutes of Health (NIH). You can explore medical image analysis with Ultralytics for more information.
Image Segmentation: For tasks requiring pixel-level understanding, such as in autonomous vehicles that need to distinguish the road from a pedestrian, CNN-based architectures like U-Net are widely used for image segmentation.
Tools And Frameworks
Developing and deploying CNNs is supported by powerful tools and frameworks:

Libraries: Popular libraries like PyTorch (see the PyTorch official site) and TensorFlow provide high-level APIs for building and training CNNs. High-level APIs like Keras further simplify development.
Platforms: Platforms like Ultralytics HUB streamline the entire process, from managing datasets to training models and deploying them. Effective model creation often requires careful hyperparameter tuning and benefits from comprehensive model training tips. For optimized performance, you can explore integrations like OpenVINO and TensorRT.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
Common Regularization Techniques
Real-World Applications
Regularization vs. Other Concepts
Glossary
Regularization
Prevent overfitting and improve model generalization with regularization techniques like L1, L2, dropout, and early stopping. Learn more!

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Regularization is a set of techniques used in machine learning (ML) to prevent a common problem known as overfitting. When a model overfits, it learns the training data too well, including its noise and random fluctuations, which negatively impacts its ability to generalize and make accurate predictions on new, unseen data. Regularization works by adding a penalty for model complexity to the loss function, discouraging the model from learning overly complex patterns. This helps create a simpler, more generalizable model that performs better on both training and validation data.

Common Regularization Techniques
There are several widely used regularization techniques that help improve model performance and robustness:

L1 and L2 Regularization: These are the most common forms of regularization. They add a penalty to the loss function based on the size of the model's weights. L1 regularization (Lasso) tends to shrink less important feature weights to exactly zero, effectively performing feature selection. L2 regularization (Ridge or Weight Decay) forces the weights to be small but rarely zero. A deeper dive into the mathematical differences can be found in resources like the Stanford CS229 course notes.
Dropout Layer: This technique is specific to neural networks. During training, it randomly sets a fraction of neuron activations to zero at each update step. This prevents neurons from co-adapting too much and forces the network to learn more robust features. The concept was introduced in a highly influential research paper.
Data Augmentation: By artificially expanding the size and diversity of the training data, data augmentation helps the model become more invariant to minor changes. Common techniques include rotating, cropping, scaling, and shifting colors in images. Ultralytics offers built-in YOLO data augmentation methods to improve model robustness.
Early Stopping: This is a practical method where the model's performance on a validation set is monitored during training. The training process is halted when the validation performance stops improving, preventing the model from starting to overfit in later epochs. A practical guide on implementing early stopping is available in PyTorch documentation.
Real-World Applications
Regularization is fundamental to developing effective deep learning (DL) models across various fields.

Computer Vision: In object detection models like Ultralytics YOLO, regularization is crucial for generalizing from datasets like COCO to real-world applications. For instance, in AI for automotive solutions, L2 regularization and dropout help a traffic sign detector work reliably under varied lighting and weather conditions, preventing it from memorizing the specific examples seen during training.
Natural Language Processing (NLP): Large Language Models (LLMs) are prone to overfitting due to their massive number of parameters. In applications like machine translation, dropout is used within Transformer architectures to ensure the model learns grammatical rules and semantic relationships rather than just memorizing specific sentence pairs from its training data.
Regularization vs. Other Concepts
It is important to differentiate regularization from other related concepts in ML:

Regularization vs. Normalization: Normalization is a data preprocessing technique that scales input features to a standard range (e.g., 0 to 1). It ensures that no single feature dominates the learning process due to its scale. Regularization, in contrast, is a technique that constrains the model's complexity during training to prevent overfitting. While both improve model performance, normalization focuses on the data, while regularization focuses on the model itself. Batch Normalization is a layer-wise normalization technique that also provides a slight regularizing effect.
Regularization vs. Hyperparameter Tuning: Regularization techniques have their own hyperparameters, such as the regularization strength (lambda) in L1/L2 or the dropout rate. Hyperparameter tuning is the process of finding the optimal values for these settings, often automated with tools like the Ultralytics Tuner class. In short, you use hyperparameter tuning to find the best way to apply regularization. Platforms like Ultralytics HUB can help manage the experiments needed for this process.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Skip to content
Towards Data Science
Publish AI, ML & data-science insights to a global community of data professionals.

Sign in
Submit an Article
Latest
Editor‚Äôs Picks
Deep Dives
Newsletter
Write For TDS
LinkedIn
X

Toggle Search
Artificial Intelligence
Understanding l1 and l2 Regularization
An overview of regularization in the Linear Regression model.

Federico Trotta
May 10, 2022
6 min read
Share
l1 and l2 graphically represented. source: https://commons.wikimedia.org/wiki/File:Regularization.jpg
l1 and l2 graphically represented. source: https://commons.wikimedia.org/wiki/File:Regularization.jpg
When training a machine learning model, there can be the possibility that our model performs accurately on the training set but performs poorly on the test data.

In this case, we have a problem with overfitting; in fact, overfitting occurs when our machine learning model tries to cover all the data points (or more) than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model.

There are a lot of methods to avoid overfitting when it occurs; in the case of Linear Regression, one method to avoid overfitting is using one of the two regularized methods that are often called l1 and l2, and we are going to understand them in this article.

1. Some Regression Notions
Let‚Äôs start by understanding the basics of regression. As Wikipedia says:

regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the ‚Äòoutcome‚Äô or ‚Äòresponse‚Äô variable) and one or more independent variables (often called ‚Äòpredictors‚Äô, ‚Äòcovariates‚Äô, ‚Äòexplanatory variables‚Äô or ‚Äòfeatures‚Äô).

The linear regression formula is:

The linear regression formula. Image by Author.
The linear regression formula. Image by Author.
Where "Yi" is (the vector of )the dependent variable (also, called "response"), while "X" is (the vector of) the independent variables (also, called "features"). Alpha and Beta are coefficients and "the game" of regression relies all on finding the "best parameters". With the "best parameters" we can find the "best line" that "best fits" the given data so that we can estimate future outcome values when we‚Äôll give future inputs (new features values).

I wanted to stress the fact that X and Y are vectors because in Machine Learning we always have to work with multiple features, so in the case of linear regression, we can not plot a line between X and Y, as we can do in high school (or at the University) when we had just "one X" (one independent variable). In these cases, all the features contribute to the outcome in some way, so we can not just plot a graph since it would be a multivariable graph (we can do it anyway, but it is very complicated).

When overfitting occurs in linear regression, we can try to regularize our linear model; Regularization is the most used technique to penalize complex models in machine learning: it avoids overfitting by penalizing the regression coefficients that have high values. More specifically, It decreases the parameters and shrinks (simplifies) the model; its aim is to try to reduce the variance of the model, without a substantial increase in the bias.

In practice, in the regularized models (l1 and l2) we add a so-called "cost function" (or "loss function") to our linear model, and it is a measure of "how wrong" our model is in terms of its ability to estimate the relationship between X and y. The "type" of cost function differentiates l1 from l2.

2. L1 Regularization, or Lasso Regularization

Lasso (Least Absolute and Selection Operator) regression performs an L1 regularization, which adds a penalty equal to the absolute value of the magnitude of the coefficients, as we can see in the image above in the blue rectangle (lambda is the regularization parameter). This type of regularization uses shrinkage, which is where data values are shrunk towards a central point, like the mean, for example.

This type of regularization can result in sparse models with few coefficients; some coefficients, in fact, can become zero and can be eliminated from the model. This means that this type of model also performs feature selection (since some coefficients can become 0, it means that the features with coefficients 0 are eliminated) and it is to be chosen when we have "a lot" of features, since it simplifies the model. So, this model is good when we have to work with a "high number" of features.

If we look at the image at the top of this article, the absolute value of the penalty factors can be graphically represented as a (rotated) square, while the elliptical contours are the cost function. If the cost function (the ellipsis) "hits‚Äù one of the corners of the (rotated) square, then the coefficient corresponding to the axis is shrunk to zero and the relative feature is eliminated.

One problem of Lasso Regression is Multicollinearity; I mean that if there are two or more highly correlated variables then Lasso regression selects one of them randomly, which is not good for the interpretation of our model. To avoid that, I advise you to plot a correlation matrix, find the eventual highly correlated features and delete one of them (eg, if feature_1 and feature_2 are highly correlated, you can decide to delete feature_2, for example, since highly correlated variables have the same impact on the final solution).

2. L2 Regularization, or Ridge Regularization
The Ridge Regression formula. Source: https://openclassrooms.com/en/courses/6401081-improve-the-performance-of-a-machine-learning-model/6561486-improve-your-regression-with-regularization (CC BY-SA 4.0)
The Ridge Regression formula. Source: https://openclassrooms.com/en/courses/6401081-improve-the-performance-of-a-machine-learning-model/6561486-improve-your-regression-with-regularization (CC BY-SA 4.0)
Ridge Regression is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated.

This model adds a cost function which is the square **** value of the magnitude of the coefficients and, in fact, if we watch the first image of this article the geometric representation of the cost function, in this case, is a circle.

Unfortunately, this model does not perform the feature selection: **** it decreases the complexity of the model but does not reduce the number of independent variables, since it never leads to 0 the coefficients. This means that the final model will include all the independent variables. To avoid this problem, since Ridge has to be used when the features are highly correlated, here (more than with the Lasso model) is important to study the features with a correlation matrix and decide which to delete from the study you are performing.

Conclusions
As we have seen, regularization has to be performed when we have problems with the overfitting of our model.

With respect to the Linear Regression model, we have better use Lasso regularization when we have a lot of features, since it performs even features selection; if we have highly correlated features, we have better use the Ridge model.

Finally, if you have doubts about the difference between correlation and regression, you can read my clarifying article on this topic here.

FREE PYTHON EBOOK:

Started learning Python Data Science but struggling with it? Subscribe to my newsletter and get my free ebook: this will give you the right learning path to follow to learn Python for Data Science with hands-on experience.

Enjoyed the story? Become a Medium member for 5$/month through my referral link: I‚Äôll earn a small commission to no additional fee to you:

Join Medium with my referral link ‚Äì Federico Trotta

Written By

Federico Trotta
See all from Federico Trotta
Artificial Intelligence
Data Science
Education
Federico Trotta
Machine Learning
Share This Article

Share on Facebook
Share on LinkedIn
Share on X
Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.

Write for TDS
Related Articles

Implementing Convolutional Neural Networks in TensorFlow
Artificial Intelligence
Step-by-step code guide to building a Convolutional Neural Network

Shreya Rao
August 20, 2024
6 min read
What Do Large Language Models ‚ÄúUnderstand‚Äù?
Artificial Intelligence
A deep dive on the meaning of understanding and how it applies to LLMs

Tarik Dzekman
August 21, 2024
31 min read
Photo by Krista Mangulsone on Unsplash
How to Forecast Hierarchical Time Series
Artificial Intelligence
A beginner‚Äôs guide to forecast reconciliation

Dr. Robert K√ºbler
August 20, 2024
13 min read
Photo by davisuko on Unsplash
Hands-on Time Series Anomaly Detection using Autoencoders, with Python
Data Science
Here‚Äôs how to use Autoencoders to detect signals with anomalies in a few lines of‚Ä¶

Piero Paialunga
August 21, 2024
12 min read
Image from Canva.
3 AI Use Cases (That Are Not a Chatbot)
Machine Learning
Feature engineering, structuring unstructured data, and lead scoring

Shaw Talebi
August 21, 2024
7 min read
Solving a Constrained Project Scheduling Problem with Quantum Annealing
Data Science
Solving the resource constrained project scheduling problem (RCPSP) with D-Wave‚Äôs hybrid constrained quadratic model (CQM)

Luis Fernando P√âREZ ARMAS, Ph.D.
August 20, 2024
29 min read

Back To Basics, Part Uno: Linear Regression and Cost Function
Data Science
An illustrated guide on essential machine learning concepts

Shreya Rao
February 3, 2023
6 min read
YouTube
X
LinkedIn
Threads
Bluesky
Towards Data Science
Your home for data science and Al. The world‚Äôs leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.

¬© Insight Media Group, LLC 2025
Subscribe to Our Newsletter
Write For TDS
About
Advertise
Privacy Policy
Terms of Use
Cookies Settings
Some areas of this page may shift around if you resize the browser window. Be sure to check heading and document order.
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
How a Dropout Layer Works
Real-World Applications
Related Concepts and Distinctions
Glossary
Dropout Layer
Discover how dropout layers prevent overfitting in neural networks by improving generalization, robustness, and model performance.

Flexible enterprise licensing solution to power your innovation
Get started
Train YOLO models with Ultralytics HUB
A dropout layer is a powerful yet simple regularization technique used in neural networks (NN) to combat overfitting. Overfitting occurs when a model learns the training data too well, including its noise and idiosyncrasies, which harms its ability to generalize to new, unseen data. The core idea behind dropout, introduced by Geoffrey Hinton and his colleagues in a groundbreaking 2014 paper, is to randomly "drop out"‚Äîor temporarily remove‚Äîneurons and their connections during each training step. This prevents neurons from becoming overly reliant on each other, forcing the network to learn more robust and redundant representations.

How a Dropout Layer Works
During the model training process, a dropout layer randomly sets the activations of a fraction of neurons in the previous layer to zero. The "dropout rate" is a hyperparameter that defines the probability of a neuron being dropped. For example, a dropout rate of 0.5 means each neuron has a 50% chance of being ignored during a given training iteration. This process can be thought of as training a large number of thinned networks that share weights.

By constantly changing the network's architecture, dropout prevents complex co-adaptations, where a neuron's output is highly dependent on the presence of a few specific other neurons. Instead, each neuron is encouraged to be a more independently useful feature detector. During the testing or inference phase, the dropout layer is turned off, and all neurons are used. To compensate for the fact that more neurons are active than during training, the outputs of the layer are scaled down by the dropout rate. This ensures the expected output from each neuron remains consistent between training and testing. Frameworks like PyTorch and TensorFlow handle this scaling automatically in their dropout layer implementations.

Real-World Applications
Dropout is widely used across various domains of artificial intelligence (AI) and machine learning (ML):

Computer Vision: In computer vision (CV), dropout helps models like Ultralytics YOLO perform better on tasks such as object detection, image classification, and instance segmentation. For example, in autonomous driving systems, dropout can make detection models more robust to variations in lighting, weather, or occlusions, improving safety and reliability. Training such models can be managed effectively using platforms like Ultralytics HUB.
Natural Language Processing (NLP): Dropout is commonly applied in NLP models like Transformers and BERT. In applications like machine translation or sentiment analysis, dropout prevents the model from memorizing specific phrases or sentence structures from the training data. This leads to a better understanding and generation of novel text, enhancing the performance of chatbots and text summarization tools.
Related Concepts and Distinctions
Dropout is one of several techniques used for regularization in deep learning. Others include:

L1 and L2 Regularization: These methods add a penalty to the loss function based on the magnitude of the model weights, encouraging smaller weights to reduce model complexity. You can read more about L1/L2 regularization. In contrast, dropout directly modifies the network's structure during training rather than just penalizing weights.
Batch Normalization: Batch Normalization (BN) normalizes the activations within a layer, which can stabilize training and sometimes provide a mild regularizing effect, potentially reducing the need for strong dropout. While BN addresses internal covariate shift, Dropout directly targets model complexity by forcing redundancy.
Data Augmentation: Techniques like rotating, scaling, or cropping images (data augmentation) artificially increase the diversity of the training dataset. This also helps prevent overfitting and improve generalization. Dropout and data augmentation are often used together to achieve even more robust results.
In summary, the Dropout Layer is a simple yet powerful regularization technique essential for training robust deep learning models across various applications, from advanced computer vision to NLP.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Dropout: A Simple Way to Prevent Neural Networks from
Overfitting
Nitish Srivastava nitish@cs.toronto.edu
Geoffrey Hinton hinton@cs.toronto.edu
Alex Krizhevsky kriz@cs.toronto.edu
Ilya Sutskever ilya@cs.toronto.edu
Ruslan Salakhutdinov rsalakhu@cs.toronto.edu
Department of Computer Science
University of Toronto
10 Kings College Road, Rm 3302
Toronto, Ontario, M5S 3G4, Canada.
Editor: Yoshua Bengio
Abstract
Deep neural nets with a large number of parameters are very powerful machine learning
systems. However, overfitting is a serious problem in such networks. Large networks are also
slow to use, making it difficult to deal with overfitting by combining the predictions of many
different large neural nets at test time. Dropout is a technique for addressing this problem.
The key idea is to randomly drop units (along with their connections) from the neural
network during training. This prevents units from co-adapting too much. During training,
dropout samples from an exponential number of different ‚Äúthinned‚Äù networks. At test time,
it is easy to approximate the effect of averaging the predictions of all these thinned networks
by simply using a single unthinned network that has smaller weights. This significantly
reduces overfitting and gives major improvements over other regularization methods. We
show that dropout improves the performance of neural networks on supervised learning
tasks in vision, speech recognition, document classification and computational biology,
obtaining state-of-the-art results on many benchmark data sets.
Keywords: neural networks, regularization, model combination, deep learning
1. Introduction
Deep neural networks contain multiple non-linear hidden layers and this makes them very
expressive models that can learn very complicated relationships between their inputs and
outputs. With limited training data, however, many of these complicated relationships
will be the result of sampling noise, so they will exist in the training set but not in real
test data even if it is drawn from the same distribution. This leads to overfitting and many
methods have been developed for reducing it. These include stopping the training as soon as
performance on a validation set starts to get worse, introducing weight penalties of various
kinds such as L1 and L2 regularization and soft weight sharing (Nowlan and Hinton, 1992).
With unlimited computation, the best way to ‚Äúregularize‚Äù a fixed-sized model is to
average the predictions of all possible settings of the parameters, weighting each setting by

c 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
(a) Standard Neural Net (b) After applying dropout.
Figure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right:
An example of a thinned net produced by applying dropout to the network on the left.
Crossed units have been dropped.
its posterior probability given the training data. This can sometimes be approximated quite
well for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but we
would like to approach the performance of the Bayesian gold standard using considerably
less computation. We propose to do this by approximating an equally weighted geometric
mean of the predictions of an exponential number of learned models that share parameters.
Model combination nearly always improves the performance of machine learning methods. With large neural networks, however, the obvious idea of averaging the outputs of
many separately trained nets is prohibitively expensive. Combining several models is most
helpful when the individual models are different from each other and in order to make
neural net models different, they should either have different architectures or be trained
on different data. Training many different architectures is hard because finding optimal
hyperparameters for each architecture is a daunting task and training each large network
requires a lot of computation. Moreover, large networks normally require large amounts of
training data and there may not be enough data available to train different networks on
different subsets of the data. Even if one was able to train many different large networks,
using them all at test time is infeasible in applications where it is important to respond
quickly.
Dropout is a technique that addresses both these issues. It prevents overfitting and
provides a way of approximately combining exponentially many different neural network
architectures efficiently. The term ‚Äúdropout‚Äù refers to dropping out units (hidden and
visible) in a neural network. By dropping a unit out, we mean temporarily removing it from
the network, along with all its incoming and outgoing connections, as shown in Figure 1.
The choice of which units to drop is random. In the simplest case, each unit is retained with
a fixed probability p independent of other units, where p can be chosen using a validation
set or can simply be set at 0.5, which seems to be close to optimal for a wide range of
networks and tasks. For the input units, however, the optimal probability of retention is
usually closer to 1 than to 0.5.
1930
Dropout
Present with
probability p
w
‚ú≤
(a) At training time
Always
present
pw
‚ú≤
(b) At test time
Figure 2: Left: A unit at training time that is present with probability p and is connected to units
in the next layer with weights w. Right: At test time, the unit is always present and
the weights are multiplied by p. The output at test time is same as the expected output
at training time.
Applying dropout to a neural network amounts to sampling a ‚Äúthinned‚Äù network from
it. The thinned network consists of all the units that survived dropout (Figure 1b). A
neural net with n units, can be seen as a collection of 2n possible thinned neural networks.
These networks all share weights so that the total number of parameters is still O(n
2
), or
less. For each presentation of each training case, a new thinned network is sampled and
trained. So training a neural network with dropout can be seen as training a collection of 2n
thinned networks with extensive weight sharing, where each thinned network gets trained
very rarely, if at all.
At test time, it is not feasible to explicitly average the predictions from exponentially
many thinned models. However, a very simple approximate averaging method works well in
practice. The idea is to use a single neural net at test time without dropout. The weights
of this network are scaled-down versions of the trained weights. If a unit is retained with
probability p during training, the outgoing weights of that unit are multiplied by p at test
time as shown in Figure 2. This ensures that for any hidden unit the expected output (under
the distribution used to drop units at training time) is the same as the actual output at
test time. By doing this scaling, 2n networks with shared weights can be combined into
a single neural network to be used at test time. We found that training a network with
dropout and using this approximate averaging method at test time leads to significantly
lower generalization error on a wide variety of classification problems compared to training
with other regularization methods.
The idea of dropout is not limited to feed-forward neural nets. It can be more generally
applied to graphical models such as Boltzmann Machines. In this paper, we introduce
the dropout Restricted Boltzmann Machine model and compare it to standard Restricted
Boltzmann Machines (RBM). Our experiments show that dropout RBMs are better than
standard RBMs in certain respects.
This paper is structured as follows. Section 2 describes the motivation for this idea.
Section 3 describes relevant previous work. Section 4 formally describes the dropout model.
Section 5 gives an algorithm for training dropout networks. In Section 6, we present our
experimental results where we apply dropout to problems in different domains and compare
it with other forms of regularization and model combination. Section 7 analyzes the effect of
dropout on different properties of a neural network and describes how dropout interacts with
the network‚Äôs hyperparameters. Section 8 describes the Dropout RBM model. In Section 9
we explore the idea of marginalizing dropout. In Appendix A we present a practical guide
1931
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
for training dropout nets. This includes a detailed analysis of the practical considerations
involved in choosing hyperparameters when training dropout networks.
2. Motivation
A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al.,
2010). Sexual reproduction involves taking half the genes of one parent and half of the
other, adding a very small amount of random mutation, and combining them to produce an
offspring. The asexual alternative is to create an offspring with a slightly mutated copy of
the parent‚Äôs genes. It seems plausible that asexual reproduction should be a better way to
optimize individual fitness because a good set of genes that have come to work well together
can be passed on directly to the offspring. On the other hand, sexual reproduction is likely
to break up these co-adapted sets of genes, especially if these sets are large and, intuitively,
this should decrease the fitness of organisms that have already evolved complicated coadaptations. However, sexual reproduction is the way most advanced organisms evolved.
One possible explanation for the superiority of sexual reproduction is that, over the long
term, the criterion for natural selection may not be individual fitness but rather mix-ability
of genes. The ability of a set of genes to be able to work well with another random set of
genes makes them more robust. Since a gene cannot rely on a large set of partners to be
present at all times, it must learn to do something useful on its own or in collaboration with
a small number of other genes. According to this theory, the role of sexual reproduction
is not just to allow useful new genes to spread throughout the population, but also to
facilitate this process by reducing complex co-adaptations that would reduce the chance of
a new gene improving the fitness of an individual. Similarly, each hidden unit in a neural
network trained with dropout must learn to work with a randomly chosen sample of other
units. This should make each hidden unit more robust and drive it towards creating useful
features on its own without relying on other hidden units to correct its mistakes. However,
the hidden units within a layer will still learn to do different things from each other. One
might imagine that the net would become robust against dropout by making many copies
of each hidden unit, but this is a poor solution for exactly the same reason as replica codes
are a poor way to deal with a noisy channel.
A closely related, but slightly different motivation for dropout comes from thinking
about successful conspiracies. Ten conspiracies each involving five people is probably a
better way to create havoc than one big conspiracy that requires fifty people to all play
their parts correctly. If conditions do not change and there is plenty of time for rehearsal, a
big conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy
the greater its chance of still working. Complex co-adaptations can be trained to work well
on a training set, but on novel test data they are far more likely to fail than multiple simpler
co-adaptations that achieve the same thing.
3. Related Work
Dropout can be interpreted as a way of regularizing a neural network by adding noise to
its hidden units. The idea of adding noise to the states of units has previously been used in
the context of Denoising Autoencoders (DAEs) by Vincent et al. (2008, 2010) where noise
1932
Dropout
is added to the input units of an autoencoder and the network is trained to reconstruct the
noise-free input. Our work extends this idea by showing that dropout can be effectively
applied in the hidden layers as well and that it can be interpreted as a form of model
averaging. We also show that adding noise is not only useful for unsupervised feature
learning but can also be extended to supervised learning problems. In fact, our method can
be applied to other neuron-based architectures, for example, Boltzmann Machines. While
5% noise typically works best for DAEs, we found that our weight scaling procedure applied
at test time enables us to use much higher noise levels. Dropping out 20% of the input units
and 50% of the hidden units was often found to be optimal.
Since dropout can be seen as a stochastic regularization technique, it is natural to
consider its deterministic counterpart which is obtained by marginalizing out the noise. In
this paper, we show that, in simple cases, dropout can be analytically marginalized out
to obtain deterministic regularization methods. Recently, van der Maaten et al. (2013)
also explored deterministic regularizers corresponding to different exponential-family noise
distributions, including dropout (which they refer to as ‚Äúblankout noise‚Äù). However, they
apply noise to the inputs and only explore models with no hidden layers. Wang and Manning
(2013) proposed a method for speeding up dropout by marginalizing dropout noise. Chen
et al. (2012) explored marginalization in the context of denoising autoencoders.
In dropout, we minimize the loss function stochastically under a noise distribution.
This can be seen as minimizing an expected loss function. Previous work of Globerson and
Roweis (2006); Dekel et al. (2010) explored an alternate setting where the loss is minimized
when an adversary gets to pick which units to drop. Here, instead of a noise distribution,
the maximum number of units that can be dropped is fixed. However, this work also does
not explore models with hidden units.
4. Model Description
This section describes the dropout neural network model. Consider a neural network with
L hidden layers. Let l ‚àà {1, . . . , L} index the hidden layers of the network. Let z
(l) denote
the vector of inputs into layer l, y
(l) denote the vector of outputs from layer l (y
(0) = x is
the input). W(l) and b
(l) are the weights and biases at layer l. The feed-forward operation
of a standard neural network (Figure 3a) can be described as (for l ‚àà {0, . . . , L ‚àí 1} and
any hidden unit i)
z
(l+1)
i = w
(l+1)
i y
l + b
(l+1)
i
,
y
(l+1)
i = f(z
(l+1)
i
),
where f is any activation function, for example, f(x) = 1/ (1 + exp(‚àíx)).
With dropout, the feed-forward operation becomes (Figure 3b)
r
(l)
j ‚àº Bernoulli(p),
ye
(l) = r
(l)
‚àó y
(l)
,
z
(l+1)
i = w
(l+1)
i ye
l + b
(l+1)
i
,
y
(l+1)
i = f(z
(l+1)
i
).
1933
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
+1
y
(l)
1
y
(l)
2
y
(l)
3
z
(l+1)
i y
(l+1)
w i (l+1)
i
b
(l+1)
i
f
(a) Standard network
+1
ye
(l)
1
ye
(l)
2
ye
(l)
3
z
(l+1)
i y
(l+1)
i
y
(l)
1
y
(l)
2
y
(l)
3
r
(l)
1
r
(l)
2
r
(l)
3
w
(l+1)
i
b
(l+1)
i
f
(b) Dropout network
Figure 3: Comparison of the basic operations of a standard and dropout network.
Here ‚àó denotes an element-wise product. For any layer l, r
(l)
is a vector of independent
Bernoulli random variables each of which has probability p of being 1. This vector is
sampled and multiplied element-wise with the outputs of that layer, y
(l)
, to create the
thinned outputs ye
(l)
. The thinned outputs are then used as input to the next layer. This
process is applied at each layer. This amounts to sampling a sub-network from a larger
network. For learning, the derivatives of the loss function are backpropagated through the
sub-network. At test time, the weights are scaled as W
(l)
test = pW(l) as shown in Figure 2.
The resulting neural network is used without dropout.
5. Learning Dropout Nets
This section describes a procedure for training dropout neural nets.
5.1 Backpropagation
Dropout neural networks can be trained using stochastic gradient descent in a manner similar to standard neural nets. The only difference is that for each training case in a mini-batch,
we sample a thinned network by dropping out units. Forward and backpropagation for that
training case are done only on this thinned network. The gradients for each parameter are
averaged over the training cases in each mini-batch. Any training case which does not use a
parameter contributes a gradient of zero for that parameter. Many methods have been used
to improve stochastic gradient descent such as momentum, annealed learning rates and L2
weight decay. Those were found to be useful for dropout neural networks as well.
One particular form of regularization was found to be especially useful for dropout‚Äî
constraining the norm of the incoming weight vector at each hidden unit to be upper
bounded by a fixed constant c. In other words, if w represents the vector of weights incident
on any hidden unit, the neural network was optimized under the constraint ||w||2 ‚â§ c. This
constraint was imposed during optimization by projecting w onto the surface of a ball of
radius c, whenever w went out of it. This is also called max-norm regularization since it
implies that the maximum value that the norm of any weight can take is c. The constant
1934
Dropout
c is a tunable hyperparameter, which is determined using a validation set. Max-norm
regularization has been previously used in the context of collaborative filtering (Srebro and
Shraibman, 2005). It typically improves the performance of stochastic gradient descent
training of deep neural nets, even when no dropout is used.
Although dropout alone gives significant improvements, using dropout along with maxnorm regularization, large decaying learning rates and high momentum provides a significant
boost over just using dropout. A possible justification is that constraining weight vectors
to lie inside a ball of fixed radius makes it possible to use a huge learning rate without the
possibility of weights blowing up. The noise provided by dropout then allows the optimization process to explore different regions of the weight space that would have otherwise been
difficult to reach. As the learning rate decays, the optimization takes shorter steps, thereby
doing less exploration and eventually settles into a minimum.
5.2 Unsupervised Pretraining
Neural networks can be pretrained using stacks of RBMs (Hinton and Salakhutdinov, 2006),
autoencoders (Vincent et al., 2010) or Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009). Pretraining is an effective way of making use of unlabeled data. Pretraining
followed by finetuning with backpropagation has been shown to give significant performance
boosts over finetuning from random initializations in certain cases.
Dropout can be applied to finetune nets that have been pretrained using these techniques. The pretraining procedure stays the same. The weights obtained from pretraining
should be scaled up by a factor of 1/p. This makes sure that for each unit, the expected
output from it under random dropout will be the same as the output during pretraining.
We were initially concerned that the stochastic nature of dropout might wipe out the information in the pretrained weights. This did happen when the learning rates used during
finetuning were comparable to the best learning rates for randomly initialized nets. However, when the learning rates were chosen to be smaller, the information in the pretrained
weights seemed to be retained and we were able to get improvements in terms of the final
generalization error compared to not using dropout when finetuning.
6. Experimental Results
We trained dropout neural networks for classification problems on data sets in different
domains. We found that dropout improved generalization performance on all data sets
compared to neural networks that did not use dropout. Table 1 gives a brief description of
the data sets. The data sets are
‚Ä¢ MNIST : A standard toy data set of handwritten digits.
‚Ä¢ TIMIT : A standard speech benchmark for clean speech recognition.
‚Ä¢ CIFAR-10 and CIFAR-100 : Tiny natural images (Krizhevsky, 2009).
‚Ä¢ Street View House Numbers data set (SVHN) : Images of house numbers collected by
Google Street View (Netzer et al., 2011).
‚Ä¢ ImageNet : A large collection of natural images.
‚Ä¢ Reuters-RCV1 : A collection of Reuters newswire articles.
1935
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
‚Ä¢ Alternative Splicing data set: RNA features for predicting alternative gene splicing
(Xiong et al., 2011).
We chose a diverse set of data sets to demonstrate that dropout is a general technique
for improving neural nets and is not specific to any particular application domain. In this
section, we present some key results that show the effectiveness of dropout. A more detailed
description of all the experiments and data sets is provided in Appendix B.
Data Set Domain Dimensionality Training Set Test Set
MNIST Vision 784 (28 √ó 28 grayscale) 60K 10K
SVHN Vision 3072 (32 √ó 32 color) 600K 26K
CIFAR-10/100 Vision 3072 (32 √ó 32 color) 60K 10K
ImageNet (ILSVRC-2012) Vision 65536 (256 √ó 256 color) 1.2M 150K
TIMIT Speech 2520 (120-dim, 21 frames) 1.1M frames 58K frames
Reuters-RCV1 Text 2000 200K 200K
Alternative Splicing Genetics 1014 2932 733
Table 1: Overview of the data sets used in this paper.
6.1 Results on Image Data Sets
We used five image data sets to evaluate dropout‚ÄîMNIST, SVHN, CIFAR-10, CIFAR-100
and ImageNet. These data sets include different image types and training set sizes. Models
which achieve state-of-the-art results on all of these data sets use dropout.
6.1.1 MNIST
Method Unit
Type Architecture Error
%
Standard Neural Net (Simard et al., 2003) Logistic 2 layers, 800 units 1.60
SVM Gaussian kernel NA NA 1.40
Dropout NN Logistic 3 layers, 1024 units 1.35
Dropout NN ReLU 3 layers, 1024 units 1.25
Dropout NN + max-norm constraint ReLU 3 layers, 1024 units 1.06
Dropout NN + max-norm constraint ReLU 3 layers, 2048 units 1.04
Dropout NN + max-norm constraint ReLU 2 layers, 4096 units 1.01
Dropout NN + max-norm constraint ReLU 2 layers, 8192 units 0.95
Dropout NN + max-norm constraint (Goodfellow
et al., 2013) Maxout 2 layers, (5 √ó 240)
units 0.94
DBN + finetuning (Hinton and Salakhutdinov, 2006) Logistic 500-500-2000 1.18
DBM + finetuning (Salakhutdinov and Hinton, 2009) Logistic 500-500-2000 0.96
DBN + dropout finetuning Logistic 500-500-2000 0.92
DBM + dropout finetuning Logistic 500-500-2000 0.79
Table 2: Comparison of different models on MNIST.
The MNIST data set consists of 28 √ó 28 pixel handwritten digit images. The task is
to classify the images into 10 digit classes. Table 2 compares the performance of dropout
with other techniques. The best performing neural networks for the permutation invariant
1936
Dropout
setting that do not use dropout or unsupervised pretraining achieve an error of about
1.60% (Simard et al., 2003). With dropout the error reduces to 1.35%. Replacing logistic
units with rectified linear units (ReLUs) (Jarrett et al., 2009) further reduces the error to
1.25%. Adding max-norm regularization again reduces it to 1.06%. Increasing the size of
the network leads to better results. A neural net with 2 layers and 8192 units per layer
gets down to 0.95% error. Note that this network has more than 65 million parameters and
is being trained on a data set of size 60,000. Training a network of this size to give good
generalization error is very hard with standard regularization methods and early stopping.
Dropout, on the other hand, prevents overfitting, even in this case. It does not even need
early stopping. Goodfellow et al. (2013) showed that results can be further improved to
0.94% by replacing ReLU units with maxout units. All dropout nets use p = 0.5 for hidden
units and p = 0.8 for input units. More experimental details can be found in Appendix B.1.
Dropout nets pretrained with stacks of RBMs and Deep Boltzmann Machines also give
improvements as shown in Table 2. DBM‚Äîpretrained dropout nets achieve a test error of
0.79% which is the best performance ever reported for the permutation invariant setting.
We note that it possible to obtain better results by using 2-D spatial information and
augmenting the training set with distorted versions of images from the standard training
set. We demonstrate the effectiveness of dropout in that setting on more interesting data
sets.
0 200000 400000 600000 800000 1000000
Number of weight updates
1.0
1.5
2.0
2.5
Classification Error %
With dropout
Without dropout
‚ùÖ‚ùò
‚ùÖ
‚ùÖ‚ùò
Figure 4: Test error for different architectures
with and without dropout. The networks have 2 to 4 hidden layers each
with 1024 to 2048 units.
In order to test the robustness of
dropout, classification experiments were
done with networks of many different architectures keeping all hyperparameters, including p, fixed. Figure 4 shows the test
error rates obtained for these different architectures as training progresses. The
same architectures trained with and without dropout have drastically different test
errors as seen as by the two separate clusters of trajectories. Dropout gives a huge
improvement across all architectures, without using hyperparameters that were tuned
specifically for each architecture.
6.1.2 Street View House Numbers
The Street View House Numbers (SVHN)
Data Set (Netzer et al., 2011) consists of
color images of house numbers collected by
Google Street View. Figure 5a shows some examples of images from this data set. The
part of the data set that we use in our experiments consists of 32 √ó 32 color images roughly
centered on a digit in a house number. The task is to identify that digit.
For this data set, we applied dropout to convolutional neural networks (LeCun et al.,
1989). The best architecture that we found has three convolutional layers followed by 2
fully connected hidden layers. All hidden units were ReLUs. Each convolutional layer was
1937
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Method Error %
Binary Features (WDCH) (Netzer et al., 2011) 36.7
HOG (Netzer et al., 2011) 15.0
Stacked Sparse Autoencoders (Netzer et al., 2011) 10.3
KMeans (Netzer et al., 2011) 9.4
Multi-stage Conv Net with average pooling (Sermanet et al., 2012) 9.06
Multi-stage Conv Net + L2 pooling (Sermanet et al., 2012) 5.36
Multi-stage Conv Net + L4 pooling + padding (Sermanet et al., 2012) 4.90
Conv Net + max-pooling 3.95
Conv Net + max pooling + dropout in fully connected layers 3.02
Conv Net + stochastic pooling (Zeiler and Fergus, 2013) 2.80
Conv Net + max pooling + dropout in all layers 2.55
Conv Net + maxout (Goodfellow et al., 2013) 2.47
Human Performance 2.0
Table 3: Results on the Street View House Numbers data set.
followed by a max-pooling layer. Appendix B.2 describes the architecture in more detail.
Dropout was applied to all the layers of the network with the probability of retaining a hidden unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (going
from input to convolutional layers to fully connected layers). Max-norm regularization was
used for weights in both convolutional and fully connected layers. Table 3 compares the
results obtained by different methods. We find that convolutional nets outperform other
methods. The best performing convolutional nets that do not use dropout achieve an error
rate of 3.95%. Adding dropout only to the fully connected layers reduces the error to 3.02%.
Adding dropout to the convolutional layers as well further reduces the error to 2.55%. Even
more gains can be obtained by using maxout units.
The additional gain in performance obtained by adding dropout in the convolutional
layers (3.02% to 2.55%) is worth noting. One may have presumed that since the convolutional layers don‚Äôt have a lot of parameters, overfitting is not a problem and therefore
dropout would not have much effect. However, dropout in the lower layers still helps because it provides noisy inputs for the higher fully connected layers which prevents them
from overfitting.
6.1.3 CIFAR-10 and CIFAR-100
The CIFAR-10 and CIFAR-100 data sets consist of 32 √ó 32 color images drawn from 10
and 100 categories respectively. Figure 5b shows some examples of images from this data
set. A detailed description of the data sets, input preprocessing, network architectures and
other experimental details is given in Appendix B.3. Table 4 shows the error rate obtained
by different methods on these data sets. Without any data augmentation, Snoek et al.
(2012) used Bayesian hyperparameter optimization to obtained an error rate of 14.98% on
CIFAR-10. Using dropout in the fully connected layers reduces that to 14.32% and adding
dropout in every layer further reduces the error to 12.61%. Goodfellow et al. (2013) showed
that the error is further reduced to 11.68% by replacing ReLU units with maxout units. On
CIFAR-100, dropout reduces the error from 43.48% to 37.20% which is a huge improvement.
No data augmentation was used for either data set (apart from the input dropout).
1938
Dropout
(a) Street View House Numbers (SVHN) (b) CIFAR-10
Figure 5: Samples from image data sets. Each row corresponds to a different category.
Method CIFAR-10 CIFAR-100
Conv Net + max pooling (hand tuned) 15.60 43.48
Conv Net + stochastic pooling (Zeiler and Fergus, 2013) 15.13 42.51
Conv Net + max pooling (Snoek et al., 2012) 14.98 -
Conv Net + max pooling + dropout fully connected layers 14.32 41.26
Conv Net + max pooling + dropout in all layers 12.61 37.20
Conv Net + maxout (Goodfellow et al., 2013) 11.68 38.57
Table 4: Error rates on CIFAR-10 and CIFAR-100.
6.1.4 ImageNet
ImageNet is a data set of over 15 million labeled high-resolution images belonging to roughly
22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual
competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has
been held. A subset of ImageNet with roughly 1000 images in each of 1000 categories is
used in this challenge. Since the number of categories is rather large, it is conventional to
report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test
images for which the correct label is not among the five labels considered most probable by
the model. Figure 6 shows some predictions made by our model on a few test images.
ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so
most of our experiments were performed on this data set. Table 5 compares the performance
of different methods. Convolutional nets with dropout outperform other methods by a large
margin. The architecture and implementation details are described in detail in Krizhevsky
et al. (2012).
1939
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Figure 6: Some ImageNet test cases with the 4 most probable labels as predicted by our model.
The length of the horizontal bars is proportional to the probability assigned to the labels
by the model. Pink indicates ground truth.
Model Top-1 Top-5
Sparse Coding (Lin et al., 2010) 47.1 28.2
SIFT + Fisher Vectors (Sanchez and Perronnin, 2011) 45.7 25.7
Conv Net + dropout (Krizhevsky et al., 2012) 37.5 17.0
Table 5: Results on the ILSVRC-2010 test set.
Model Top-1
(val)
Top-5
(val)
Top-5
(test)
SVM on Fisher Vectors of Dense SIFT and Color Statistics - - 27.3
Avg of classifiers over FVs of SIFT, LBP, GIST and CSIFT - - 26.2
Conv Net + dropout (Krizhevsky et al., 2012) 40.7 18.2 -
Avg of 5 Conv Nets + dropout (Krizhevsky et al., 2012) 38.1 16.4 16.4
Table 6: Results on the ILSVRC-2012 validation/test set.
Our model based on convolutional nets and dropout won the ILSVRC-2012 competition.
Since the labels for the test set are not available, we report our results on the test set for
the final submission and include the validation set results for different variations of our
model. Table 6 shows the results from the competition. While the best methods based on
standard vision features achieve a top-5 error rate of about 26%, convolutional nets with
dropout achieve a test error of about 16% which is a staggering difference. Figure 6 shows
some examples of predictions made by our model. We can see that the model makes very
reasonable predictions, even when its best guess is not correct.
6.2 Results on TIMIT
Next, we applied dropout to a speech recognition task. We use the TIMIT data set which
consists of recordings from 680 speakers covering 8 major dialects of American English
reading ten phonetically-rich sentences in a controlled noise-free environment. Dropout
neural networks were trained on windows of 21 log-filter bank frames to predict the label
of the central frame. No speaker dependent operations were performed. Appendix B.4
describes the data preprocessing and training details. Table 7 compares dropout neural
1940
Dropout
nets with other models. A 6-layer net gives a phone error rate of 23.4%. Dropout further
improves it to 21.8%. We also trained dropout nets starting from pretrained weights. A
4-layer net pretrained with a stack of RBMs get a phone error rate of 22.7%. With dropout,
this reduces to 19.7%. Similarly, for an 8-layer net the error reduces from 20.5% to 19.7%.
Method Phone Error Rate%
NN (6 layers) (Mohamed et al., 2010) 23.4
Dropout NN (6 layers) 21.8
DBN-pretrained NN (4 layers) 22.7
DBN-pretrained NN (6 layers) (Mohamed et al., 2010) 22.4
DBN-pretrained NN (8 layers) (Mohamed et al., 2010) 20.7
mcRBM-DBN-pretrained NN (5 layers) (Dahl et al., 2010) 20.5
DBN-pretrained NN (4 layers) + dropout 19.7
DBN-pretrained NN (8 layers) + dropout 19.7
Table 7: Phone error rate on the TIMIT core test set.
6.3 Results on a Text Data Set
To test the usefulness of dropout in the text domain, we used dropout networks to train a
document classifier. We used a subset of the Reuters-RCV1 data set which is a collection of
over 800,000 newswire articles from Reuters. These articles cover a variety of topics. The
task is to take a bag of words representation of a document and classify it into 50 disjoint
topics. Appendix B.5 describes the setup in more detail. Our best neural net which did
not use dropout obtained an error rate of 31.05%. Adding dropout reduced the error to
29.62%. We found that the improvement was much smaller compared to that for the vision
and speech data sets.
6.4 Comparison with Bayesian Neural Networks
Dropout can be seen as a way of doing an equally-weighted averaging of exponentially many
models with shared weights. On the other hand, Bayesian neural networks (Neal, 1996) are
the proper way of doing model averaging over the space of neural network structures and
parameters. In dropout, each model is weighted equally, whereas in a Bayesian neural
network each model is weighted taking into account the prior and how well the model fits
the data, which is the more correct approach. Bayesian neural nets are extremely useful for
solving problems in domains where data is scarce such as medical diagnosis, genetics, drug
discovery and other computational biology applications. However, Bayesian neural nets are
slow to train and difficult to scale to very large network sizes. Besides, it is expensive to
get predictions from many large nets at test time. On the other hand, dropout neural nets
are much faster to train and use at test time. In this section, we report experiments that
compare Bayesian neural nets with dropout neural nets on a small data set where Bayesian
neural networks are known to perform well and obtain state-of-the-art results. The aim is
to analyze how much does dropout lose compared to Bayesian neural nets.
The data set that we use (Xiong et al., 2011) comes from the domain of genetics. The
task is to predict the occurrence of alternative splicing based on RNA features. Alternative
splicing is a significant cause of cellular diversity in mammalian tissues. Predicting the
1941
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Method Code Quality (bits)
Neural Network (early stopping) (Xiong et al., 2011) 440
Regression, PCA (Xiong et al., 2011) 463
SVM, PCA (Xiong et al., 2011) 487
Neural Network with dropout 567
Bayesian Neural Network (Xiong et al., 2011) 623
Table 8: Results on the Alternative Splicing Data Set.
occurrence of alternate splicing in certain tissues under different conditions is important for
understanding many human diseases. Given the RNA features, the task is to predict the
probability of three splicing related events that biologists care about. The evaluation metric
is Code Quality which is a measure of the negative KL divergence between the target and
the predicted probability distributions (higher is better). Appendix B.6 includes a detailed
description of the data set and this performance metric.
Table 8 summarizes the performance of different models on this data set. Xiong et al.
(2011) used Bayesian neural nets for this task. As expected, we found that Bayesian neural
nets perform better than dropout. However, we see that dropout improves significantly
upon the performance of standard neural nets and outperforms all other methods. The
challenge in this data set is to prevent overfitting since the size of the training set is small.
One way to prevent overfitting is to reduce the input dimensionality using PCA. Thereafter,
standard techniques such as SVMs or logistic regression can be used. However, with dropout
we were able to prevent overfitting without the need to do dimensionality reduction. The
dropout nets are very large (1000s of hidden units) compared to a few tens of units in the
Bayesian network. This shows that dropout has a strong regularizing effect.
6.5 Comparison with Standard Regularizers
Several regularization methods have been proposed for preventing overfitting in neural networks. These include L2 weight decay (more generally Tikhonov regularization (Tikhonov,
1943)), lasso (Tibshirani, 1996), KL-sparsity and max-norm regularization. Dropout can
be seen as another way of regularizing neural networks. In this section we compare dropout
with some of these regularization methods using the MNIST data set.
The same network architecture (784-1024-1024-2048-10) with ReLUs was trained using stochastic gradient descent with different regularizations. Table 9 shows the results.
The values of different hyperparameters associated with each kind of regularization (decay
constants, target sparsity, dropout rate, max-norm upper bound) were obtained using a
validation set. We found that dropout combined with max-norm regularization gives the
lowest generalization error.
7. Salient Features
The experiments described in the previous section provide strong evidence that dropout
is a useful technique for improving neural networks. In this section, we closely examine
how dropout affects a neural network. We analyze the effect of dropout on the quality of
features produced. We see how dropout affects the sparsity of hidden unit activations. We
1942
Dropout
Method Test Classification error %
L2 1.62
L2 + L1 applied towards the end of training 1.60
L2 + KL-sparsity 1.55
Max-norm 1.35
Dropout + L2 1.25
Dropout + Max-norm 1.05
Table 9: Comparison of different regularization methods on MNIST.
also see how the advantages obtained from dropout vary with the probability of retaining
units, size of the network and the size of the training set. These observations give some
insight into why dropout works so well.
7.1 Effect on Features
(a) Without dropout (b) Dropout with p = 0.5.
Figure 7: Features learned on MNIST with one hidden layer autoencoders having 256 rectified
linear units.
In a standard neural network, the derivative received by each parameter tells it how it
should change so the final loss function is reduced, given what all other units are doing.
Therefore, units may change in a way that they fix up the mistakes of the other units.
This may lead to complex co-adaptations. This in turn leads to overfitting because these
co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit,
dropout prevents co-adaptation by making the presence of other hidden units unreliable.
Therefore, a hidden unit cannot rely on other specific units to correct its mistakes. It must
perform well in a wide variety of different contexts provided by the other hidden units. To
observe this effect directly, we look at the first level features learned by neural networks
trained on visual tasks with and without dropout.
1943
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Figure 7a shows features learned by an autoencoder on MNIST with a single hidden
layer of 256 rectified linear units without dropout. Figure 7b shows the features learned by
an identical autoencoder which used dropout in the hidden layer with p = 0.5. Both autoencoders had similar test reconstruction errors. However, it is apparent that the features
shown in Figure 7a have co-adapted in order to produce good reconstructions. Each hidden
unit on its own does not seem to be detecting a meaningful feature. On the other hand, in
Figure 7b, the hidden units seem to detect edges, strokes and spots in different parts of the
image. This shows that dropout does break up co-adaptations, which is probably the main
reason why it leads to lower generalization errors.
7.2 Effect on Sparsity
(a) Without dropout (b) Dropout with p = 0.5.
Figure 8: Effect of dropout on sparsity. ReLUs were used for both models. Left: The histogram
of mean activations shows that most units have a mean activation of about 2.0. The
histogram of activations shows a huge mode away from zero. Clearly, a large fraction of
units have high activation. Right: The histogram of mean activations shows that most
units have a smaller mean mean activation of about 0.7. The histogram of activations
shows a sharp peak at zero. Very few units have high activation.
We found that as a side-effect of doing dropout, the activations of the hidden units
become sparse, even when no sparsity inducing regularizers are present. Thus, dropout automatically leads to sparse representations. To observe this effect, we take the autoencoders
trained in the previous section and look at the sparsity of hidden unit activations on a random mini-batch taken from the test set. Figure 8a and Figure 8b compare the sparsity for
the two models. In a good sparse model, there should only be a few highly activated units
for any data case. Moreover, the average activation of any unit across data cases should
be low. To assess both of these qualities, we plot two histograms for each model. For each
model, the histogram on the left shows the distribution of mean activations of hidden units
across the minibatch. The histogram on the right shows the distribution of activations of
the hidden units.
Comparing the histograms of activations we can see that fewer hidden units have high
activations in Figure 8b compared to Figure 8a, as seen by the significant mass away from
1944
Dropout
zero for the net that does not use dropout. The mean activations are also smaller for the
dropout net. The overall mean activation of hidden units is close to 2.0 for the autoencoder
without dropout but drops to around 0.7 when dropout is used.
7.3 Effect of Dropout Rate
Dropout has a tunable hyperparameter p (the probability of retaining a unit in the network).
In this section, we explore the effect of varying this hyperparameter. The comparison is
done in two situations.
1. The number of hidden units is held constant.
2. The number of hidden units is changed so that the expected number of hidden units
that will be retained after dropout is held constant.
In the first case, we train the same network architecture with different amounts of
dropout. We use a 784-2048-2048-2048-10 architecture. No input dropout was used. Figure 9a shows the test error obtained as a function of p. If the architecture is held constant,
having a small p means very few units will turn on during training. It can be seen that this
has led to underfitting since the training error is also high. We see that as p increases, the
error goes down. It becomes flat when 0.4 ‚â§ p ‚â§ 0.8 and then increases as p becomes close
to 1.
0.0 0.2 0.4 0.6 0.8 1.0
Probability of retaining a unit (p)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Classification Error %
Test Error
Training Error
(a) Keeping n fixed.
0.0 0.2 0.4 0.6 0.8 1.0
Probability of retaining a unit (p)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Classification Error %
Test Error
Training Error
(b) Keeping pn fixed.
Figure 9: Effect of changing dropout rates on MNIST.
Another interesting setting is the second case in which the quantity pn is held constant
where n is the number of hidden units in any particular layer. This means that networks
that have small p will have a large number of hidden units. Therefore, after applying
dropout, the expected number of units that are present will be the same across different
architectures. However, the test networks will be of different sizes. In our experiments,
we set pn = 256 for the first two hidden layers and pn = 512 for the last hidden layer.
Figure 9b shows the test error obtained as a function of p. We notice that the magnitude
of errors for small values of p has reduced by a lot compared to Figure 9a (for p = 0.1 it fell
from 2.7% to 1.7%). Values of p that are close to 0.6 seem to perform best for this choice
of pn but our usual default value of 0.5 is close to optimal.
1945
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
7.4 Effect of Data Set Size
One test of a good regularizer is that it should make it possible to get good generalization
error from models with a large number of parameters trained on small data sets. This
section explores the effect of changing the data set size when dropout is used with feedforward networks. Huge neural networks trained in the standard way overfit massively on
small data sets. To see if dropout can help, we run classification experiments on MNIST
and vary the amount of data given to the network.
10
2 10
3 10
4 10
5
Dataset size
0
5
10
15
20
25
30
Classification Error %
With dropout
Without dropout
Figure 10: Effect of varying data set size.
The results of these experiments are
shown in Figure 10. The network was given
data sets of size 100, 500, 1K, 5K, 10K
and 50K chosen randomly from the MNIST
training set. The same network architecture (784-1024-1024-2048-10) was used for
all data sets. Dropout with p = 0.5 was performed at all the hidden layers and p = 0.8
at the input layer. It can be observed that
for extremely small data sets (100, 500)
dropout does not give any improvements.
The model has enough parameters that it
can overfit on the training data, even with
all the noise coming from dropout. As the
size of the data set is increased, the gain
from doing dropout increases up to a point and then declines. This suggests that for any
given architecture and dropout rate, there is a ‚Äúsweet spot‚Äù corresponding to some amount
of data that is large enough to not be memorized in spite of the noise but not so large that
overfitting is not a problem anyways.
7.5 Monte-Carlo Model Averaging vs. Weight Scaling
0 20 40 60 80 100 120
Number of samples used for Monte-Carlo averaging (k)
1.00
1.05
1.10
1.15
1.20
1.25
1.30
1.35
Test Classification error %
Monte-Carlo Model Averaging
Approximate averaging by weight scaling
Figure 11: Monte-Carlo model averaging vs.
weight scaling.
The efficient test time procedure that we
propose is to do an approximate model combination by scaling down the weights of the
trained neural network. An expensive but
more correct way of averaging the models
is to sample k neural nets using dropout for
each test case and average their predictions.
As k ‚Üí ‚àû, this Monte-Carlo model average
gets close to the true model average. It is interesting to see empirically how many samples k are needed to match the performance
of the approximate averaging method. By
computing the error for different values of k
we can see how quickly the error rate of the
finite-sample average approaches the error
rate of the true model average.
1946
Dropout
We again use the MNIST data set and do classification by averaging the predictions
of k randomly sampled neural networks. Figure 11 shows the test error rate obtained for
different values of k. This is compared with the error obtained using the weight scaling
method (shown as a horizontal line). It can be seen that around k = 50, the Monte-Carlo
method becomes as good as the approximate method. Thereafter, the Monte-Carlo method
is slightly better than the approximate method but well within one standard deviation of
it. This suggests that the weight scaling method is a fairly good approximation of the true
model average.
8. Dropout Restricted Boltzmann Machines
Besides feed-forward neural networks, dropout can also be applied to Restricted Boltzmann
Machines (RBM). In this section, we formally describe this model and show some results
to illustrate its key properties.
8.1 Model Description
Consider an RBM with visible units v ‚àà {0, 1}
D and hidden units h ‚àà {0, 1}
F . It defines
the following probability distribution
P(h, v; Œ∏) = 1
Z(Œ∏)
exp(v
>Wh + a
>h + b
>v).
Where Œ∏ = {W, a, b} represents the model parameters and Z is the partition function.
Dropout RBMs are RBMs augmented with a vector of binary random variables r ‚àà
{0, 1}
F . Each random variable rj takes the value 1 with probability p, independent of
others. If rj takes the value 1, the hidden unit hj is retained, otherwise it is dropped from
the model. The joint distribution defined by a Dropout RBM can be expressed as
P(r, h, v; p, Œ∏) = P(r; p)P(h, v|r; Œ∏),
P(r; p) = Y
F
j=1
p
rj
(1 ‚àí p)
1‚àírj
,
P(h, v|r; Œ∏) = 1
Z0(Œ∏, r)
exp(v
>Wh + a
>h + b
>v)
Y
F
j=1
g(hj , rj ),
g(hj , rj ) = 1(rj = 1) + 1(rj = 0)1(hj = 0).
Z
0
(Œ∏, r) is the normalization constant. g(hj , rj ) imposes the constraint that if rj = 0,
hj must be 0. The distribution over h, conditioned on v and r is factorial
P(h|r, v) = Y
F
j=1
P(hj |rj , v),
P(hj = 1|rj , v) = 1(rj = 1)œÉ

bj +
X
i
Wijvi
!
.
1947
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
(a) Without dropout (b) Dropout with p = 0.5.
Figure 12: Features learned on MNIST by 256 hidden unit RBMs. The features are ordered by L2
norm.
The distribution over v conditioned on h is same as that of an RBM
P(v|h) = Y
D
i=1
P(vi
|h),
P(vi = 1|h) = œÉ
Ô£´
Ô£≠ai +
X
j
Wijhj
Ô£∂
Ô£∏ .
Conditioned on r, the distribution over {v, h} is same as the distribution that an RBM
would impose, except that the units for which rj = 0 are dropped from h. Therefore, the
Dropout RBM model can be seen as a mixture of exponentially many RBMs with shared
weights each using a different subset of h.
8.2 Learning Dropout RBMs
Learning algorithms developed for RBMs such as Contrastive Divergence (Hinton et al.,
2006) can be directly applied for learning Dropout RBMs. The only difference is that r is
first sampled and only the hidden units that are retained are used for training. Similar to
dropout neural networks, a different r is sampled for each training case in every minibatch.
In our experiments, we use CD-1 for training dropout RBMs.
8.3 Effect on Features
Dropout in feed-forward networks improved the quality of features by reducing co-adaptations.
This section explores whether this effect transfers to Dropout RBMs as well.
Figure 12a shows features learned by a binary RBM with 256 hidden units. Figure 12b
shows features learned by a dropout RBM with the same number of hidden units. Features
1948
Dropout
(a) Without dropout (b) Dropout with p = 0.5.
Figure 13: Effect of dropout on sparsity. Left: The activation histogram shows that a large number of units have activations away from zero. Right: A large number of units have
activations close to zero and very few units have high activation.
learned by the dropout RBM appear qualitatively different in the sense that they seem to
capture features that are coarser compared to the sharply defined stroke-like features in the
standard RBM. There seem to be very few dead units in the dropout RBM relative to the
standard RBM.
8.4 Effect on Sparsity
Next, we investigate the effect of dropout RBM training on sparsity of the hidden unit
activations. Figure 13a shows the histograms of hidden unit activations and their means on
a test mini-batch after training an RBM. Figure 13b shows the same for dropout RBMs.
The histograms clearly indicate that the dropout RBMs learn much sparser representations
than standard RBMs even when no additional sparsity inducing regularizer is present.
9. Marginalizing Dropout
Dropout can be seen as a way of adding noise to the states of hidden units in a neural
network. In this section, we explore the class of models that arise as a result of marginalizing
this noise. These models can be seen as deterministic versions of dropout. In contrast to
standard (‚ÄúMonte-Carlo‚Äù) dropout, these models do not need random bits and it is possible
to get gradients for the marginalized loss functions. In this section, we briefly explore these
models.
Deterministic algorithms have been proposed that try to learn models that are robust to
feature deletion at test time (Globerson and Roweis, 2006). Marginalization in the context
of denoising autoencoders has been explored previously (Chen et al., 2012). The marginalization of dropout noise in the context of linear regression was discussed in Srivastava (2013).
Wang and Manning (2013) further explored the idea of marginalizing dropout to speed-up
training. van der Maaten et al. (2013) investigated different input noise distributions and
1949
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
the regularizers obtained by marginalizing this noise. Wager et al. (2013) describes how
dropout can be seen as an adaptive regularizer.
9.1 Linear Regression
First we explore a very simple case of applying dropout to the classical problem of linear
regression. Let X ‚àà R
N√óD be a data matrix of N data points. y ‚àà R
N be a vector of
targets. Linear regression tries to find a w ‚àà R
D that minimizes
||y ‚àí Xw||2
.
When the input X is dropped out such that any input dimension is retained with
probability p, the input can be expressed as R‚àóX where R ‚àà {0, 1}
N√óD is a random matrix
with Rij ‚àº Bernoulli(p) and ‚àó denotes an element-wise product. Marginalizing the noise,
the objective function becomes
minimize
w
ER‚àºBernoulli(p)
||y ‚àí (R ‚àó X)w||2

.
This reduces to
minimize
w
||y ‚àí pXw||2 + p(1 ‚àí p)||Œìw||2
,
where Œì = (diag(X>X))1/2
. Therefore, dropout with linear regression is equivalent, in
expectation, to ridge regression with a particular form for Œì. This form of Œì essentially
scales the weight cost for weight wi by the standard deviation of the ith dimension of the
data. If a particular data dimension varies a lot, the regularizer tries to squeeze its weight
more.
Another interesting way to look at this objective is to absorb the factor of p into w.
This leads to the following form
minimize
w
||y ‚àí Xwe ||2 +
1 ‚àí p
p
||Œìwe ||2
,
where we = pw. This makes the dependence of the regularization constant on p explicit.
For p close to 1, all the inputs are retained and the regularization constant is small. As
more dropout is done (by decreasing p), the regularization constant grows larger.
9.2 Logistic Regression and Deep Networks
For logistic regression and deep neural nets, it is hard to obtain a closed form marginalized
model. However, Wang and Manning (2013) showed that in the context of dropout applied
to logistic regression, the corresponding marginalized model can be trained approximately.
Under reasonable assumptions, the distributions over the inputs to the logistic unit and over
the gradients of the marginalized model are Gaussian. Their means and variances can be
computed efficiently. This approximate marginalization outperforms Monte-Carlo dropout
in terms of training time and generalization performance.
However, the assumptions involved in this technique become successively weaker as more
layers are added. Therefore, the results are not directly applicable to deep networks.
1950
Dropout
Data Set Architecture Bernoulli dropout Gaussian dropout
MNIST 2 layers, 1024 units each 1.08 ¬± 0.04 0.95 ¬± 0.04
CIFAR-10 3 conv + 2 fully connected layers 12.6 ¬± 0.1 12.5 ¬± 0.1
Table 10: Comparison of classification error % with Bernoulli and Gaussian dropout. For MNIST,
the Bernoulli model uses p = 0.5 for the hidden units and p = 0.8 for the input units.
For CIFAR-10, we use p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) going from the input layer to the
top. The value of œÉ for the Gaussian dropout models was set to be q1‚àíp
p
. Results were
averaged over 10 different random seeds.
10. Multiplicative Gaussian Noise
Dropout involves multiplying hidden activations by Bernoulli distributed random variables
which take the value 1 with probability p and 0 otherwise. This idea can be generalized
by multiplying the activations with random variables drawn from other distributions. We
recently discovered that multiplying by a random variable drawn from N (1, 1) works just
as well, or perhaps better than using Bernoulli noise. This new form of dropout amounts
to adding a Gaussian distributed random variable with zero mean and standard deviation
equal to the activation of the unit. That is, each hidden activation hi
is perturbed to
hi + hir where r ‚àº N (0, 1), or equivalently hir
0 where r
0 ‚àº N (1, 1). We can generalize
this to r
0 ‚àº N (1, œÉ2
) where œÉ becomes an additional hyperparameter to tune, just like p
was in the standard (Bernoulli) dropout. The expected value of the activations remains
unchanged, therefore no weight scaling is required at test time.
In this paper, we described dropout as a method where we retain units with probability p
at training time and scale down the weights by multiplying them by a factor of p at test time.
Another way to achieve the same effect is to scale up the retained activations by multiplying
by 1/p at training time and not modifying the weights at test time. These methods are
equivalent with appropriate scaling of the learning rate and weight initializations at each
layer.
Therefore, dropout can be seen as multiplying hi by a Bernoulli random variable rb that
takes the value 1/p with probability p and 0 otherwise. E[rb] = 1 and V ar[rb] = (1 ‚àí p)/p.
For the Gaussian multiplicative noise, if we set œÉ
2 = (1 ‚àí p)/p, we end up multiplying
hi by a random variable rg, where E[rg] = 1 and V ar[rg] = (1 ‚àí p)/p. Therefore, both
forms of dropout can be set up so that the random variable being multiplied by has the
same mean and variance. However, given these first and second order moments, rg has the
highest entropy and rb has the lowest. Both these extremes work well, although preliminary
experimental results shown in Table 10 suggest that the high entropy case might work
slightly better. For each layer, the value of œÉ in the Gaussian model was set to be q1‚àíp
p
using the p from the corresponding layer in the Bernoulli model.
11. Conclusion
Dropout is a technique for improving neural networks by reducing overfitting. Standard
backpropagation learning builds up brittle co-adaptations that work for the training data
but do not generalize to unseen data. Random dropout breaks up these co-adaptations by
1951
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
making the presence of any particular hidden unit unreliable. This technique was found
to improve the performance of neural nets in a wide variety of application domains including object classification, digit recognition, speech recognition, document classification and
analysis of computational biology data. This suggests that dropout is a general technique
and is not specific to any domain. Methods that use dropout achieve state-of-the-art results on SVHN, ImageNet, CIFAR-100 and MNIST. Dropout considerably improved the
performance of standard neural nets on other data sets as well.
This idea can be extended to Restricted Boltzmann Machines and other graphical models. The central idea of dropout is to take a large model that overfits easily and repeatedly
sample and train smaller sub-models from it. RBMs easily fit into this framework. We developed Dropout RBMs and empirically showed that they have certain desirable properties.
One of the drawbacks of dropout is that it increases training time. A dropout network
typically takes 2-3 times longer to train than a standard neural network of the same architecture. A major cause of this increase is that the parameter updates are very noisy.
Each training case effectively tries to train a different random architecture. Therefore, the
gradients that are being computed are not gradients of the final architecture that will be
used at test time. Therefore, it is not surprising that training takes a long time. However,
it is likely that this stochasticity prevents overfitting. This creates a trade-off between overfitting and training time. With more training time, one can use high dropout and suffer less
overfitting. However, one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the
dropout procedure, in expectation. We showed that for linear regression this regularizer is
a modified form of L2 regularization. For more complicated models, it is not obvious how to
obtain an equivalent regularizer. Speeding up dropout is an interesting direction for future
work.
Acknowledgments
This research was supported by OGS, NSERC and an Early Researcher Award.
Appendix A. A Practical Guide for Training Dropout Networks
Neural networks are infamous for requiring extensive hyperparameter tuning. Dropout
networks are no exception. In this section, we describe heuristics that might be useful for
applying dropout.
A.1 Network Size
It is to be expected that dropping units will reduce the capacity of a neural network. If
n is the number of hidden units in any layer and p is the probability of retaining a unit,
then instead of n hidden units, only pn units will be present after dropout, in expectation.
Moreover, this set of pn units will be different each time and the units are not allowed to
build co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neural
net on any given task, a good dropout net should have at least n/p units. We found this to
be a useful heuristic for setting the number of hidden units in both convolutional and fully
connected networks.
1952
Dropout
A.2 Learning Rate and Momentum
Dropout introduces a significant amount of noise in the gradients compared to standard
stochastic gradient descent. Therefore, a lot of gradients tend to cancel each other. In
order to make up for this, a dropout net should typically use 10-100 times the learning rate
that was optimal for a standard neural net. Another way to reduce the effect the noise is
to use a high momentum. While momentum values of 0.9 are common for standard nets,
with dropout we found that values around 0.95 to 0.99 work quite a lot better. Using high
learning rate and/or momentum significantly speed up learning.
A.3 Max-norm Regularization
Though large momentum and learning rate speed up learning, they sometimes cause the
network weights to grow very large. To prevent this, we can use max-norm regularization.
This constrains the norm of the vector of incoming weights at each hidden unit to be bound
by a constant c. Typical values of c range from 3 to 4.
A.4 Dropout Rate
Dropout introduces an extra hyperparameter‚Äîthe probability of retaining a unit p. This
hyperparameter controls the intensity of dropout. p = 1, implies no dropout and low values
of p mean more dropout. Typical values of p for hidden units are in the range 0.5 to 0.8.
For input layers, the choice depends on the kind of input. For real-valued inputs (image
patches or speech frames), a typical value is 0.8. For hidden layers, the choice of p is coupled
with the choice of number of hidden units n. Smaller p requires big n which slows down
the training and leads to underfitting. Large p may not produce enough dropout to prevent
overfitting.
Appendix B. Detailed Description of Experiments and Data Sets
.
This section describes the network architectures and training details for the experimental
results reported in this paper. The code for reproducing these results can be obtained from
http://www.cs.toronto.edu/~nitish/dropout. The implementation is GPU-based. We
used the excellent CUDA libraries‚Äîcudamat (Mnih, 2009) and cuda-convnet (Krizhevsky
et al., 2012) to implement our networks.
B.1 MNIST
The MNIST data set consists of 60,000 training and 10,000 test examples each representing
a 28√ó28 digit image. We held out 10,000 random training images for validation. Hyperparameters were tuned on the validation set such that the best validation error was produced
after 1 million weight updates. The validation set was then combined with the training set
and training was done for 1 million weight updates. This net was used to evaluate the performance on the test set. This way of using the validation set was chosen because we found
that it was easy to set up hyperparameters so that early stopping was not required at all.
Therefore, once the hyperparameters were fixed, it made sense to combine the validation
and training sets and train for a very long time.
1953
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
The architectures shown in Figure 4 include all combinations of 2, 3, and 4 layer networks
with 1024 and 2048 units in each layer. Thus, there are six architectures in all. For all the
architectures (including the ones reported in Table 2), we used p = 0.5 in all hidden layers
and p = 0.8 in the input layer. A final momentum of 0.95 and weight constraints with c = 2
was used in all the layers.
To test the limits of dropout‚Äôs regularization power, we also experimented with 2 and 3
layer nets having 4096 and 8192 units. 2 layer nets gave improvements as shown in Table 2.
However, the three layer nets performed slightly worse than 2 layer ones with the same
level of dropout. When we increased dropout, performance improved but not enough to
outperform the 2 layer nets.
B.2 SVHN
The SVHN data set consists of approximately 600,000 training images and 26,000 test
images. The training set consists of two parts‚ÄîA standard labeled training set and another
set of labeled examples that are easy. A validation set was constructed by taking examples
from both the parts. Two-thirds of it were taken from the standard set (400 per class) and
one-third from the extra set (200 per class), a total of 6000 samples. This same process
is used by Sermanet et al. (2012). The inputs were RGB pixels normalized to have zero
mean and unit variance. Other preprocessing techniques such as global or local contrast
normalization or ZCA whitening did not give any noticeable improvements.
The best architecture that we found uses three convolutional layers each followed by
a max-pooling layer. The convolutional layers have 96, 128 and 256 filters respectively.
Each convolutional layer has a 5 √ó 5 receptive field applied with a stride of 1 pixel. Each
max pooling layer pools 3 √ó 3 regions at strides of 2 pixels. The convolutional layers are
followed by two fully connected hidden layers having 2048 units each. All units use the
rectified linear activation function. Dropout was applied to all the layers of the network
with the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the
different layers of the network (going from input to convolutional layers to fully connected
layers). In addition, the max-norm constraint with c = 4 was used for all the weights. A
momentum of 0.95 was used in all the layers. These hyperparameters were tuned using a
validation set. Since the training set was quite large, we did not combine the validation
set with the training set for final training. We reported test error of the model that had
smallest validation error.
B.3 CIFAR-10 and CIFAR-100
The CIFAR-10 and CIFAR-100 data sets consists of 50,000 training and 10,000 test images
each. They have 10 and 100 image categories respectively. These are 32 √ó 32 color images.
We used 5,000 of the training images for validation. We followed the procedure similar
to MNIST, where we found the best hyperparameters using the validation set and then
combined it with the training set. The images were preprocessed by doing global contrast
normalization in each color channel followed by ZCA whitening. Global contrast normalization means that for image and each color channel in that image, we compute the mean
of the pixel intensities and subtract it from the channel. ZCA whitening means that we
mean center the data, rotate it onto its principle components, normalize each component
1954
Dropout
and then rotate it back. The network architecture and dropout rates are same as that for
SVHN, except the learning rates for the input layer which had to be set to smaller values.
B.4 TIMIT
The open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into logfilter banks. A monophone system was trained to do a forced alignment and to get labels for
speech frames. Dropout neural networks were trained on windows of 21 consecutive frames
to predict the label of the central frame. No speaker dependent operations were performed.
The inputs were mean centered and normalized to have unit variance.
We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.
Max-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a
high learning rate of 0.1 was used. The learning rate was decayed as 0(1 + t/T)
‚àí1
. For
DBN pretraining, we trained RBMs using CD-1. The variance of each input unit for the
Gaussian RBM was fixed to 1. For finetuning the DBN with dropout, we found that in
order to get the best results it was important to use a smaller learning rate (about 0.01).
Adding max-norm constraints did not give any improvements.
B.5 Reuters
The Reuters RCV1 corpus contains more than 800,000 documents categorized into 103
classes. These classes are arranged in a tree hierarchy. We created a subset of this data set
consisting of 402,738 articles and a vocabulary of 2000 words comprising of 50 categories
in which each document belongs to exactly one class. The data was split into equal sized
training and test sets. We tried many network architectures and found that dropout gave
improvements in classification accuracy over all of them. However, the improvement was
not as significant as that for the image and speech data sets. This might be explained by
the fact that this data set is quite big (more than 200,000 training examples) and overfitting
is not a very serious problem.
B.6 Alternative Splicing
The alternative splicing data set consists of data for 3665 cassette exons, 1014 RNA features
and 4 tissue types derived from 27 mouse tissues. For each input, the target consists of 4
softmax units (one for tissue type). Each softmax unit has 3 states (inc, exc, nc) which are
of the biological importance. For each softmax unit, the aim is to predict a distribution over
these 3 states that matches the observed distribution from wet lab experiments as closely
as possible. The evaluation metric is Code Quality which is defined as
|data points
X
|
i=1
X
t‚ààtissue types
X
s‚àà{inc, exc, nc}
p
s
i,t log(q
s
t
(ri)
p¬Ø
s
),
where, p
s
i,t is the target probability for state s and tissue type t in input i; q
s
t
(ri) is the
predicted probability for state s in tissue type t for input ri and ¬Øp
s
is the average of p
s
i,t
over i and t.
A two layer dropout network with 1024 units in each layer was trained on this data set.
A value of p = 0.5 was used for the hidden layer and p = 0.7 for the input layer. Max-norm
regularization with high decaying learning rates was used. Results were averaged across the
same 5 folds used by Xiong et al. (2011).
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
How Cross-Validation Works
Cross-Validation vs. Simple Validation Split
Real-World Applications
Glossary
Cross-Validation
Discover the power of cross-validation in machine learning to enhance model accuracy, prevent overfitting, and ensure robust performance.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Cross-Validation is a powerful model evaluation technique in machine learning (ML) used to assess how the results of a statistical analysis will generalize to an independent dataset. It is a resampling procedure used to evaluate ML models on a limited data sample. The primary goal is to prevent overfitting, where a model learns the training data so well that it performs poorly on new, unseen data. By simulating how a model would perform in the real world, Cross-Validation provides a more robust and reliable estimate of model performance.

How Cross-Validation Works
The most common method of Cross-Validation is K-Fold Cross-Validation. This process involves partitioning a single dataset into multiple parts:

Splitting the Data: The entire training dataset is randomly split into 'k' equal-sized subsets, or "folds."
Iterative Training and Validation: The model is trained 'k' times. In each iteration, one of the folds is held out as the validation set, and the model is trained on the remaining k-1 folds.
Performance Evaluation: The model's performance is evaluated on the held-out fold. Key metrics, such as accuracy or mean Average Precision (mAP), are recorded for each iteration.
Averaging Results: After completing all 'k' iterations, the performance metrics are averaged to produce a single, more stable estimation of the model's effectiveness.
This approach ensures that every data point gets to be in a validation set exactly once and in a training set k-1 times. A detailed guide on implementation can be found in the Ultralytics K-Fold Cross-Validation guide.

Cross-Validation vs. Simple Validation Split
In a typical ML project, data is divided into training, validation, and test sets.

Validation Data: Used during the training phase for hyperparameter tuning and to make decisions about the model architecture.
Test Data: Used only after all training and tuning are complete to provide a final, unbiased assessment of the model's generalization ability.
A simple train/validation split can sometimes be misleading if the validation set, by chance, contains samples that are particularly easy or difficult. Cross-Validation overcomes this by using every part of the dataset for both training and validation, providing a more reliable measure of the model's ability to generalize. This makes it particularly useful when the amount of available data is limited. Popular frameworks like Scikit-learn provide robust implementations of cross-validation techniques.

Real-World Applications
Cross-Validation is indispensable in building dependable AI systems across various domains:

Medical Image Analysis: When developing a Convolutional Neural Network (CNN) for medical image analysis, such as detecting tumors in brain scans using datasets like the Brain Tumor dataset, CV is used to rigorously evaluate the model's diagnostic accuracy and generalization across diverse patient data. This robust evaluation is critical before considering clinical trials or seeking regulatory approval from bodies like the FDA.
Autonomous Vehicles: For object detection models like Ultralytics YOLO used in autonomous vehicles, CV helps ensure reliable performance in detecting pedestrians, cyclists, and other vehicles across various environmental conditions. This validation on complex datasets like Argoverse is critical before model deployment in safety-critical systems like those in AI in Automotive solutions.
Other applications include evaluating models for image segmentation, natural language processing (NLP) tasks like sentiment analysis, and risk assessment in financial modeling. Platforms like Ultralytics HUB can help manage the experiments and artifacts produced during such evaluation techniques, streamlining the development lifecycle.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
K-Fold Cross Validation




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Guides
YOLO Common Issues
YOLO Performance Metrics
YOLO Thread-Safe Inference
YOLO Data Augmentation
Model Deployment Options
Model YAML Configuration Guide
K-Fold Cross Validation
Hyperparameter Tuning
SAHI Tiled Inference
AzureML Quickstart
Conda Quickstart
Docker Quickstart
Raspberry Pi
NVIDIA Jetson
DeepStream on NVIDIA Jetson
Triton Inference Server
Isolating Segmentation Objects
Edge TPU on Raspberry Pi
Viewing Inference Images in a Terminal
OpenVINO Latency vs Throughput modes
ROS Quickstart
Steps of a Computer Vision Project
Defining A Computer Vision Project's Goals
Data Collection and Annotation
Preprocessing Annotated Data
Tips for Model Training
Insights on Model Evaluation and Fine-Tuning
A Guide on Model Testing
Best Practices for Model Deployment
Maintaining Your Computer Vision Model
Deploying YOLO on Vertex AI in Docker container
Explorer
Explorer API
Explorer Dashboard Demo
VOC Exploration Example
YOLOv5
Quickstart
Environments
Tutorials
Table of contents
Introduction
Setup
Generating Feature Vectors for Object Detection Dataset
K-Fold Dataset Split
Save Records (Optional)
Train YOLO using K-Fold Data Splits
Conclusion
FAQ
What is K-Fold Cross Validation and why is it useful in object detection?
How do I implement K-Fold Cross Validation using Ultralytics YOLO?
Why should I use Ultralytics YOLO for object detection?
How can I ensure my annotations are in the correct format for Ultralytics YOLO?
Can I use K-Fold Cross Validation with custom datasets other than Fruit Detection?
K-Fold Cross Validation with Ultralytics
Introduction
This comprehensive guide illustrates the implementation of K-Fold Cross Validation for object detection datasets within the Ultralytics ecosystem. We'll leverage the YOLO detection format and key Python libraries such as sklearn, pandas, and PyYaml to guide you through the necessary setup, the process of generating feature vectors, and the execution of a K-Fold dataset split.

K-Fold Cross Validation Overview

Whether your project involves the Fruit Detection dataset or a custom data source, this tutorial aims to help you comprehend and apply K-Fold Cross Validation to bolster the reliability and robustness of your machine learning models. While we're applying k=5 folds for this tutorial, keep in mind that the optimal number of folds can vary depending on your dataset and the specifics of your project.

Without further ado, let's dive in!

Setup
Your annotations should be in the YOLO detection format.

This guide assumes that annotation files are locally available.

For our demonstration, we use the Fruit Detection dataset.

This dataset contains a total of 8479 images.
It includes 6 class labels, each with its total instance counts listed below.
Class Label Instance Count
Apple 7049
Grapes  7202
Pineapple 1613
Orange  15549
Banana  3536
Watermelon  1976
Necessary Python packages include:

ultralytics
sklearn
pandas
pyyaml
This tutorial operates with k=5 folds. However, you should determine the best number of folds for your specific dataset.

Initiate a new Python virtual environment (venv) for your project and activate it. Use pip (or your preferred package manager) to install:

The Ultralytics library: pip install -U ultralytics. Alternatively, you can clone the official repo.
Scikit-learn, pandas, and PyYAML: pip install -U scikit-learn pandas pyyaml.
Verify that your annotations are in the YOLO detection format.

For this tutorial, all annotation files are found in the Fruit-Detection/labels directory.
Generating Feature Vectors for Object Detection Dataset
Start by creating a new example.py Python file for the steps below.

Proceed to retrieve all label files for your dataset.


from pathlib import Path

dataset_path = Path("./Fruit-detection")  # replace with 'path/to/dataset' for your custom data
labels = sorted(dataset_path.rglob("*labels/*.txt"))  # all data in 'labels'
Now, read the contents of the dataset YAML file and extract the indices of the class labels.


import yaml

yaml_file = "path/to/data.yaml"  # your data YAML with data directories and names dictionary
with open(yaml_file, encoding="utf8") as y:
    classes = yaml.safe_load(y)["names"]
cls_idx = sorted(classes.keys())
Initialize an empty pandas DataFrame.


import pandas as pd

index = [label.stem for label in labels]  # uses base filename as ID (no extension)
labels_df = pd.DataFrame([], columns=cls_idx, index=index)
Count the instances of each class-label present in the annotation files.


from collections import Counter

for label in labels:
    lbl_counter = Counter()

    with open(label) as lf:
        lines = lf.readlines()

    for line in lines:
        # classes for YOLO label uses integer at first position of each line
        lbl_counter[int(line.split(" ", 1)[0])] += 1

    labels_df.loc[label.stem] = lbl_counter

labels_df = labels_df.fillna(0.0)  # replace `nan` values with `0.0`
The following is a sample view of the populated DataFrame:


                                                       0    1    2    3    4    5
'0000a16e4b057580_jpg.rf.00ab48988370f64f5ca8ea4...'  0.0  0.0  0.0  0.0  0.0  7.0
'0000a16e4b057580_jpg.rf.7e6dce029fb67f01eb19aa7...'  0.0  0.0  0.0  0.0  0.0  7.0
'0000a16e4b057580_jpg.rf.bc4d31cdcbe229dd022957a...'  0.0  0.0  0.0  0.0  0.0  7.0
'00020ebf74c4881c_jpg.rf.508192a0a97aa6c4a3b6882...'  0.0  0.0  0.0  1.0  0.0  0.0
'00020ebf74c4881c_jpg.rf.5af192a2254c8ecc4188a25...'  0.0  0.0  0.0  1.0  0.0  0.0
 ...                                                  ...  ...  ...  ...  ...  ...
'ff4cd45896de38be_jpg.rf.c4b5e967ca10c7ced3b9e97...'  0.0  0.0  0.0  0.0  0.0  2.0
'ff4cd45896de38be_jpg.rf.ea4c1d37d2884b3e3cbce08...'  0.0  0.0  0.0  0.0  0.0  2.0
'ff5fd9c3c624b7dc_jpg.rf.bb519feaa36fc4bf630a033...'  1.0  0.0  0.0  0.0  0.0  0.0
'ff5fd9c3c624b7dc_jpg.rf.f0751c9c3aa4519ea3c9d6a...'  1.0  0.0  0.0  0.0  0.0  0.0
'fffe28b31f2a70d4_jpg.rf.7ea16bd637ba0711c53b540...'  0.0  6.0  0.0  0.0  0.0  0.0
The rows index the label files, each corresponding to an image in your dataset, and the columns correspond to your class-label indices. Each row represents a pseudo feature-vector, with the count of each class-label present in your dataset. This data structure enables the application of K-Fold Cross Validation to an object detection dataset.

K-Fold Dataset Split
Now we will use the KFold class from sklearn.model_selection to generate k splits of the dataset.

Important:
Setting shuffle=True ensures a randomized distribution of classes in your splits.
By setting random_state=M where M is a chosen integer, you can obtain repeatable results.

import random

from sklearn.model_selection import KFold

random.seed(0)  # for reproducibility
ksplit = 5
kf = KFold(n_splits=ksplit, shuffle=True, random_state=20)  # setting random_state for repeatable results

kfolds = list(kf.split(labels_df))
The dataset has now been split into k folds, each having a list of train and val indices. We will construct a DataFrame to display these results more clearly.


folds = [f"split_{n}" for n in range(1, ksplit + 1)]
folds_df = pd.DataFrame(index=index, columns=folds)

for i, (train, val) in enumerate(kfolds, start=1):
    folds_df[f"split_{i}"].loc[labels_df.iloc[train].index] = "train"
    folds_df[f"split_{i}"].loc[labels_df.iloc[val].index] = "val"
Now we will calculate the distribution of class labels for each fold as a ratio of the classes present in val to those present in train.


fold_lbl_distrb = pd.DataFrame(index=folds, columns=cls_idx)

for n, (train_indices, val_indices) in enumerate(kfolds, start=1):
    train_totals = labels_df.iloc[train_indices].sum()
    val_totals = labels_df.iloc[val_indices].sum()

    # To avoid division by zero, we add a small value (1E-7) to the denominator
    ratio = val_totals / (train_totals + 1e-7)
    fold_lbl_distrb.loc[f"split_{n}"] = ratio
The ideal scenario is for all class ratios to be reasonably similar for each split and across classes. This, however, will be subject to the specifics of your dataset.

Next, we create the directories and dataset YAML files for each split.


import datetime

supported_extensions = [".jpg", ".jpeg", ".png"]

# Initialize an empty list to store image file paths
images = []

# Loop through supported extensions and gather image files
for ext in supported_extensions:
    images.extend(sorted((dataset_path / "images").rglob(f"*{ext}")))

# Create the necessary directories and dataset YAML files
save_path = Path(dataset_path / f"{datetime.date.today().isoformat()}_{ksplit}-Fold_Cross-val")
save_path.mkdir(parents=True, exist_ok=True)
ds_yamls = []

for split in folds_df.columns:
    # Create directories
    split_dir = save_path / split
    split_dir.mkdir(parents=True, exist_ok=True)
    (split_dir / "train" / "images").mkdir(parents=True, exist_ok=True)
    (split_dir / "train" / "labels").mkdir(parents=True, exist_ok=True)
    (split_dir / "val" / "images").mkdir(parents=True, exist_ok=True)
    (split_dir / "val" / "labels").mkdir(parents=True, exist_ok=True)

    # Create dataset YAML files
    dataset_yaml = split_dir / f"{split}_dataset.yaml"
    ds_yamls.append(dataset_yaml)

    with open(dataset_yaml, "w") as ds_y:
        yaml.safe_dump(
            {
                "path": split_dir.as_posix(),
                "train": "train",
                "val": "val",
                "names": classes,
            },
            ds_y,
        )
Lastly, copy images and labels into the respective directory ('train' or 'val') for each split.

NOTE: The time required for this portion of the code will vary based on the size of your dataset and your system hardware.

import shutil

from tqdm import tqdm

for image, label in tqdm(zip(images, labels), total=len(images), desc="Copying files"):
    for split, k_split in folds_df.loc[image.stem].items():
        # Destination directory
        img_to_path = save_path / split / k_split / "images"
        lbl_to_path = save_path / split / k_split / "labels"

        # Copy image and label files to new directory (SamefileError if file already exists)
        shutil.copy(image, img_to_path / image.name)
        shutil.copy(label, lbl_to_path / label.name)
Save Records (Optional)
Optionally, you can save the records of the K-Fold split and label distribution DataFrames as CSV files for future reference.


folds_df.to_csv(save_path / "kfold_datasplit.csv")
fold_lbl_distrb.to_csv(save_path / "kfold_label_distribution.csv")
Train YOLO using K-Fold Data Splits
First, load the YOLO model.


from ultralytics import YOLO

weights_path = "path/to/weights.pt"  # use yolo11n.pt for a small model
model = YOLO(weights_path, task="detect")
Next, iterate over the dataset YAML files to run training. The results will be saved to a directory specified by the project and name arguments. By default, this directory is 'runs/detect/train#' where # is an integer index.


results = {}

# Define your additional arguments here
batch = 16
project = "kfold_demo"
epochs = 100

for k, dataset_yaml in enumerate(ds_yamls):
    model = YOLO(weights_path, task="detect")
    results[k] = model.train(
        data=dataset_yaml, epochs=epochs, batch=batch, project=project, name=f"fold_{k + 1}"
    )  # include any additional train arguments
You can also use Ultralytics data.utils.autosplit function for automatic dataset splitting:


from ultralytics.data.split import autosplit

# Automatically split dataset into train/val/test
autosplit(path="path/to/images", weights=(0.8, 0.2, 0.0), annotated_only=True)
Conclusion
In this guide, we have explored the process of using K-Fold cross-validation for training the YOLO object detection model. We learned how to split our dataset into K partitions, ensuring a balanced class distribution across the different folds.

We also explored the procedure for creating report DataFrames to visualize the data splits and label distributions across these splits, providing us a clear insight into the structure of our training and validation sets.

Optionally, we saved our records for future reference, which could be particularly useful in large-scale projects or when troubleshooting model performance.

Finally, we implemented the actual model training using each split in a loop, saving our training results for further analysis and comparison.

This technique of K-Fold cross-validation is a robust way of making the most out of your available data, and it helps to ensure that your model performance is reliable and consistent across different data subsets. This results in a more generalizable and reliable model that is less likely to overfit to specific data patterns.

Remember that although we used YOLO in this guide, these steps are mostly transferable to other machine learning models. Understanding these steps allows you to apply cross-validation effectively in your own machine learning projects. Happy coding!

FAQ
What is K-Fold Cross Validation and why is it useful in object detection?
K-Fold Cross Validation is a technique where the dataset is divided into 'k' subsets (folds) to evaluate model performance more reliably. Each fold serves as both training and validation data. In the context of object detection, using K-Fold Cross Validation helps to ensure your Ultralytics YOLO model's performance is robust and generalizable across different data splits, enhancing its reliability. For detailed instructions on setting up K-Fold Cross Validation with Ultralytics YOLO, refer to K-Fold Cross Validation with Ultralytics.

How do I implement K-Fold Cross Validation using Ultralytics YOLO?
To implement K-Fold Cross Validation with Ultralytics YOLO, you need to follow these steps:

Verify annotations are in the YOLO detection format.
Use Python libraries like sklearn, pandas, and pyyaml.
Create feature vectors from your dataset.
Split your dataset using KFold from sklearn.model_selection.
Train the YOLO model on each split.
For a comprehensive guide, see the K-Fold Dataset Split section in our documentation.

Why should I use Ultralytics YOLO for object detection?
Ultralytics YOLO offers state-of-the-art, real-time object detection with high accuracy and efficiency. It's versatile, supporting multiple computer vision tasks such as detection, segmentation, and classification. Additionally, it integrates seamlessly with tools like Ultralytics HUB for no-code model training and deployment. For more details, explore the benefits and features on our Ultralytics YOLO page.

How can I ensure my annotations are in the correct format for Ultralytics YOLO?
Your annotations should follow the YOLO detection format. Each annotation file must list the object class, alongside its bounding box coordinates in the image. The YOLO format ensures streamlined and standardized data processing for training object detection models. For more information on proper annotation formatting, visit the YOLO detection format guide.

Can I use K-Fold Cross Validation with custom datasets other than Fruit Detection?
Yes, you can use K-Fold Cross Validation with any custom dataset as long as the annotations are in the YOLO detection format. Replace the dataset paths and class labels with those specific to your custom dataset. This flexibility ensures that any object detection project can benefit from robust model evaluation using K-Fold Cross Validation. For a practical example, review our Generating Feature Vectors section.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 1 month ago
glenn-jocher
RizwanMunawar
ambitious-octopus
Y-T-G
willie.maddox@gmail.com
Burhan-Q
M-Amrollahi
IvorZhu331

Tweet

Share

Comments
 Back to top
Previous
Model YAML Configuration Guide
Next
Hyperparameter Tuning
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Back to Ultralytics Glossary
Types of Model Pruning
Real-World Applications
Pruning vs. Other Optimization Techniques
Glossary
Model Pruning
Optimize machine learning models with model pruning. Achieve faster inference, reduced memory use, and energy efficiency for resource-limited deployments.

Flexible enterprise licensing solution to power your innovation
Get started
Train YOLO models with Ultralytics HUB
Model pruning is a model optimization technique that makes neural networks smaller and more computationally efficient. The core idea is to identify and remove redundant or unimportant parameters (weights, neurons, or channels) from a trained model. This process reduces the model's size and can significantly speed up inference, making it ideal for deployment on edge devices with limited memory and processing power. The concept is based on the observation that many large models are over-parameterized, meaning they contain components that contribute very little to the final prediction. Seminal papers like Optimal Brain Damage established early on that not all parameters are created equal.

Types of Model Pruning
Model pruning techniques are typically categorized by the granularity of what is removed from the network:

Weight Pruning (Unstructured): This is the most fine-grained method, where individual model weights with values below a certain threshold are set to zero. This creates a "sparse" model, which can be highly compressed. However, it often requires specialized hardware or software libraries, like NVIDIA's tools for sparse models, to achieve significant speedups during inference.
Neuron Pruning: In this approach, entire neurons and all their incoming and outgoing connections are removed if they are deemed unimportant. This is a more structured form of pruning than removing individual weights.
Filter/Channel Pruning (Structured): Particularly relevant for Convolutional Neural Networks (CNNs), this method removes entire filters or channels. Because it preserves the dense, regular structure of the network layers, this approach often results in direct performance gains on standard hardware without needing specialized libraries. Tools like Neural Magic's DeepSparse are designed to accelerate these sparse models on CPUs.
After pruning, models typically undergo fine-tuning, which involves retraining the smaller network for a few epochs to recover any accuracy lost during parameter removal. The famous Lottery Ticket Hypothesis suggests that within a large network, there exists a smaller subnetwork that can achieve similar performance when trained from scratch. Frameworks like PyTorch offer built-in tools for implementation, as demonstrated in the official PyTorch Pruning Tutorial.

Real-World Applications
Model pruning is critical for deploying efficient AI models in various scenarios:

Optimizing Object Detection on Edge Devices: Models like Ultralytics YOLO can be pruned to run efficiently for object detection tasks on resource-constrained hardware such as a Raspberry Pi or NVIDIA Jetson. This enables real-time applications like traffic management, smart surveillance, and integrating computer vision in robotics.
Deploying Large Language Models (LLMs) Locally: Pruning is used to shrink massive models based on the Transformer architecture, enabling them to run on devices like smartphones for natural language processing (NLP) tasks. This approach, sometimes combined with other techniques like quantization, allows for powerful, on-device AI assistants and translation apps while enhancing data privacy and reducing latency. Research and tools from organizations like Hugging Face explore LLM pruning.
Pruning vs. Other Optimization Techniques
Model pruning is one of several complementary model optimization techniques:

Model Quantization: This technique reduces the numerical precision of model weights and activations (e.g., from 32-bit floating-point numbers to 8-bit integers). Unlike pruning, which removes parameters, quantization makes the existing parameters smaller in size. It is often applied after pruning for maximum optimization, especially when targeting hardware with specialized support like TensorRT.
Knowledge Distillation: This method involves training a smaller "student" model to mimic the output of a larger, pre-trained "teacher" model. The goal is to transfer the teacher's learned knowledge to a more compact architecture. This differs from pruning, which slims down an already-trained model rather than training a new one.
Ultimately, these techniques can be used in combination to create highly efficient models. Once optimized, a model can be exported to standard formats like ONNX using Ultralytics' export options for broad deployment across different inference engines. Platforms such as Ultralytics HUB provide the tools to manage the entire lifecycle of computer vision models, from training to optimized deployment.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
The Role of Validation Data
Validation Data vs. Training and Test Data
Real-World Examples
Cross-Validation
Glossary
Validation Data
Optimize machine learning models with validation data to prevent overfitting, tune hyperparameters, and ensure robust, real-world performance.

Train AI models in seconds with Ultralytics YOLO
Get started
Train YOLO models with Ultralytics HUB
Validation data is a sample of data held back from the training process that is used to provide an unbiased evaluation of a model's fit while tuning its hyperparameters. The primary role of the validation set is to guide the development of a machine learning (ML) model by offering a frequent, independent assessment of its performance. This feedback loop is essential for building models that not only perform well on the data they have seen but also generalize effectively to new, unseen data, a concept central to creating robust Artificial Intelligence (AI) systems.

The Role of Validation Data
The main purpose of validation data is to prevent overfitting. Overfitting occurs when a model learns the training data too well, capturing noise and details that do not apply to new data, thereby hurting its performance. By testing the model against the validation set at regular intervals (e.g., after each epoch), developers can monitor its generalization error. If performance on the training data continues to improve while performance on the validation data stagnates or degrades, it's a clear sign of overfitting.

This evaluation process is crucial for hyperparameter tuning. Hyperparameters are configuration settings external to the model, such as the learning rate or batch size, which are not learned from the data. The validation set allows for experimenting with different hyperparameter combinations to find the set that yields the best performance. This iterative process is a core part of model selection and optimization.

Validation Data vs. Training and Test Data
In a typical ML project, the dataset is split into three subsets, and understanding their distinct roles is fundamental. A common approach to data splitting is to allocate 70% for training, 15% for validation, and 15% for testing.

Training Data: This is the largest portion of the data, used to teach the model. The model iteratively learns patterns, features, and relationships from this dataset by adjusting its internal model weights.
Validation Data: This separate subset is used to provide an unbiased evaluation during the training process. It helps tune hyperparameters and make key decisions, such as when to implement early stopping to prevent overfitting. In the Ultralytics ecosystem, this evaluation is handled in the validation mode.
Test Data: This dataset is held out until the model is fully trained and tuned. It is used only once to provide a final, unbiased assessment of the model's performance. The test set's performance indicates how the model is expected to perform in a real-world deployment scenario.
Maintaining a strict separation, especially between the validation and test sets, is critical for accurately assessing a model's capabilities and avoiding the bias-variance tradeoff.

Real-World Examples
Computer Vision Object Detection: When training an Ultralytics YOLO model for detecting objects in images (e.g., using the VisDrone dataset), a portion of the labeled images is set aside as validation data. During training, the model's mAP (mean Average Precision) is calculated on this validation set after each epoch. This validation mAP helps decide when to stop training or which set of data augmentation techniques works best, before a final performance check on the test set. Effective model evaluation strategies rely heavily on this split.
Natural Language Processing Text Classification: In developing a model to classify customer reviews as positive or negative (sentiment analysis), a validation set is used to choose the optimal architecture (e.g., LSTM vs. Transformer) or tune hyperparameters like dropout rates. The model achieving the highest F1-score or accuracy on the validation set would be selected for final testing. Resources like Hugging Face Datasets often provide datasets pre-split for this purpose.
Cross-Validation
When the amount of available data is limited, a technique called Cross-Validation (specifically K-Fold Cross-Validation) is often employed. Here, the training data is split into 'K' subsets (folds). The model is trained K times, each time using K-1 folds for training and the remaining fold as the validation set. The performance is then averaged across all K runs. This provides a more robust estimate of model performance and makes better use of limited data, as explained in resources like the scikit-learn documentation and the Ultralytics K-Fold Cross-Validation guide.

In summary, validation data is a cornerstone of building reliable and high-performing AI models with frameworks like PyTorch and TensorFlow. It enables effective hyperparameter tuning, model selection, and overfitting prevention, ensuring that models generalize well beyond the data they were trained on. Platforms like Ultralytics HUB offer integrated tools for managing these datasets effectively.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Datasets: Class-imbalanced datasets

bookmark_border
Spark icon
AI-generated Key Takeaways



This section explores the following three questions:

What's the difference between class-balanced datasets and class-imbalanced datasets?
Why is training an imbalanced dataset difficult?
How can you overcome the problems of training imbalanced datasets?
Class-balanced datasets versus class-imbalanced datasets
Consider a dataset containing a categorical label whose value is either the positive class or the negative class. In a class-balanced dataset, the number of positive classes and negative classes is about equal. For example, a dataset containing 235 positive classes and 247 negative classes is a balanced dataset.

In a class-imbalanced dataset, one label is considerably more common than the other. In the real world, class-imbalanced datasets are far more common than class-balanced datasets. For example, in a dataset of credit card transactions, fraudulent purchases might make up less than 0.1% of the examples. Similarly, in a medical diagnosis dataset, the number of patients with a rare virus might be less than 0.01% of the total examples. In a class-imbalanced dataset:

The more common label is called the majority class.
The less common label is called the minority class.
The difficulty of training severely class-imbalanced datasets
Training aims to create a model that successfully distinguishes the positive class from the negative class. To do so, batches need a sufficient number of both positive classes and negative classes. That's not a problem when training on a mildly class-imbalanced dataset since even small batches typically contain sufficient examples of both the positive class and the negative class. However, a severely class-imbalanced dataset might not contain enough minority class examples for proper training.

For example, consider the class-imbalanced dataset illustrated in Figure 6 in which:

200 labels are in the majority class.
2 labels are in the minority class.
Figure 6. A dataset with a 202 examples. 200 of the examples have
                      a sunflower label and 2 of the examples have a
                      rose label.
Figure 6. A highly imbalanced floral dataset containing far more sunflowers than roses.
 

If the batch size is 20, most batches won't contain any examples of the minority class. If the batch size is 100, each batch will contain an average of only one minority class example, which is insufficient for proper training. Even a much larger batch size will still yield such an imbalanced proportion that the model might not train properly.

Note: Accuracy is usually a poor metric for assessing a model trained on a class-imbalanced dataset. See Classification: Accuracy, recall, precision, and related metrics for details.
Training a class-imbalanced dataset
During training, a model should learn two things:

What each class looks like; that is, what feature values correspond to what class?
How common each class is; that is, what is the relative distribution of the classes?
Standard training conflates these two goals. In contrast, the following two-step technique called downsampling and upweighting the majority class separates these two goals, enabling the model to achieve both goals.

Note: Many students read the following section and say some variant of, "That just can't be right." Be warned that downsampling and upweighting the majority class is somewhat counterintuitive.
Step 1: Downsample the majority class
Downsampling means training on a disproportionately low percentage of majority class examples. That is, you artificially force a class-imbalanced dataset to become somewhat more balanced by omitting many of the majority class examples from training. Downsampling greatly increases the probability that each batch contains enough examples of the minority class to train the model properly and efficiently.

For example, the class-imbalanced dataset shown in Figure 6 consists of 99% majority class and 1% minority class examples. Downsampling the majority class by a factor of 25 artificially creates a more balanced training set (80% majority class to 20% minority class) suggested in Figure 7:

Figure 7. 10 examples, 8 of which are sunflowers and 2 of which
                      are roses.
Figure 7. Downsampling the majority class by a factor of 25.
 

Step 2: Upweight the downsampled class
Downsampling introduces a prediction bias by showing the model an artificial world where the classes are more balanced than in the real world. To correct this bias, you must "upweight" the majority classes by the factor to which you downsampled. Upweighting means treating the loss on a majority class example more harshly than the loss on a minority class example.

For example, we downsampled the majority class by a factor of 25, so we must upweight the majority class by a factor of 25. That is, when the model mistakenly predicts the majority class, treat the loss as if it were 25 errors (multiply the regular loss by 25).

Figure 8. The loss for a bad prediction on the minority class
                      is treated normally. However, the loss for a bad
                      prediction on the majority class is treated 25 times
                      more harshly.
Figure 8. Upweighting the majority class by a factor of 25.
 

How much should you downsample and upweight to rebalance your dataset? To determine the answer, you should experiment with different downsampling and upweighting factors just as you would experiment with other hyperparameters.

Benefits of this technique
Downsampling and upweighting the majority class brings the following benefits:

Better model: The resultant model "knows" both of the following:
The connection between features and labels
The true distribution of the classes
Faster convergence: During training, the model sees the minority class more often, which helps the model converge faster.
Early stopping

Article
Talk
Read
Edit
View history

Tools
Appearance hide
Text

Small

Standard

Large
Width

Standard

Wide
Color (beta)

Automatic

Light

Dark
From Wikipedia, the free encyclopedia
In machine learning, early stopping is a form of regularization used to avoid overfitting when training a model with an iterative method, such as gradient descent. Such methods update the model to make it better fit the training data with each iteration. Up to a point, this improves the model's performance on data outside of the training set (e.g., the validation set). Past that point, however, improving the model's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.

Background
This section presents some of the basic machine-learning concepts required for a description of early stopping methods.

Overfitting
Main article: Overfitting

Figure 1.  The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data illustrated by black-outlined dots, compared to the black line.
Machine learning algorithms train a model based on a finite set of training data. During this training, the model is evaluated based on how well it predicts the observations contained in the training set. In general, however, the goal of a machine learning scheme is to produce a model that generalizes, that is, that predicts previously unseen observations. Overfitting occurs when a model fits the data in the training set well, while incurring larger generalization error.

Regularization
Main article: Regularization (mathematics)
Regularization, in the context of machine learning, refers to the process of modifying a learning algorithm so as to prevent overfitting. This generally involves imposing some sort of smoothness constraint on the learned model.[1] This smoothness may be enforced explicitly, by fixing the number of parameters in the model, or by augmenting the cost function as in Tikhonov regularization. Tikhonov regularization, along with principal component regression and many other regularization schemes, fall under the umbrella of spectral regularization, regularization characterized by the application of a filter. Early stopping also belongs to this class of methods.

Gradient descent methods
Main article: Gradient descent
Gradient descent methods are first-order, iterative, optimization methods. Each iteration updates an approximate solution to the optimization problem by taking a step in the direction of the negative of the gradient of the objective function. By choosing the step-size appropriately, such a method can be made to converge to a local minimum of the objective function. Gradient descent is used in machine-learning by defining a loss function that reflects the error of the learner on the training set and then minimizing that function.

Early stopping based on analytical results
Early stopping in statistical learning theory
Early-stopping can be used to regularize non-parametric regression problems encountered in machine learning. For a given input space, 
X
{\displaystyle X}, output space, 
Y
{\displaystyle Y}, and samples drawn from an unknown probability measure, 
œÅ
{\displaystyle \rho }, on 
Z
=
X
√ó
Y
{\displaystyle Z=X\times Y}, the goal of such problems is to approximate a regression function, 
f
œÅ
{\displaystyle f_{\rho }}, given by

f
œÅ
(
x
)
=
‚à´
Y
y
d
œÅ
(
y
‚à£
x
)
,
x
‚àà
X
,
{\displaystyle f_{\rho }(x)=\int _{Y}y\,d\rho (y\mid x),\,x\in X,}
where 
œÅ
(
y
‚à£
x
)
{\displaystyle \rho (y\mid x)} is the conditional distribution at 
x
{\displaystyle x} induced by 
œÅ
{\displaystyle \rho }.[2] One common choice for approximating the regression function is to use functions from a reproducing kernel Hilbert space.[2] These spaces can be infinite dimensional, in which they can supply solutions that overfit training sets of arbitrary size. Regularization is, therefore, especially important for these methods. One way to regularize non-parametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent.

The early stopping rules proposed for these problems are based on analysis of upper bounds on the generalization error as a function of the iteration number. They yield prescriptions for the number of iterations to run that can be computed prior to starting the solution process.[3] [4]

Example: Least-squares loss
(Adapted from Yao, Rosasco and Caponnetto, 2007[3])

Let 
X
‚äÜ
R
n
{\displaystyle X\subseteq \mathbb {R} ^{n}} and 
Y
=
R
.
{\displaystyle Y=\mathbb {R} .} Given a set of samples

z
=
{
(
x
i
,
y
i
)
‚àà
X
√ó
Y
:
i
=
1
,
‚Ä¶
,
m
}
‚àà
Z
m
,
{\displaystyle \mathbf {z} =\left\{(x_{i},y_{i})\in X\times Y:i=1,\dots ,m\right\}\in Z^{m},}
drawn independently from 
œÅ
{\displaystyle \rho }, minimize the functional

E
(
f
)
=
‚à´
X
√ó
Y
(
f
(
x
)
‚àí
y
)
2
d
œÅ
{\displaystyle {\mathcal {E}}(f)=\int _{X\times Y}(f(x)-y)^{2}\,d\rho }
where, 
f
{\displaystyle f} is a member of the reproducing kernel Hilbert space 
H
{\displaystyle {\mathcal {H}}}. That is, minimize the expected risk for a Least-squares loss function. Since 
E
{\displaystyle {\mathcal {E}}} depends on the unknown probability measure 
œÅ
{\displaystyle \rho }, it cannot be used for computation. Instead, consider the following empirical risk

E
z
(
f
)
=
1
m
‚àë
i
=
1
m
(
f
(
x
i
)
‚àí
y
i
)
2
.
{\displaystyle {\mathcal {E}}_{\mathbf {z} }(f)={\frac {1}{m}}\sum _{i=1}^{m}\left(f(x_{i})-y_{i}\right)^{2}.}
Let 
f
t
{\displaystyle f_{t}} and 
f
t
z
{\displaystyle f_{t}^{\mathbf {z} }} be the t-th iterates of gradient descent applied to the expected and empirical risks, respectively, where both iterations are initialized at the origin, and both use the step size 
Œ≥
t
{\displaystyle \gamma _{t}}. The 
f
t
{\displaystyle f_{t}} form the population iteration, which converges to 
f
œÅ
{\displaystyle f_{\rho }}, but cannot be used in computation, while the 
f
t
z
{\displaystyle f_{t}^{\mathbf {z} }} form the sample iteration which usually converges to an overfitting solution.

We want to control the difference between the expected risk of the sample iteration and the minimum expected risk, that is, the expected risk of the regression function:

E
(
f
t
z
)
‚àí
E
(
f
œÅ
)
{\displaystyle {\mathcal {E}}(f_{t}^{\mathbf {z} })-{\mathcal {E}}(f_{\rho })}
This difference can be rewritten as the sum of two terms: the difference in expected risk between the sample and population iterations and that between the population iteration and the regression function:

E
(
f
t
z
)
‚àí
E
(
f
œÅ
)
=
[
E
(
f
t
z
)
‚àí
E
(
f
t
)
]
+
[
E
(
f
t
)
‚àí
E
(
f
œÅ
)
]
{\displaystyle {\mathcal {E}}(f_{t}^{\mathbf {z} })-{\mathcal {E}}(f_{\rho })=\left[{\mathcal {E}}(f_{t}^{\mathbf {z} })-{\mathcal {E}}(f_{t})\right]+\left[{\mathcal {E}}(f_{t})-{\mathcal {E}}(f_{\rho })\right]}
This equation presents a bias-variance tradeoff, which is then solved to give an optimal stopping rule that may depend on the unknown probability distribution. That rule has associated probabilistic bounds on the generalization error. For the analysis leading to the early stopping rule and bounds, the reader is referred to the original article.[3] In practice, data-driven methods, e.g. cross-validation can be used to obtain an adaptive stopping rule.

Early stopping in boosting
Boosting refers to a family of algorithms in which a set of weak learners (learners that are only slightly correlated with the true process) are combined to produce a strong learner. It has been shown, for several boosting algorithms (including AdaBoost), that regularization via early stopping can provide guarantees of consistency, that is, that the result of the algorithm approaches the true solution as the number of samples goes to infinity.[5] [6] [7] [8]

L2-boosting
Boosting methods have close ties to the gradient descent methods described above can be regarded as a boosting method based on the 
L
2
{\displaystyle L_{2}} loss: L2Boost.[3]

Validation-based early stopping
These early stopping rules work by splitting the original training set into a new training set and a validation set. The error on the validation set is used as a proxy for the generalization error in determining when overfitting has begun. These methods are employed in the training of many iterative machine learning algorithms including neural networks. Prechelt gives the following summary of a naive implementation of holdout-based early stopping as follows:[9]

Split the training data into a training set and a validation set, e.g. in a 2-to-1 proportion.
Train only on the training set and evaluate the per-example error on the validation set once in a while, e.g. after every fifth epoch.
Stop training as soon as the error on the validation set is higher than it was the last time it was checked.
Use the weights the network had in that previous step as the result of the training run.
‚Äî‚ÄäLutz Prechelt, Early Stopping ‚Äì But When?
Cross-validation is an alternative that is applicable to non time-series scenarios. Cross-validation involves splitting multiple partitions of the data into training set and validation set ‚Äì instead of a single partition into a training set and validation set. Even this simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad hoc rules for deciding when overfitting has truly begun.[9]
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
Val




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Tasks
Detect
Segment
Classify
Pose
OBB
Modes
Train
Val
Predict
Export
Track
Benchmark
Table of contents
Introduction
Why Validate with Ultralytics YOLO?
Key Features of Val Mode
Usage Examples
Arguments for YOLO Model Validation
Example Validation with Arguments
FAQ
How do I validate my YOLO11 model with Ultralytics?
What metrics can I get from YOLO11 model validation?
What are the advantages of using Ultralytics YOLO for validation?
Can I validate my YOLO11 model using a custom dataset?
How do I save validation results to a JSON file in YOLO11?
Model Validation with Ultralytics YOLO
Ultralytics YOLO ecosystem and integrations

Introduction
Validation is a critical step in the machine learning pipeline, allowing you to assess the quality of your trained models. Val mode in Ultralytics YOLO11 provides a robust suite of tools and metrics for evaluating the performance of your object detection models. This guide serves as a complete resource for understanding how to effectively use the Val mode to ensure that your models are both accurate and reliable.



Watch: Ultralytics Modes Tutorial: Validation

Why Validate with Ultralytics YOLO?
Here's why using YOLO11's Val mode is advantageous:

Precision: Get accurate metrics like mAP50, mAP75, and mAP50-95 to comprehensively evaluate your model.
Convenience: Utilize built-in features that remember training settings, simplifying the validation process.
Flexibility: Validate your model with the same or different datasets and image sizes.
Hyperparameter Tuning: Use validation metrics to fine-tune your model for better performance.
Key Features of Val Mode
These are the notable functionalities offered by YOLO11's Val mode:

Automated Settings: Models remember their training configurations for straightforward validation.
Multi-Metric Support: Evaluate your model based on a range of accuracy metrics.
CLI and Python API: Choose from command-line interface or Python API based on your preference for validation.
Data Compatibility: Works seamlessly with datasets used during the training phase as well as custom datasets.
Tip

YOLO11 models automatically remember their training settings, so you can validate a model at the same image size and on the original dataset easily with just yolo val model=yolo11n.pt or model('yolo11n.pt').val()
Usage Examples
Validate trained YOLO11n model accuracy on the COCO8 dataset. No arguments are needed as the model retains its training data and arguments as model attributes. See Arguments section below for a full list of validation arguments.

Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load an official model
model = YOLO("path/to/best.pt")  # load a custom model

# Validate the model
metrics = model.val()  # no arguments needed, dataset and settings remembered
metrics.box.map  # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps  # a list contains map50-95 of each category

Arguments for YOLO Model Validation
When validating YOLO models, several arguments can be fine-tuned to optimize the evaluation process. These arguments control aspects such as input image size, batch processing, and performance thresholds. Below is a detailed breakdown of each argument to help you customize your validation settings effectively.

Argument  Type  Default Description
data  str None  Specifies the path to the dataset configuration file (e.g., coco8.yaml). This file should include the path to the validation data.
imgsz int 640 Defines the size of input images. All images are resized to this dimension before processing. Larger sizes may improve accuracy for small objects but increase computation time.
batch int 16  Sets the number of images per batch. Higher values utilize GPU memory more efficiently but require more VRAM. Adjust based on available hardware resources.
save_json bool  False If True, saves the results to a JSON file for further analysis, integration with other tools, or submission to evaluation servers like COCO.
conf  float 0.001 Sets the minimum confidence threshold for detections. Lower values increase recall but may introduce more false positives. Used during validation to compute precision-recall curves.
iou float 0.7 Sets the Intersection Over Union threshold for Non-Maximum Suppression. Controls duplicate detection elimination.
max_det int 300 Limits the maximum number of detections per image. Useful in dense scenes to prevent excessive detections and manage computational resources.
half  bool  True  Enables half-precision (FP16) computation, reducing memory usage and potentially increasing speed with minimal impact on accuracy.
device  str None  Specifies the device for validation (cpu, cuda:0, etc.). When None, automatically selects the best available device. Multiple CUDA devices can be specified with comma separation.
dnn bool  False If True, uses the OpenCV DNN module for ONNX model inference, offering an alternative to PyTorch inference methods.
plots bool  False When set to True, generates and saves plots of predictions versus ground truth, confusion matrices, and PR curves for visual evaluation of model performance.
classes list[int] None  Specifies a list of class IDs to train on. Useful for filtering out and focusing only on certain classes during evaluation.
rect  bool  True  If True, uses rectangular inference for batching, reducing padding and potentially increasing speed and efficiency by processing images in their original aspect ratio.
split str 'val' Determines the dataset split to use for validation (val, test, or train). Allows flexibility in choosing the data segment for performance evaluation.
project str None  Name of the project directory where validation outputs are saved. Helps organize results from different experiments or models.
name  str None  Name of the validation run. Used for creating a subdirectory within the project folder, where validation logs and outputs are stored.
verbose bool  False If True, displays detailed information during the validation process, including per-class metrics, batch progress, and additional debugging information.
save_txt  bool  False If True, saves detection results in text files, with one file per image, useful for further analysis, custom post-processing, or integration with other systems.
save_conf bool  False If True, includes confidence values in the saved text files when save_txt is enabled, providing more detailed output for analysis and filtering.
workers int 8 Number of worker threads for data loading. Higher values can speed up data preprocessing but may increase CPU usage. Setting to 0 uses main thread, which can be more stable in some environments.
augment bool  False Enables test-time augmentation (TTA) during validation, potentially improving detection accuracy at the cost of inference speed by running inference on transformed versions of the input.
agnostic_nms  bool  False Enables class-agnostic Non-Maximum Suppression, which merges overlapping boxes regardless of their predicted class. Useful for instance-focused applications.
single_cls  bool  False Treats all classes as a single class during validation. Useful for evaluating model performance on binary detection tasks or when class distinctions aren't important.
visualize bool  False Visualizes the ground truths, true positives, false positives and false negatives for each image. Useful for debugging and model interpretation.
compile bool or str False Enables PyTorch 2.x torch.compile graph compilation with backend='inductor'. Accepts True ‚Üí "default", False ‚Üí disables, or a string mode such as "default", "reduce-overhead", "max-autotune-no-cudagraphs". Falls back to eager with a warning if unsupported.
Each of these settings plays a vital role in the validation process, allowing for a customizable and efficient evaluation of YOLO models. Adjusting these parameters according to your specific needs and resources can help achieve the best balance between accuracy and performance.

Example Validation with Arguments


Watch: How to Export Model Validation Results in CSV, JSON, SQL, Polars DataFrame & More

Explore model validation and different export methods in Google Colab

The below examples showcase YOLO model validation with custom arguments in Python and CLI.

Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")

# Customize validation settings
metrics = model.val(data="coco8.yaml", imgsz=640, batch=16, conf=0.25, iou=0.6, device="0")

Export ConfusionMatrix

You can also save the ConfusionMatrix results in different formats using the provided code.


from ultralytics import YOLO

model = YOLO("yolo11n.pt")

results = model.val(data="coco8.yaml", plots=True)
print(results.confusion_matrix.to_df())
Method  Return Type Description
summary() List[Dict[str, Any]]  Converts validation results to a summarized dictionary.
to_df() DataFrame Returns the validation results as a structured Polars DataFrame.
to_csv()  str Exports the validation results in CSV format and returns the CSV string.
to_json() str Exports the validation results in JSON format and returns the JSON string.
For more details see the DataExportMixin class documentation.

FAQ
How do I validate my YOLO11 model with Ultralytics?
To validate your YOLO11 model, you can use the Val mode provided by Ultralytics. For example, using the Python API, you can load a model and run validation with:


from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")

# Validate the model
metrics = model.val()
print(metrics.box.map)  # map50-95
Alternatively, you can use the command-line interface (CLI):


yolo val model=yolo11n.pt
For further customization, you can adjust various arguments like imgsz, batch, and conf in both Python and CLI modes. Check the Arguments for YOLO Model Validation section for the full list of parameters.

What metrics can I get from YOLO11 model validation?
YOLO11 model validation provides several key metrics to assess model performance. These include:

mAP50 (mean Average Precision at IoU threshold 0.5)
mAP75 (mean Average Precision at IoU threshold 0.75)
mAP50-95 (mean Average Precision across multiple IoU thresholds from 0.5 to 0.95)
Using the Python API, you can access these metrics as follows:


metrics = model.val()  # assumes `model` has been loaded
print(metrics.box.map)  # mAP50-95
print(metrics.box.map50)  # mAP50
print(metrics.box.map75)  # mAP75
print(metrics.box.maps)  # list of mAP50-95 for each category
For a complete performance evaluation, it's crucial to review all these metrics. For more details, refer to the Key Features of Val Mode.

What are the advantages of using Ultralytics YOLO for validation?
Using Ultralytics YOLO for validation provides several advantages:

Precision: YOLO11 offers accurate performance metrics including mAP50, mAP75, and mAP50-95.
Convenience: The models remember their training settings, making validation straightforward.
Flexibility: You can validate against the same or different datasets and image sizes.
Hyperparameter Tuning: Validation metrics help in fine-tuning models for better performance.
These benefits ensure that your models are evaluated thoroughly and can be optimized for superior results. Learn more about these advantages in the Why Validate with Ultralytics YOLO section.

Can I validate my YOLO11 model using a custom dataset?
Yes, you can validate your YOLO11 model using a custom dataset. Specify the data argument with the path to your dataset configuration file. This file should include the path to the validation data.

Note

Validation is performed using the model's own class names, which you can view using model.names, and which may be different to those specified in the dataset configuration file.

Example in Python:


from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")

# Validate with a custom dataset
metrics = model.val(data="path/to/your/custom_dataset.yaml")
print(metrics.box.map)  # map50-95
Example using CLI:


yolo val model=yolo11n.pt data=path/to/your/custom_dataset.yaml
For more customizable options during validation, see the Example Validation with Arguments section.

How do I save validation results to a JSON file in YOLO11?
To save the validation results to a JSON file, you can set the save_json argument to True when running validation. This can be done in both the Python API and CLI.

Example in Python:


from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")

# Save validation results to JSON
metrics = model.val(save_json=True)
Example using CLI:


yolo val model=yolo11n.pt save_json=True
This functionality is particularly useful for further analysis or integration with other tools. Check the Arguments for YOLO Model Validation for more details.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 8 days ago
glenn-jocher
MatthewNoyce
jb297686
Laughing-q
RizwanMunawar
UltralyticsAssistant
jk4e
Burhan-Q

Tweet

Share

Comments
 Back to top
Previous
Train
Next
Predict
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Integrations
Visualizing training metrics with the TensorBoard integration

Abirami Vina
4 min read

January 23, 2025

See how the TensorBoard integration enhances Ultralytics YOLO11 workflows with powerful visualizations and experiment tracking for optimized model performance.


What is TensorBoard?
Key features of TensorBoard
Overview of the Ultralytics YOLO models
Using the TensorBoard integration
Analyzing training metrics
Benefits of the TensorBoard integration
Best practices for using the TensorBoard integration
Key takeaways
Developing reliable computer vision models often involves several steps such as data collection, model training, and an iterative fine-tuning process to address potential challenges and improve performance. Of these steps, training the model is often considered the most important.

Visualizing the training process can help make this step more clear. However, creating detailed graphs, analyzing visual data, and generating charts can take a lot of time and effort. Tools like the TensorBoard integration supported by Ultralytics simplify this process by providing straightforward visuals and in-depth analysis.

TensorBoard is a reliable visualization tool that provides real-time insights into a model‚Äôs training progress. When used with Ultralytics YOLO models like Ultralytics YOLO11, renowned for their accuracy in computer vision tasks such as object detection and instance segmentation, TensorBoard offers a visual dashboard to track training progress. With this integration, we can track key metrics, monitor training performance, and gain actionable insights to fine-tune the model and achieve desired results.

In this article, we‚Äôll explore how using the TensorBoard integration improves Ultralytics YOLO11 model training through real-time visualizations, actionable insights, and practical tips for optimizing performance.

What is TensorBoard?
TensorBoard is an open-source visualization tool developed by TensorFlow. It provides essential metrics and visualizations to support the development and training of machine learning and computer vision models. This toolkit‚Äôs dashboard presents data in various formats, including graphs, images, text, and audio, offering a deeper understanding of the model behavior. With these visualizations, we can make better data-driven decisions to improve model performance.

__wf_reserved_inherit
Fig 1. The TensorBoard dashboard with model training graphs.
‚Äç

Key features of TensorBoard
TensorBoard offers a variety of features to enhance different aspects of model workflows. For instance, performance metrics such as accuracy, learning rate, and loss can be visualized in real-time, providing valuable insights into how the model is learning and highlighting issues like overfitting or underfitting during training. 

Another interesting feature is the 'graph' tool, which visually maps how data flows through the model. This graphical representation makes it easier to understand the model‚Äôs architecture and complexities at a glance.

Here are some other key features of the TensorBoard integration:

Analyze data distribution: TensorBoard provides a detailed distribution of a model‚Äôs internal values, such as weights, biases, and activations. We can use it to learn how data flows through the model‚Äôs network and identify potential areas for improvement.
Evaluate data patterns: Using the "Histogram" feature, we can visualize the distribution of model parameters, such as weights, biases, and gradients, over time. By reading these patterns, we can identify potential biases and bottlenecks in the model.
Explore high-dimensional data: The ‚ÄúProjector‚Äù feature can convert complex high-dimensional data into a lower-dimensional space. This makes it easier to visualize how the model groups different objects together.
Visualize model predictions: TensorBoard lets you compare input images, their correct labels (ground truth), and the model‚Äôs predictions side by side. By doing this, you can easily spot mistakes, like when the model incorrectly identifies something (false positives) or misses something important (false negatives). 
Overview of the Ultralytics YOLO models
Ultralytics YOLO (You Only Look Once) models are among the most popular and widely used computer vision models today. They are mainly used for high-performance computer vision tasks like object detection and instance segmentation. Widely known for their speed, accuracy, and ease of use, YOLO models are being adopted across various industries, including agriculture, manufacturing, and healthcare. 

It all started with Ultralytics YOLOv5, which made it easier to use Vision AI models with tools like PyTorch. Next, Ultralytics YOLOv8 added features like pose estimation and image classification. 

Now, YOLO11 offers even better performance. In fact, YOLO11m achieves a higher mean average precision (mAP) on the COCO dataset while using 22% fewer parameters than YOLOv8m, making it both more precise and efficient at detecting objects.

__wf_reserved_inherit
Fig 2. An example of using YOLO11 for object detection.
‚Äç

The TensorBoard integration can be used to track and monitor key metrics, perform in-depth analysis, and streamline the custom training and development process of YOLO11. Its real-time visualization features make building, fine-tuning, and optimizing YOLO11 more efficient, helping developers and AI researchers achieve better results with less effort.

Using the TensorBoard integration
Using the TensorBoard integration while custom-training Ultralytics YOLO11 is easy. Since TensorBoard is seamlessly integrated with the Ultralytics Python package, there‚Äôs no need for additional installations or setup steps. 

Once training begins, the package automatically logs key metrics such as loss, accuracy, learning rate, and mean average precision (mAP) to a designated directory, enabling detailed performance analysis. An output message will confirm that TensorBoard is actively monitoring your training session, and you can view the dashboard at a URL like `http://localhost:6006/`.  

To access the logged data, you can launch TensorBoard using the URL and find real-time visualizations of metrics such as loss, accuracy, learning rate, and mAP, along with tools like graphs, scalars, and histograms for deeper analysis. 

These dynamic and interactive visuals make it easier to monitor training progress, spot issues, and pinpoint areas for improvement. By leveraging these features, the TensorBoard integration ensures that the YOLO11 training process remains transparent, organized, and easy to understand.

For users working in Google Colab, TensorBoard integrates directly within the notebook cell, where the configuration commands are executed for seamless access to training insights. 

For step-by-step guidance and best practices on installation, you can refer to the YOLO11 Installation Guide. If you face any challenges while setting up the required packages, the Common Issues Guide offers helpful solutions and troubleshooting tips. 

Analyzing training metrics
Understanding key training metrics is essential for evaluating model performance and the TensorBoard integration provides in-depth visualizations to do so. But how does this work?

Let‚Äôs say you are observing an evaluation accuracy curve - a graph that shows how the model‚Äôs accuracy improves on validation data as training progresses. In the beginning, you might see a sharp increase in accuracy, indicating that your model is learning quickly and improving its performance. 

However, as training continues, the rate of improvement may slow, and the curve might begin to flatten. This flattening suggests that the model is nearing its optimal state. Continuing training beyond this point is unlikely to bring significant improvements and may lead to overfitting. 

By visualizing these trends with the TensorBoard integration, as shown below, you can identify the model‚Äôs optimal state and make necessary adjustments to the training process.

__wf_reserved_inherit
Fig 3. An example of a TensorBoard graph. Image by Author.
‚Äç

Benefits of the TensorBoard integration
The TensorBoard integration offers a wide range of benefits that improve YOLO11 model training and performance optimization. Some of the key benefits are as follows:

Compare experiments: You can easily compare multiple training runs to identify the best-performing model configuration.
Save time and effort: This integration streamlines the process of monitoring and analyzing training metrics, reducing manual effort and accelerating model development.
Track custom metrics: You can configure logging to monitor specific metrics relevant to the application, providing deeper insights tailored to your model.
Efficient resource utilization: Beyond training metrics, you can monitor GPU usage, memory allocation, and computation time through custom logging for optimal hardware performance.
__wf_reserved_inherit
Fig 3. Benefits of using the TensorBoard integration. Image by author.
‚Äç

Best practices for using the TensorBoard integration
Now that we‚Äôve understood what the TensorBoard integration is and how to use it, let‚Äôs explore some of the best practices for using this integration: 

Use clear naming conventions: Create structured names for experiments that include the model type, dataset, and key parameters to avoid confusion and make comparisons easier.
Set optimal logging frequency: Log data at intervals that provide useful insights without slowing down the YOLO11 model training process.
Ensure reproducibility and compatibility: Regularly update packages such as TensorBoard, Ultralytics, and datasets to ensure access to new features, bug fixes, and compatibility with evolving data requirements.
By following these best practices, you can make the YOLO11 development process more efficient, organized, and productive. Explore other available integrations to boost your computer vision workflows and maximize your model‚Äôs potential.

Key takeaways
The TensorBoard integration supported by Ultralytics makes it easier to monitor and track the model development process, improving overall performance. With its intuitive visualization features, TensorBoard provides insights into training metrics, tracks trends in loss and accuracy, and enables seamless comparisons across experiments.

It simplifies decision-making by streamlining data preparation, fine-tuning settings, and analyzing metrics to optimize model performance. These features also deliver significant business advantages, including faster time-to-market for computer vision applications and lower development costs. By using best practices, like clear naming and keeping things updated, developers can make training easier. They can work more efficiently and explore new options with advanced computer vision models like YOLO11.

Become part of our community and explore our GitHub repository to dive into AI. Discover how computer vision in manufacturing and AI in healthcare are driving innovation by visiting our solutions pages. Don‚Äôt forget to check out our licensing options to get started with your Vision AI journey today!

Read more in this category
Integrations

A guide on U-Net architecture and its applications
5 min read

Jul 15, 2025
Integrations

Popular open-source OCR models and how they work
5 min read

Jul 7, 2025
Integrations

Seamlessly deploy Ultralytics YOLO11 using OpenVINO‚Ñ¢
5 min read

Jul 1, 2025
Browse all articles
Let‚Äôs build the future
of AI together!
Begin your journey with the future of machine learning

Start for free
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
TensorBoard




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Integrations
SONY IMX500
Amazon SageMaker
ClearML
Comet ML
DVC
Google Colab
IBM Watsonx
JupyterLab
Kaggle
MLflow
Paperspace Gradient
Ray Tune
TensorBoard
VS Code
Weights & Biases
Albumentations
TorchScript
ONNX
OpenVINO
TensorRT
CoreML
TF SavedModel
TF GraphDef
TFLite
TFLite Edge TPU
TF.js
PaddlePaddle
MNN
NCNN
Rockchip RKNN
Neural Magic
Seeed Studio reCamera
Gradio
Roboflow
Table of contents
TensorBoard
YOLO11 Training with TensorBoard
Installation
Configuring TensorBoard for Google Colab
Usage
Understanding Your TensorBoard for YOLO11 Training
Time Series
Key Features of Time Series in TensorBoard
Importance of Time Series in YOLO11 Training
Scalars
Key Features of Scalars in TensorBoard
Importance of Monitoring Scalars
Difference Between Scalars and Time Series
Graphs
Summary
FAQ
What benefits does using TensorBoard with YOLO11 offer?
How can I monitor training metrics using TensorBoard when training a YOLO11 model?
What kind of metrics can I visualize with TensorBoard when training YOLO11 models?
Can I use TensorBoard in a Google Colab environment for training YOLO11?
Gain Visual Insights with YOLO11's Integration with TensorBoard
Understanding and fine-tuning computer vision models like Ultralytics' YOLO11 becomes more straightforward when you take a closer look at their training processes. Model training visualization helps with getting insights into the model's learning patterns, performance metrics, and overall behavior. YOLO11's integration with TensorBoard makes this process of visualization and analysis easier and enables more efficient and informed adjustments to the model.

This guide covers how to use TensorBoard with YOLO11. You'll learn about various visualizations, from tracking metrics to analyzing model graphs. These tools will help you understand your YOLO11 model's performance better.

TensorBoard
Tensorboard Overview

TensorBoard, TensorFlow's visualization toolkit, is essential for machine learning experimentation. TensorBoard features a range of visualization tools, crucial for monitoring machine learning models. These tools include tracking key metrics like loss and accuracy, visualizing model graphs, and viewing histograms of weights and biases over time. It also provides capabilities for projecting embeddings to lower-dimensional spaces and displaying multimedia data.

YOLO11 Training with TensorBoard
Using TensorBoard while training YOLO11 models is straightforward and offers significant benefits.

Installation
To install the required package, run:

Installation


CLI

# Install the required package for YOLO11 and Tensorboard
pip install ultralytics

TensorBoard is conveniently pre-installed with YOLO11, eliminating the need for additional setup for visualization purposes.

For detailed instructions and best practices related to the installation process, be sure to check our YOLO11 Installation guide. While installing the required packages for YOLO11, if you encounter any difficulties, consult our Common Issues guide for solutions and tips.

Configuring TensorBoard for Google Colab
When using Google Colab, it's important to set up TensorBoard before starting your training code:

Configure TensorBoard for Google Colab


Python

%load_ext tensorboard
%tensorboard --logdir path/to/runs

Usage
Before diving into the usage instructions, be sure to check out the range of YOLO11 models offered by Ultralytics. This will help you choose the most appropriate model for your project requirements.

Enable or Disable TensorBoard

By default, TensorBoard logging is disabled. You can enable or disable the logging by using the yolo settings command.


CLI

# Enable TensorBoard logging
yolo settings tensorboard=True

# Disable TensorBoard logging
yolo settings tensorboard=False

Usage


Python

from ultralytics import YOLO

# Load a pre-trained model
model = YOLO("yolo11n.pt")

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

Upon running the usage code snippet above, you can expect the following output:


TensorBoard: Start with 'tensorboard --logdir path_to_your_tensorboard_logs', view at http://localhost:6006/
This output indicates that TensorBoard is now actively monitoring your YOLO11 training session. You can access the TensorBoard dashboard by visiting the provided URL (http://localhost:6006/) to view real-time training metrics and model performance. For users working in Google Colab, the TensorBoard will be displayed in the same cell where you executed the TensorBoard configuration commands.

For more information related to the model training process, be sure to check our YOLO11 Model Training guide. If you are interested in learning more about logging, checkpoints, plotting, and file management, read our usage guide on configuration.

Understanding Your TensorBoard for YOLO11 Training
Now, let's focus on understanding the various features and components of TensorBoard in the context of YOLO11 training. The three key sections of the TensorBoard are Time Series, Scalars, and Graphs.

Time Series
The Time Series feature in the TensorBoard offers a dynamic and detailed perspective of various training metrics over time for YOLO11 models. It focuses on the progression and trends of metrics across training epochs. Here's an example of what you can expect to see.

image

Key Features of Time Series in TensorBoard
Filter Tags and Pinned Cards: This functionality allows users to filter specific metrics and pin cards for quick comparison and access. It's particularly useful for focusing on specific aspects of the training process.

Detailed Metric Cards: Time Series divides metrics into different categories like learning rate (lr), training (train), and validation (val) metrics, each represented by individual cards.

Graphical Display: Each card in the Time Series section shows a detailed graph of a specific metric over the course of training. This visual representation aids in identifying trends, patterns, or anomalies in the training process.

In-Depth Analysis: Time Series provides an in-depth analysis of each metric. For instance, different learning rate segments are shown, offering insights into how adjustments in learning rate impact the model's learning curve.

Importance of Time Series in YOLO11 Training
The Time Series section is essential for a thorough analysis of the YOLO11 model's training progress. It lets you track the metrics in real time to promptly identify and solve issues. It also offers a detailed view of each metric's progression, which is crucial for fine-tuning the model and enhancing its performance.

Scalars
Scalars in the TensorBoard are crucial for plotting and analyzing simple metrics like loss and accuracy during the training of YOLO11 models. They offer a clear and concise view of how these metrics evolve with each training epoch, providing insights into the model's learning effectiveness and stability. Here's an example of what you can expect to see.

image

Key Features of Scalars in TensorBoard
Learning Rate (lr) Tags: These tags show the variations in the learning rate across different segments (e.g., pg0, pg1, pg2). This helps us understand the impact of learning rate adjustments on the training process.

Metrics Tags: Scalars include performance indicators such as:

mAP50 (B): Mean Average Precision at 50% Intersection over Union (IoU), crucial for assessing object detection accuracy.

mAP50-95 (B): Mean Average Precision calculated over a range of IoU thresholds, offering a more comprehensive evaluation of accuracy.

Precision (B): Indicates the ratio of correctly predicted positive observations, key to understanding prediction accuracy.

Recall (B): Important for models where missing a detection is significant, this metric measures the ability to detect all relevant instances.

To learn more about the different metrics, read our guide on performance metrics.

Training and Validation Tags (train, val): These tags display metrics specifically for the training and validation datasets, allowing for a comparative analysis of model performance across different data sets.

Importance of Monitoring Scalars
Observing scalar metrics is crucial for fine-tuning the YOLO11 model. Variations in these metrics, such as spikes or irregular patterns in loss graphs, can highlight potential issues such as overfitting, underfitting, or inappropriate learning rate settings. By closely monitoring these scalars, you can make informed decisions to optimize the training process, ensuring that the model learns effectively and achieves the desired performance.

Difference Between Scalars and Time Series
While both Scalars and Time Series in TensorBoard are used for tracking metrics, they serve slightly different purposes. Scalars focus on plotting simple metrics such as loss and accuracy as scalar values. They provide a high-level overview of how these metrics change with each training epoch. Meanwhile, the time-series section of the TensorBoard offers a more detailed timeline view of various metrics. It is particularly useful for monitoring the progression and trends of metrics over time, providing a deeper dive into the specifics of the training process.

Graphs
The Graphs section of the TensorBoard visualizes the computational graph of the YOLO11 model, showing how operations and data flow within the model. It's a powerful tool for understanding the model's structure, ensuring that all layers are connected correctly, and for identifying any potential bottlenecks in data flow. Here's an example of what you can expect to see.

image

Graphs are particularly useful for debugging the model, especially in complex architectures typical in deep learning models like YOLO11. They help in verifying layer connections and the overall design of the model.

Summary
This guide aims to help you use TensorBoard with YOLO11 for visualization and analysis of machine learning model training. It focuses on explaining how key TensorBoard features can provide insights into training metrics and model performance during YOLO11 training sessions.

For a more detailed exploration of these features and effective utilization strategies, you can refer to TensorFlow's official TensorBoard documentation and their GitHub repository.

Want to learn more about the various integrations of Ultralytics? Check out the Ultralytics integrations guide page to see what other exciting capabilities are waiting to be discovered!

FAQ
What benefits does using TensorBoard with YOLO11 offer?
Using TensorBoard with YOLO11 provides several visualization tools essential for efficient model training:

Real-Time Metrics Tracking: Track key metrics such as loss, accuracy, precision, and recall live.
Model Graph Visualization: Understand and debug the model architecture by visualizing computational graphs.
Embedding Visualization: Project embeddings to lower-dimensional spaces for better insight.
These tools enable you to make informed adjustments to enhance your YOLO11 model's performance. For more details on TensorBoard features, check out the TensorFlow TensorBoard guide.

How can I monitor training metrics using TensorBoard when training a YOLO11 model?
To monitor training metrics while training a YOLO11 model with TensorBoard, follow these steps:

Install TensorBoard and YOLO11: Run pip install ultralytics which includes TensorBoard.
Configure TensorBoard Logging: During the training process, YOLO11 logs metrics to a specified log directory.
Start TensorBoard: Launch TensorBoard using the command tensorboard --logdir path/to/your/tensorboard/logs.
The TensorBoard dashboard, accessible via http://localhost:6006/, provides real-time insights into various training metrics. For a deeper dive into training configurations, visit our YOLO11 Configuration guide.

What kind of metrics can I visualize with TensorBoard when training YOLO11 models?
When training YOLO11 models, TensorBoard allows you to visualize an array of important metrics including:

Loss (Training and Validation): Indicates how well the model is performing during training and validation.
Accuracy/Precision/Recall: Key performance metrics to evaluate detection accuracy.
Learning Rate: Track learning rate changes to understand its impact on training dynamics.
mAP (mean Average Precision): For a comprehensive evaluation of object detection accuracy at various IoU thresholds.
These visualizations are essential for tracking model performance and making necessary optimizations. For more information on these metrics, refer to our Performance Metrics guide.

Can I use TensorBoard in a Google Colab environment for training YOLO11?
Yes, you can use TensorBoard in a Google Colab environment to train YOLO11 models. Here's a quick setup:

Configure TensorBoard for Google Colab


Python

%load_ext tensorboard
%tensorboard --logdir path/to/runs
Then, run the YOLO11 training script:


from ultralytics import YOLO

# Load a pre-trained model
model = YOLO("yolo11n.pt")

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

TensorBoard will visualize the training progress within Colab, providing real-time insights into metrics like loss and accuracy. For additional details on configuring YOLO11 training, see our detailed YOLO11 Installation guide.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 3 months ago
glenn-jocher
Y-T-G
RizwanMunawar
UltralyticsAssistant
willie.maddox@gmail.com
MatthewNoyce
abirami-vina

Tweet

Share

Comments
 Back to top
Previous
Ray Tune
Next
VS Code
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Skip to main content
TensorFlow
Install
Learn

API

Resources

Community

Why TensorFlow

Search
/

English
GitHub

Ray
TensorBoard
Overview
Guide
Filter

TensorFlow
Resources
TensorBoard
Guide
Was this helpful?

Get started with TensorBoard

bookmark_border
Run in Google Colab
View source on GitHub
Download notebook
In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.

This quickstart will show how to quickly get started with TensorBoard. The remaining guides in this website provide more details on specific capabilities, many of which are not included here.


# Load the TensorBoard notebook extension
%load_ext tensorboard

import tensorflow as tf
import datetime

# Clear any logs from previous runs
rm -rf ./logs/
Using the MNIST dataset as the example, normalize the data and write a function that creates a simple Keras model for classifying the images into 10 classes.


mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

def create_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(28, 28), name='layers_input'),
    tf.keras.layers.Flatten(name='layers_flatten'),
    tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),
    tf.keras.layers.Dropout(0.2, name='layers_dropout'),
    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')
  ])
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
Using TensorBoard with Keras Model.fit()
When training with Keras's Model.fit(), adding the tf.keras.callbacks.TensorBoard callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with histogram_freq=1 (this is off by default)

Place the logs in a timestamped subdirectory to allow easy selection of different training runs.

model = create_model()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.fit(x=x_train, 
          y=y_train, 
          epochs=5, 
          validation_data=(x_test, y_test), 
          callbacks=[tensorboard_callback])
Train on 60000 samples, validate on 10000 samples
Epoch 1/5
60000/60000 [==============================] - 15s 246us/sample - loss: 0.2217 - accuracy: 0.9343 - val_loss: 0.1019 - val_accuracy: 0.9685
Epoch 2/5
60000/60000 [==============================] - 14s 229us/sample - loss: 0.0975 - accuracy: 0.9698 - val_loss: 0.0787 - val_accuracy: 0.9758
Epoch 3/5
60000/60000 [==============================] - 14s 231us/sample - loss: 0.0718 - accuracy: 0.9771 - val_loss: 0.0698 - val_accuracy: 0.9781
Epoch 4/5
60000/60000 [==============================] - 14s 227us/sample - loss: 0.0540 - accuracy: 0.9820 - val_loss: 0.0685 - val_accuracy: 0.9795
Epoch 5/5
60000/60000 [==============================] - 14s 228us/sample - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0623 - val_accuracy: 0.9823
<tensorflow.python.keras.callbacks.History at 0x7fc8a5ee02e8>
Start TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the %tensorboard line magic. On the command line, run the same command without "%".

%tensorboard --logdir logs/fit


A brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:

Scalars show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the Time Series or Scalars dashboards.
Graphs help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the Graphs dashboard.
Histograms and Distributions show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the Time Series or Histograms dashboards. Distributions can be found in the Distributions dashboard.
Additional TensorBoard dashboards are automatically enabled when you log other types of data. For example, the Keras TensorBoard callback lets you log images and embeddings as well. You can see what other dashboards are available in TensorBoard by clicking on the "inactive" dropdown towards the top right.

Using TensorBoard with other methods
When training with methods such as tf.GradientTape(), use tf.summary to log the required information.

Use the same dataset as above, but convert it to tf.data.Dataset to take advantage of batching capabilities:

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))

train_dataset = train_dataset.shuffle(60000).batch(64)
test_dataset = test_dataset.batch(64)
The training code follows the advanced quickstart tutorial, but shows how to log metrics to TensorBoard. Choose loss and optimizer:

loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()
Create stateful metrics that can be used to accumulate values during training and logged at any point:

# Define our metrics
train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')
test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')
Define the training and test functions:

def train_step(model, optimizer, x_train, y_train):
  with tf.GradientTape() as tape:
    predictions = model(x_train, training=True)
    loss = loss_object(y_train, predictions)
  grads = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(grads, model.trainable_variables))

  train_loss(loss)
  train_accuracy(y_train, predictions)

def test_step(model, x_test, y_test):
  predictions = model(x_test)
  loss = loss_object(y_test, predictions)

  test_loss(loss)
  test_accuracy(y_test, predictions)
Set up summary writers to write the summaries to disk in a different logs directory:

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
train_log_dir = 'logs/gradient_tape/' + current_time + '/train'
test_log_dir = 'logs/gradient_tape/' + current_time + '/test'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
test_summary_writer = tf.summary.create_file_writer(test_log_dir)
Start training. Use tf.summary.scalar() to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk. You have control over which metrics to log and how often to do it. Other tf.summary functions enable logging other types of data.

model = create_model() # reset our model

EPOCHS = 5

for epoch in range(EPOCHS):
  for (x_train, y_train) in train_dataset:
    train_step(model, optimizer, x_train, y_train)
  with train_summary_writer.as_default():
    tf.summary.scalar('loss', train_loss.result(), step=epoch)
    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)

  for (x_test, y_test) in test_dataset:
    test_step(model, x_test, y_test)
  with test_summary_writer.as_default():
    tf.summary.scalar('loss', test_loss.result(), step=epoch)
    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)

  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_loss.result(), 
                         train_accuracy.result()*100,
                         test_loss.result(), 
                         test_accuracy.result()*100))

  # Reset metrics every epoch
  train_loss.reset_state()
  test_loss.reset_state()
  train_accuracy.reset_state()
  test_accuracy.reset_state()
Epoch 1, Loss: 0.24321186542510986, Accuracy: 92.84333801269531, Test Loss: 0.13006582856178284, Test Accuracy: 95.9000015258789
Epoch 2, Loss: 0.10446818172931671, Accuracy: 96.84833526611328, Test Loss: 0.08867532759904861, Test Accuracy: 97.1199951171875
Epoch 3, Loss: 0.07096975296735764, Accuracy: 97.80166625976562, Test Loss: 0.07875105738639832, Test Accuracy: 97.48999786376953
Epoch 4, Loss: 0.05380449816584587, Accuracy: 98.34166717529297, Test Loss: 0.07712937891483307, Test Accuracy: 97.56999969482422
Epoch 5, Loss: 0.041443776339292526, Accuracy: 98.71833038330078, Test Loss: 0.07514958828687668, Test Accuracy: 97.5
Open TensorBoard again, this time pointing it at the new log directory. We could have also started TensorBoard to monitor training while it progresses.

%tensorboard --logdir logs/gradient_tape


That's it! You have now seen how to use TensorBoard both through the Keras callback and through tf.summary for more custom scenarios.

Was this helpful?

Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2024-11-12 UTC.

Stay connected
Blog
Forum
GitHub
Twitter
YouTube
Support
Issue tracker
Release notes
Stack Overflow
Brand guidelines
Cite TensorFlow
Terms
Privacy
Sign up for the TensorFlow newsletter
Subscribe

English
The new page has loaded.
Access new GPU Droplets, powered by AMD! MI325X is here

Storage autoscaling is now available for Managed Databases. Learn more

Announcing compute and networking updates to scale infrastructure. Read blog

Announcing cost-efficient storage updates. Read blog

Gateway API on Kubernetes is here, offering a next-generation traffic management solution.

Access new GPU Droplets, accelerated by NVIDIA! H200s are here

Single Sign-On is now available on DigitalOcean through Okta. Learn more

Model Context Protocol (MCP) is now available on DigitalOcean. Learn more

Blog
Docs
Get Support
Contact Sales
DigitalOcean

Products
Featured Products

Droplets
Scalable virtual machines
Kubernetes
Scale more effectively
Gradient‚Ñ¢ AI Agentic Cloud
Build and scale with AI
Cloudways
Managed cloud hosting
App Platform
Get apps to market faster
Managed Databases
Fully-managed database hosting
Compute

Droplets
Kubernetes
CPU-Optimized Droplets
Functions
App Platform
Gradient‚Ñ¢ AI Agentic Cloud

GPU Droplets
1-Click Models
Platform
Bare Metal GPUs
Backups & Snapshots

Backups
Snapshots
SnapShooter
Networking

Virtual Private Cloud (VPC)
Partner Network Connect
Cloud Firewalls
Load Balancers
DNS
DDoS Protection
Managed Databases

MongoDB
Kafka
MySQL
PostgreSQL
Valkey
OpenSearch
Storage

Spaces Object Storage
Volume Block Storage
Developer Tools

API
CLI
Support Plans
Monitoring
Uptime
Identity and Access Management
Marketplace

Droplet 1-Click
Kubernetes 1-Click
AI 1-Click Models
Add-Ons
Cloud Website Hosting

Cloudways
See all products

Solutions
AI and Machine Learning
Develop, train, and deploy AI apps
GPUs
Platform
1-Click Models
HR Knowledge Assistant
Code Copilot
Support Ticket Triage
Recommendation Engine
Blockchain
Infrastructure for decentralized apps
Blogs, Forums and Content Websites
Lightning-fast, reliable CMS hosting
Wordpress
Ghost
Mastodon
Data Analytics
Real-time data processing at scale
Data Streaming
AdTech & Martech
Kafka
Developer Tools
DevOps and CI/CD solutions
CI/CD
Prototyping
Digital Marketing Agencies
Power your clients‚Äô websites and campaigns
Freelancer
IT Consulting
Ecommerce
Build beautiful online storefronts
Dropshipping
WooCommerce
Magento
Game Development
Low-latency multiplayer servers
Minecraft Hosting
IoT
Connect to the power of the cloud
Kafka
ISVs
Streamlined ISV application development
Secure Web Hosting
Powerful protection from DDoS and more
Private VPN
Startup Cloud Hosting
Scalable, cost-effective infrastructure
Small Business
Video Streaming
High-bandwidth, low-latency delivery
Kafka
Web and Mobile Apps
Simple cross-platform app hosting
cPanel
Docker
Next.js
Node.js
Website Hosting
Fast page loads and reliable site uptime
VPS Hosting
Virtual Machines
Get help

Migration Assistance
Talk to an expert
See all solutions

Developers
Our Community

Community Home
DevOps and development guides
CSS-Tricks
All things web design
The Wave
Content to level up your business.
Resources

Tutorials
Questions and Answers
Marketplace
Tools
Write for DOnations
Cloud Chats
Customer Stories
DigitalOcean Blog
Pricing Calculator
Get Involved

Hatch Startup Program
Open Source Sponsorships
Hacktoberfest
Deploy 2025
Wavemakers Program
Documentation

Quickstart
Compute
Gradient‚Ñ¢ AI Platform
Storage
Managed Databases
Containers
Billing
API Reference

Partners
DigitalOcean Partner Programs

Become a Partner
Partner Services Program
DigitalOcean AI Partner Program
Marketplace
Hatch Partner Program
Connect with a Partner
Partner Programs Resources

Customer Stories
DigitalOcean Onboarding Series
Training for Agencies and Freelancers
Price Estimate Calculator
Featured Partner Articles

Cloud cost optimization best practices

Read more
How to choose a cloud provider

Read more
DigitalOcean vs. AWS Lightsail: Which Cloud Platform is Right for You?

Read more
Questions?

Talk to an expert
Pricing
Log in
Sign up
Tutorials
Questions
Product Docs
Cloud Chats

Search Community
Table of contents
Prerequisites
From Prediction Score to Class Label
PrecisionRecall Curve
Average Precision AP
Intersection over Union IoU
Mean Average Precision mAP for Object Detection
FAQs
Conclusion
TutorialsAI/MLEvaluating Object Detection Models Using Mean Average Precision (mAP)
Tutorial
Evaluating Object Detection Models Using Mean Average Precision (mAP)
Updated on August 5, 2025

AI/ML
Deep Learning
Object Detection
Computer Vision
Ahmed Fawzy GadJames SkeltonShaoni Mukherjee
By Ahmed Fawzy Gad, James Skelton and Shaoni Mukherjee

Evaluating Object Detection Models Using Mean Average Precision (mAP)
To evaluate object detection models like R-CNN and YOLO, the mean average precision (mAP) is used. The mAP compares the ground-truth bounding box to the detected box and returns a score. The higher the score, the more accurate the model is in its detections.

In my last article we looked in detail at the confusion matrix, model accuracy, precision, and recall. We used the Scikit-learn library to calculate these metrics as well. Now we‚Äôll extend our discussion to see how precision and recall are used to calculate the mAP.

Here are the sections covered in this tutorial:

From Prediction Score to Class Label
Precision-Recall Curve
Average Precision (AP)
Intersection over Union (IoU)
Mean Average Precision (mAP) for Object Detection
Key takeaways:

Mean Average Precision (mAP) is a common metric for evaluating object detection models that captures the trade-off between precision and recall across all classes by summarizing the area under the precision-recall curve for each class and then averaging these values.
To compute mAP, one calculates the Average Precision (AP) for each object class‚Äîtypically by integrating the precision-recall curve or using set recall thresholds‚Äîthen averages the APs of all classes to produce a single number representing overall detection performance.
A high mAP score indicates that a model detects objects with both high precision (few false positives) and high recall (few missed targets), whereas a low mAP suggests the model struggles (e.g., either missing many objects or raising many false alarms); modern benchmarks like COCO use a stringent mAP definition that averages performance across multiple IoU thresholds (e.g., 50% to 95%) for a more thorough evaluation.
Prerequisites
In order to follow along with this article experience with Python code, and a beginners understanding of Deep Learning. We will operate under the assumption that all readers have access to sufficiently powerful machines, so they can run the code provided.

For instructions on getting started with Python code, we recommend trying this beginners guide to set up your system and preparing to run beginner tutorials.

Info: Experience the power of AI and machine learning with DigitalOcean GPU Droplets. Leverage NVIDIA H100 GPUs to accelerate your AI/ML workloads, deep learning projects, and high-performance computing tasks with simple, flexible, and cost-effective cloud solutions.

Sign up today to access GPU Droplets and scale your AI projects on demand without breaking the bank.

From Prediction Score to Class Label
In this section we‚Äôll do a quick review of how a class label is derived from a prediction score.

Given that there are two classes, Positive and Negative, here are the ground-truth labels of 10 samples.

    y_true = ["positive", "negative", "negative", "positive", "positive", "positive", "negative", "positive", "negative", "positive"]
When these samples are fed to the model it returns the following prediction scores. Based on these scores, how do we classify the samples (i.e. assign a class label to each sample)?

    pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3]
To convert the scores into a class label, a threshold is used. When the score is equal to or above the threshold, the sample is classified as one class. Otherwise, it is classified as the other class. Let‚Äôs agree that a sample is Positive if its score is above or equal to the threshold. Otherwise, it is Negative. The next block of code converts the scores into class labels with a threshold of 0.5.

    import numpy

    pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3]
    y_true = ["positive", "negative", "negative", "positive", "positive", "positive", "negative", "positive", "negative", "positive"]

    threshold = 0.5
    y_pred = ["positive" if score >= threshold else "negative" for score in pred_scores]
    print(y_pred)

    ['positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative']
Now both the ground-truth and predicted labels are available in the y_true and y_pred variables. Based on these labels, the confusion matrix, precision, and recall can be calculated.

   r = numpy.flip(sklearn.metrics.confusion_matrix(y_true, y_pred))
   print(r)
   
   precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label="positive")
   print(precision)
   
   recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label="positive")
   print(recall)

   # Confusion Matrix (From Left to Right & Top to Bottom: True Positive, False Negative, False Positive, True Negative)
   [[4 2]
    [1 3]]
   
   # Precision = 4/(4+1)
   0.8
   
   # Recall = 4/(4+2)
   0.6666666666666666
After this quick review of calculating the precision and recall, in the next section we‚Äôll discuss creating the precision-recall curve.

Precision-Recall Curve
From the definition of both the precision and recall given in Part 1, remember that the higher the precision, the more confident the model is when it classifies a sample as Positive. The higher the recall, the more positive samples the model correctly classified as Positive.

When a model has high recall but low precision, then the model classifies most of the positive samples correctly but it has many false positives (i.e. classifies many Negative samples as Positive). When a model has high precision but low recall, then the model is accurate when it classifies a sample as Positive but it may classify only some of the positive samples.

Due to the importance of both precision and recall, there is a precision-recall curve the shows the tradeoff between the precision and recall values for different thresholds. This curve helps to select the best threshold to maximize both metrics.

There are some inputs needed to create the precision-recall curve:

The ground-truth labels.
The prediction scores of the samples.
Some thresholds to convert the prediction scores into class labels.
The next block of code creates the y_true list to hold the ground-truth labels, the pred_scores list for the prediction scores, and finally the thresholds list for different threshold values.

    import numpy

    y_true = ["positive", "negative", "negative", "positive", "positive", "positive", "negative", "positive", "negative", "positive", "positive", "positive", "positive", "negative", "negative", "negative"]

    pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]

    thresholds = numpy.arange(start=0.2, stop=0.7, step=0.05)
Here are the thresholds saved in the thresholds list. Because there are 10 thresholds, 10 values for precision and recall will be created.

    [0.2, 
     0.25, 
     0.3, 
     0.35, 
     0.4, 
     0.45, 
     0.5, 
     0.55, 
     0.6, 
     0.65]
The next function named precision_recall_curve() accepts the ground-truth labels, prediction scores, and thresholds. It returns two equal-length lists representing the precision and recall values.

    import sklearn.metrics
    
    def precision_recall_curve(y_true, pred_scores, thresholds):
        precisions = []
        recalls = []

        for threshold in thresholds:
            y_pred = ["positive" if score >= threshold else "negative" for score in pred_scores]

            precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label="positive")
            recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label="positive")

            precisions.append(precision)
            recalls.append(recall)

        return precisions, recalls
The next code calls the precision_recall_curve() function after passing the three previously prepared lists. It returns the precisions and recalls lists that hold all the values of the precisions and recalls, respectively.

    precisions, recalls = precision_recall_curve(y_true=y_true, 
                                                 pred_scores=pred_scores,
                                                 thresholds=thresholds)
Here are the returned values in the precisions list.

    [0.5625,
     0.5714285714285714,
     0.5714285714285714,
     0.6363636363636364,
     0.7,
     0.875,
     0.875,
     1.0,
     1.0,
     1.0]
Here is the list of  values in the recalls list.

    [1.0,
     0.8888888888888888,
     0.8888888888888888,
     0.7777777777777778,
     0.7777777777777778,
     0.7777777777777778,
     0.7777777777777778,
     0.6666666666666666,
     0.5555555555555556,
     0.4444444444444444]
Given the two lists of equal lengths, it is possible to plot their values in a 2D plot as shown below.

    matplotlib.pyplot.plot(recalls, precisions, linewidth=4, color="red")
    matplotlib.pyplot.xlabel("Recall", fontsize=12, fontweight='bold')
    matplotlib.pyplot.ylabel("Precision", fontsize=12, fontweight='bold')
    matplotlib.pyplot.title("Precision-Recall Curve", fontsize=15, fontweight="bold")
    matplotlib.pyplot.show()
The precision-recall curve is shown in the next figure. Note that as the recall increases, the precision decreases. The reason is that when the number of positive samples increases (high recall), the accuracy of classifying each sample correctly decreases (low precision). This is expected, as the model is more likely to fail when there are many samples.

Precision Recall Curve

The precision-recall curve makes it easy to decide the point where both the precision and recall are high. According to the previous figure, the best point is (recall, precision)=(0.778, 0.875).

Graphically deciding the best values for both the precision and recall might work using the previous figure because the curve is not complex. A better way is to use a metric called the f1 score, which is calculated according to the next equation.

Precision Recall Formula

The f1 metric measures the balance between precision and recall. When the value of f1 is high, this means both the precision and recall are high. A lower f1 score means a greater imbalance between precision and recall.

According to the previous example, the f1 is calculated according to the code below. According to the values in the f1 list, the highest score is 0.82352941. It is the 6th element in the list (i.e. index 5). The 6th elements in the recalls and precisions lists are 0.778 and 0.875, respectively. The corresponding threshold value is 0.45.

    f1 = 2 * ((numpy.array(precisions) * numpy.array(recalls)) / (numpy.array(precisions) + numpy.array(recalls)))

    [0.72, 
     0.69565217, 
     0.69565217, 
     0.7,
     0.73684211,
     0.82352941, 
     0.82352941, 
     0.8, 
     0.71428571, 0
     .61538462]
The next figure shows, in blue, the location of the point that corresponds to the best balance between the recall and the precision. In conclusion, the best threshold to balance the precision and recall is 0.45 at which the precision is 0.875 and the recall is 0.778.

    matplotlib.pyplot.plot(recalls, precisions, linewidth=4, color="red", zorder=0)
    matplotlib.pyplot.scatter(recalls[5], precisions[5], zorder=1, linewidth=6)

    matplotlib.pyplot.xlabel("Recall", fontsize=12, fontweight='bold')
    matplotlib.pyplot.ylabel("Precision", fontsize=12, fontweight='bold')
    matplotlib.pyplot.title("Precision-Recall Curve", fontsize=15, fontweight="bold")
    matplotlib.pyplot.show()
Precision Recall Curve

After the precision-recall curve is discussed, the next section discusses how to calculate the average precision.

Average Precision (AP)
The average precision (AP) is a way to summarize the precision-recall curve into a single value representing the average of all precisions. The AP is calculated according to the next equation. Using a loop that goes through all precisions/recalls, the difference between the current and next recalls is calculated and then multiplied by the current precision. In other words, the AP is the weighted sum of precisions at each threshold where the weight is the increase in recall.

Formula

It is important to append the recalls and precisions lists by 0 and 1, respectively. For example, if the recalls list is [0.8, 0.6], then it should have 0 appended to be [0.8, 0.6, 0.0]. The same happens for the precisions list but have 1 rather than 0 appended (e.g. [0.8, 0.2, 1.0]).

Given that both recalls and precisions are NumPy arrays, the previous equation is modeled according to the next Python line.

    AP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])
Here is the complete code that calculates the AP.

    import numpy
    import sklearn.metrics

    def precision_recall_curve(y_true, pred_scores, thresholds):
        precisions = []
        recalls = []

        for threshold in thresholds:
            y_pred = ["positive" if score >= threshold else "negative" for score in pred_scores]

            precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label="positive")
            recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label="positive")

            precisions.append(precision)
            recalls.append(recall)

        return precisions, recalls

    y_true = ["positive", "negative", "negative", "positive", "positive", "positive", "negative", "positive", "negative", "positive", "positive", "positive", "positive", "negative", "negative", "negative"]
    pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.4, 0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.2, 0.3, 0.35]
    thresholds=numpy.arange(start=0.2, stop=0.7, step=0.05)

    precisions, recalls = precision_recall_curve(y_true=y_true, 
                                                 pred_scores=pred_scores, 
                                                 thresholds=thresholds)

    precisions.append(1)
    recalls.append(0)
    
    precisions = numpy.array(precisions)
    recalls = numpy.array(recalls)
    
    AP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])
    print(AP)
This is all about the average precision. Here is a summary of the steps to calculate the AP:

Generate the prediction scores using the model.
Convert the prediction scores to class labels.
Calculate the confusion matrix.
Calculate the precision and recall metrics.
Create the precision-recall curve.
Measure the average precision.
The next section talks about the intersection over union (IoU) which is how an object detection generates the prediction scores.

Intersection over Union (IoU)
To train an object detection model, usually, there are 2 inputs:

An image.
Ground-truth bounding boxes for each object in the image.
The model predicts the bounding boxes of the detected objects. It is expected that the predicted box will not match exactly the ground-truth box. The next figure shows a cat image. The ground-truth box of the object is in red while the predicted one is in yellow. Based on the visualization of the 2 boxes, is the model made a good prediction with a high match score?

It is difficult to subjectively evaluate the model predictions. For example, someone may conclude that there is a 50% match while someone else notices that there is a 60% match.

Fig05-1

Image without labels from Pixabay by susannp4

A better alternative is to use a quantitative measure to score how the ground-truth and predicted boxes match. This measure is the intersection over union (IoU). The IoU helps to know if a region has an object or not.

The IoU is calculated according to the next equation by dividing the area of intersection between the 2 boxes by the area of their union. The higher the IoU, the better the prediction.

Intersection Area - Union Area

The next figure shows 3 cases with different IoUs. Note that the IoUs at the top of each case are objectively measured and may differ a bit from the reality but it makes sense.

For case A, the predicted box in yellow is so far from being aligned on the red ground-truth box and thus the IoU score is 0.2 (i.e. there is only a 20% overlap between the 2 boxes).

For case B, the intersection area between the 2 boxes is larger but the 2 boxes are still not aligned well and thus the IoU score is 0.5.

For case C, the coordinates of the 2 boxes are so close and thus their IoU is 0.9 (i.e. there is a 90% overlap between the 2 boxes).

Note that the IoU is 0.0 when there is a 0% overlap between the predicted and ground-truth boxes. The IoU is 1.0 when the 2 boxes fit each other 100%.

IoU

To calculate the IoU for an image, here is a function named intersection_over_union(). It accepts the following 2 parameters:

gt_box: Ground-truth bounding box.
pred_box: Predicted bounding box.
It calculates the intersection and union between the 2 boxes in the intersection and union variables, respectively. Moreover, the IoU is calculated in the iou variable. It returns all of these 3 variables.

    def intersection_over_union(gt_box, pred_box):
        inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]
        inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]
    
        inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]
        inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]
    
        intersection = inter_box_w * inter_box_h
        union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection
        
        iou = intersection / union
    
        return iou, intersection, union
The bounding box passed to the function is a list of 4 elements which are:

The x-axis of the top-left corner.
The y-axis of the top-left corner.
Width.
Height.
Here are the ground-truth and predicted bounding boxes of the car image.

    gt_box = [320, 220, 680, 900]
    pred_box = [500, 320, 550, 700]
Given that the image is named cat.jpg, here is the complete that draws the bounding boxes over the image.

    import imageio
    import matplotlib.pyplot
    import matplotlib.patches
    
    def intersection_over_union(gt_box, pred_box):
        inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]
        inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]
    
        inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]
        inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]
    
        intersection = inter_box_w * inter_box_h
        union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection
        
        iou = intersection / union
    
        return iou, intersection, union
    
    im = imageio.imread("cat.jpg")
    
    gt_box = [320, 220, 680, 900]
    pred_box = [500, 320, 550, 700]
    
    fig, ax = matplotlib.pyplot.subplots(1)
    ax.imshow(im)
    
    gt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),
                                           gt_box[2],
                                           gt_box[3],
                                           linewidth=5,
                                           edgecolor='r',
                                           facecolor='none')
    
    pred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),
                                             pred_box[2],
                                             pred_box[3],
                                             linewidth=5,
                                             edgecolor=(1, 1, 0),
                                             facecolor='none')
    ax.add_patch(gt_rect)
    ax.add_patch(pred_rect)
    
    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])
The next figure shows the image with the bounding boxes.

Fig08

To calculate the IoU, just call the intersection_over_union() function. Based on the bounding boxes, the IoU score is 0.54.

    iou, intersect, union = intersection_over_union(gt_box, pred_box)
    print(iou, intersect, union)

    0.5409582689335394 350000 647000
The IoU score 0.54 means there is a 54% overlap between the ground-truth and predicted bounding boxes. Looking at the boxes, someone may visually feel it is good enough to conclude that the model detected the cat object. Someone else may feel the model is not yet accurate as the predicted box does not fit the ground-truth box well.

To objectively judge whether the model predicted the box location correctly or not, a threshold is used. If the model predicts a box with an IoU score greater than or equal to the threshold, then there is a high overlap between the predicted box and one of the ground-truth boxes. This means the model was able to detect an object successfully. The detected region is classified as Positive (i.e. contains an object).

On the other hand, when the IoU score is smaller than the threshold, then the model made a bad prediction as the predicted box does not overlap with the ground-truth box. This means the detected region is classified as Negative (i.e. does not contain an object).

class(IoU)

Let‚Äôs have an example to clarify how the IoU scores help to classify a region as an object or not. Assume the object detection model is fed by the next image where there are 2 target objects with their ground-truth boxes in red and the predicted boxes are in yellow.

The next code reads the image (given it is named pets.jpg), draws the boxes, and calculates the IoU for each object. The IoU for the left object is 0.76 while the other object has an IoU score of 0.26.

    import matplotlib.pyplot
    import matplotlib.patches
    import imageio

    def intersection_over_union(gt_box, pred_box):
        inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]
        inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]

        inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]
        inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]

        intersection = inter_box_w * inter_box_h
        union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection

        iou = intersection / union

        return iou, intersection, union, 

    im = imageio.imread("pets.jpg")

    gt_box = [10, 130, 370, 350]
    pred_box = [30, 100, 370, 350]

    iou, intersect, union = intersection_over_union(gt_box, pred_box)
    print(iou, intersect, union)

    fig, ax = matplotlib.pyplot.subplots(1)
    ax.imshow(im)

    gt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),
                                           gt_box[2],
                                           gt_box[3],
                                           linewidth=5,
                                           edgecolor='r',
                                           facecolor='none')

    pred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),
                                             pred_box[2],
                                             pred_box[3],
                                             linewidth=5,
                                             edgecolor=(1, 1, 0),
                                             facecolor='none')
    ax.add_patch(gt_rect)
    ax.add_patch(pred_rect)

    gt_box = [645, 130, 310, 320]
    pred_box = [500, 60, 310, 320]

    iou, intersect, union = intersection_over_union(gt_box, pred_box)
    print(iou, intersect, union)

    gt_rect = matplotlib.patches.Rectangle((gt_box[0], gt_box[1]),
                                           gt_box[2],
                                           gt_box[3],
                                           linewidth=5,
                                           edgecolor='r',
                                           facecolor='none')

    pred_rect = matplotlib.patches.Rectangle((pred_box[0], pred_box[1]),
                                             pred_box[2],
                                             pred_box[3],
                                             linewidth=5,
                                             edgecolor=(1, 1, 0),
                                             facecolor='none')
    ax.add_patch(gt_rect)
    ax.add_patch(pred_rect)

    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])
Given that the IoU threshold is 0.6, then only the regions with IoU scores greater than or equal to 0.6 are classified as Positive (i.e. having objects). Thus, the box with IoU score 0.76 is Positive while the other box with IoU of 0.26 is Negative.

Fig10

Image without Labels from hindustantimes.com

If the threshold changed to be 0.2 rather than 0.6, then both predictions are Positive. If the threshold is 0.8, then both predictions are Negative.

As a summary, the IoU score measures how close is the predicted box to the ground-truth box. It ranges from 0.0 to 1.0 where 1.0 is the optimal result. When the IoU is greater than the threshold, then the box is classified as Positive as it surrounds an object. Otherwise, it is classified as Negative.

The next section shows how to benefit from the IoUs to calculate the mean average precision (mAP) for an object detection model.

Mean Average Precision (mAP) for Object Detection
Usually, the object detection models are evaluated with different IoU thresholds where each threshold may give different predictions from the other thresholds. Assume that the model is fed by an image that has 10 objects distributed across 2 classes. How to calculate the mAP?

To calculate the mAP, start by calculating the AP for each class. The mean of the APs for all classes is the mAP.

Assuming that the dataset used has only 2 classes. For the first class, here are the ground-truth labels and predicted scores in the y_true and pred_scores variables, respectively.

    y_true = ["positive", "negative", "positive", "negative", "positive", "positive", "positive", "negative", "positive", "negative"]

    pred_scores = [0.7, 0.3, 0.5, 0.6, 0.55, 0.9, 0.75, 0.2, 0.8, 0.3]
Here are the y_true and pred_scores variables of the second class.

    y_true = ["negative", "positive", "positive", "negative", "negative", "positive", "positive", "positive", "negative", "positive"]

    pred_scores = [0.32, 0.9, 0.5, 0.1, 0.25, 0.9, 0.55, 0.3, 0.35, 0.85]
The list of IoU thresholds starts from 0.2 to 0.9 with 0.25 step.

    thresholds = numpy.arange(start=0.2, stop=0.9, step=0.05
To calculate the AP for a class, just feed its y_true and pred_scores variables to the next code.

    precisions, recalls = precision_recall_curve(y_true=y_true, 
                                                 pred_scores=pred_scores, 
                                                 thresholds=thresholds)

    matplotlib.pyplot.plot(recalls, precisions, linewidth=4, color="red", zorder=0)

    matplotlib.pyplot.xlabel("Recall", fontsize=12, fontweight='bold')
    matplotlib.pyplot.ylabel("Precision", fontsize=12, fontweight='bold')
    matplotlib.pyplot.title("Precision-Recall Curve", fontsize=15, fontweight="bold")
    matplotlib.pyplot.show()

    precisions.append(1)
    recalls.append(0)

    precisions = numpy.array(precisions)
    recalls = numpy.array(recalls)

    AP = numpy.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])
    print(AP)
For the first class, here is its precision-recall curve. Based on this curve, the AP is 0.949.

Precision Recall Curve

The precision-recall curve of the second class is shown below. Its AP is 0.958.

Precision Recall Curve

Based on the APs of the 2 classes (0.949 and 0.958), the mAP of the object detection model is calculated according to the next equation.

the number of classes

Based on this equation, the mAP is 0.9535.

    mAP = (0.949 + 0.958)/2 = 0.9535
FAQ‚Äôs
Q: How to calculate mAP for object detection models step-by-step?

A: Calculating mAP involves several sequential steps:

Step 1: Generate predictions with confidence scores and bounding boxes for test dataset.
Step 2: Sort predictions by confidence score in descending order.
Step 3: For each confidence threshold, calculate precision and recall using IoU threshold (typically 0.5).
Step 4: Plot precision-recall curve for each class.
Step 5: Calculate Average Precision (AP) for each class by computing area under PR curve using 11-point interpolation or all-point interpolation.
Step 6: Average AP scores across all classes to get mAP. COCO mAP: Average mAP across IoU thresholds from 0.5 to 0.95 in steps of 0.05.
Implementation: Use libraries like pycocotools or implement custom evaluation functions.
Validation: Ensure consistent IoU calculation and proper handling of multiple detections per object.
Q: mAP vs IoU vs Dice coefficient: which metric to use for different computer vision tasks?

A: Choose evaluation metrics based on task requirements:

mAP for object detection evaluating both localization and classification accuracy across multiple classes and confidence thresholds.
IoU (Intersection over Union) for measuring spatial overlap between predicted and ground truth regions, suitable for single-object localization tasks. Dice coefficient for segmentation tasks, particularly medical imaging where class imbalance is common.
Use cases: mAP for detection benchmarks (COCO, Pascal VOC), IoU for bounding box regression evaluation, Dice for pixel-level segmentation.
Advantages: mAP provides comprehensive detection performance assessment, IoU offers intuitive spatial overlap measurement, Dice handles class imbalance better than IoU.
Considerations: mAP requires careful threshold selection, IoU sensitive to small objects, Dice may favor larger objects.
Combined approach: Use multiple metrics for comprehensive evaluation.
Q: What are common mistakes when computing mAP for object detection evaluation?

A: Common mAP calculation errors include: Incorrect IoU computation using wrong coordinate systems or bounding box formats (xywh vs xyxy). Improper confidence threshold handling not sorting predictions correctly or using wrong thresholds for precision-recall calculation. Multiple detection handling failing to suppress duplicate detections of same object using NMS properly. Class-specific evaluation computing mAP incorrectly by not handling missing classes or zero-detection cases. Ground truth matching using wrong IoU thresholds or not implementing proper assignment algorithms. Interpolation errors in precision-recall curve calculation using incorrect 11-point vs all-point methods. Implementation fixes: Use established libraries like pycocotools, validate against known benchmarks, implement comprehensive unit tests for evaluation functions.

Q: How to interpret mAP scores for different object detection applications?

A: mAP interpretation varies by application domain and requirements:

High-precision applications (medical imaging, autonomous vehicles) require mAP > 0.90 for safety-critical decisions.
General object detection considers mAP > 0.50 good, > 0.70 excellent for most commercial applications.
Real-time applications may accept lower mAP (0.30-0.50) for speed requirements.
Class-specific analysis: Examine per-class AP to identify struggling categories and data imbalance issues.
IoU sensitivity: COCO mAP (averaged across IoU 0.5-0.95) is more stringent than Pascal VOC.
Contextual factors: Consider dataset difficulty, object sizes, class distribution when interpreting scores.
Improvement strategies: Focus on classes with lowest AP scores, analyze failure cases, implement targeted data augmentation.
Benchmarking: Compare against published results on same datasets for meaningful evaluation.
Q: What tools and libraries are best for computing mAP in object detection projects?

A: Several tools provide reliable mAP computation:

pycocotools - gold standard for COCO-style evaluation with comprehensive metrics and established validation.
torchmetrics - PyTorch-integrated metrics with GPU acceleration and easy integration into training loops.
Object Detection Metrics - comprehensive Python library supporting multiple evaluation protocols.
TensorFlow Object Detection API - built-in evaluation tools with pre-configured metrics.
Detectron2 - Facebook‚Äôs framework with robust evaluation utilities and visualization tools.
Custom implementations - for specialized requirements or educational purposes, but validate against established libraries.
Features to consider: Multi-threading support, memory efficiency for large datasets, visualization capabilities, export formats for results.
Best practices: Use pycocotools for final evaluation, torchmetrics for training monitoring, validate custom implementations against established tools.
Conclusion
This tutorial discussed how to calculate the mean average precision (mAP) for an object detection model. We started by discussing how to convert a prediction score to a class label. Using different thresholds, a precision-recall curve is created. From that curve, the average precision (AP) is measured.

For an object detection model, the threshold is the intersection over union (IoU) that scores the detected objects. Once the AP is measured for each class in the dataset, the mAP is calculated.

Thanks for learning with the DigitalOcean Community. Check out our offerings for compute, storage, networking, and managed databases.

Learn more about our products

About the author(s)
Ahmed Fawzy Gad
Ahmed Fawzy Gad
Author
James Skelton
James Skelton
Editor
Technical Evangelist // AI Arcanist
See author profile
Shaoni Mukherjee
Shaoni Mukherjee
Editor
Technical Writer
See author profile
With a strong background in data science and over six years of experience, I am passionate about creating in-depth content on technologies. Currently focused on AI, machine learning, and GPU computing, working on topics ranging from deep learning frameworks to optimizing GPU-based workloads.

Category:
Tutorial
Tags:
AI/ML
Deep Learning
Object Detection
Computer Vision

Still looking for an answer?
Ask a question
Search for more help
Was this helpful?
Yes
No
Comments(0)
Follow-up questions(0)

















Ôªø
This textbox defaults to using Markdown to format your answer.

You can type !ref in this text area to quickly search our full set of tutorials, documentation & marketplace offerings and insert the link!

Sign in/up to comment
Creative Commons
This work is licensed under a Creative Commons Attribution-NonCommercial- ShareAlike 4.0 International License.
Limited Time: Introductory GPU Droplet pricing.
Get simple AI infrastructure starting at $2.99/GPU/hr on-demand. Try GPU Droplets now!

Popular Topics
AI/ML
Ubuntu
Linux Basics
JavaScript
Python
MySQL
Docker
Kubernetes
All tutorials
Talk to an expert
Connect on Discord
Join the conversation in our Discord to connect with fellow developers
Visit Discord
Featured tutorials
SOLID Design Principles Explained: Building Better Software Architecture
How To Remove Docker Images, Containers, and Volumes
How to Create a MySQL User and Grant Privileges (Step-by-Step)
All tutorials
All topic tags

Congratulations on unlocking the whale ambience easter egg!
Click the whale button in the bottom left of your screen to toggle some ambient whale noises while you read.

Reset easter egg to be discovered again

Permanently dismiss and hide easter egg


Thank you to the Glacier Bay National Park & Preserve and Merrick079 for the sounds behind this easter egg.


Interested in whales, protecting them, and their connection to helping prevent climate change? We recommend checking out the Whale and Dolphin Conservation.


Become a contributor for community
Get paid to write technical tutorials and select a tech-focused charity to receive a matching donation.

Sign Up


DigitalOcean Documentation
Full documentation for every DigitalOcean product.

Learn more


Resources for startups and SMBs
The Wave has everything you need to know about building a business, from raising funding to marketing your product.

Learn more

Get our newsletter
Stay up to date by signing up for DigitalOcean‚Äôs Infrastructure as a Newsletter.

Email address
Submit
New accounts only. By submitting your email you agree to our Privacy Policy

The developer cloud
Scale up as you grow ‚Äî whether you're running one virtual machine or ten thousand.

View all products

Get started for free
Sign up and get $200 in credit for your first 60 days with DigitalOcean.*

Get started

*This promotional offer applies to new accounts only.

Company
About
Leadership
Blog
Careers
Customers
Partners
Referral Program
Affiliate Program
Press
Legal
Privacy Policy
Security
Investor Relations
Products
Overview
Droplets
Kubernetes
Functions
App Platform
Gradient‚Ñ¢ AI GPU Droplets
Gradient‚Ñ¢ AI Bare Metal GPUs
Gradient‚Ñ¢ AI 1-Click Models
Gradient‚Ñ¢ AI Platform
Load Balancers
Managed Databases
Spaces
Block Storage
API
Uptime
Identity and Access Management
Cloudways
Resources
Community Tutorials
Community Q&A
CSS-Tricks
Write for DOnations
Currents Research
Hatch Startup Program
Wavemakers Program
Compass Council
Open Source
Newsletter Signup
Marketplace
Pricing
Pricing Calculator
Documentation
Release Notes
Code of Conduct
Shop Swag
Solutions
Website Hosting
VPS Hosting
Web & Mobile Apps
Game Development
Streaming
VPN
SaaS Platforms
Cloud Hosting for Blockchain
Startup Resources
Migration Assistance
Contact
Support
Sales
Report Abuse
System Status
Share your ideas
¬© 2025 DigitalOcean, LLC.
Sitemap.
Cookie Preferences
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
Tips for Model Training




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Guides
YOLO Common Issues
YOLO Performance Metrics
YOLO Thread-Safe Inference
YOLO Data Augmentation
Model Deployment Options
Model YAML Configuration Guide
K-Fold Cross Validation
Hyperparameter Tuning
SAHI Tiled Inference
AzureML Quickstart
Conda Quickstart
Docker Quickstart
Raspberry Pi
NVIDIA Jetson
DeepStream on NVIDIA Jetson
Triton Inference Server
Isolating Segmentation Objects
Edge TPU on Raspberry Pi
Viewing Inference Images in a Terminal
OpenVINO Latency vs Throughput modes
ROS Quickstart
Steps of a Computer Vision Project
Defining A Computer Vision Project's Goals
Data Collection and Annotation
Preprocessing Annotated Data
Tips for Model Training
Insights on Model Evaluation and Fine-Tuning
A Guide on Model Testing
Best Practices for Model Deployment
Maintaining Your Computer Vision Model
Deploying YOLO on Vertex AI in Docker container
Explorer
Explorer API
Explorer Dashboard Demo
VOC Exploration Example
YOLOv5
Quickstart
Environments
Tutorials
Table of contents
Introduction
How to Train a Machine Learning Model
Training on Large Datasets
Batch Size and GPU Utilization
Subset Training
Multi-scale Training
Caching
Mixed Precision Training
Pre-trained Weights
Other Techniques to Consider When Handling a Large Dataset
The Number of Epochs To Train For
Early Stopping
Choosing Between Cloud and Local Training
Selecting an Optimizer
Common Optimizers
Connecting with the Community
Community Resources
Official Documentation
Key Takeaways
FAQ
How can I improve GPU utilization when training a large dataset with Ultralytics YOLO?
What is mixed precision training, and how do I enable it in YOLO11?
How does multiscale training enhance YOLO11 model performance?
How can I use pre-trained weights to speed up training in YOLO11?
What is the recommended number of epochs for training a model, and how do I set this in YOLO11?
Machine Learning Best Practices and Tips for Model Training
Introduction
One of the most important steps when working on a computer vision project is model training. Before reaching this step, you need to define your goals and collect and annotate your data. After preprocessing the data to make sure it is clean and consistent, you can move on to training your model.



Watch: Model Training Tips | How to Handle Large Datasets | Batch Size, GPU Utilization and Mixed Precision

So, what is model training? Model training is the process of teaching your model to recognize visual patterns and make predictions based on your data. It directly impacts the performance and accuracy of your application. In this guide, we'll cover best practices, optimization techniques, and troubleshooting tips to help you train your computer vision models effectively.

How to Train a Machine Learning Model
A computer vision model is trained by adjusting its internal parameters to minimize errors. Initially, the model is fed a large set of labeled images. It makes predictions about what is in these images, and the predictions are compared to the actual labels or contents to calculate errors. These errors show how far off the model's predictions are from the true values.

During training, the model iteratively makes predictions, calculates errors, and updates its parameters through a process called backpropagation. In this process, the model adjusts its internal parameters (weights and biases) to reduce the errors. By repeating this cycle many times, the model gradually improves its accuracy. Over time, it learns to recognize complex patterns such as shapes, colors, and textures.

What is Backpropagation?

This learning process makes it possible for the computer vision model to perform various tasks, including object detection, instance segmentation, and image classification. The ultimate goal is to create a model that can generalize its learning to new, unseen images so that it can accurately understand visual data in real-world applications.

Now that we know what is happening behind the scenes when we train a model, let's look at points to consider when training a model.

Training on Large Datasets
There are a few different aspects to think about when you are planning on using a large dataset to train a model. For example, you can adjust the batch size, control the GPU utilization, choose to use multiscale training, etc. Let's walk through each of these options in detail.

Batch Size and GPU Utilization
When training models on large datasets, efficiently utilizing your GPU is key. Batch size is an important factor. It is the number of data samples that a machine learning model processes in a single training iteration. Using the maximum batch size supported by your GPU, you can fully take advantage of its capabilities and reduce the time model training takes. However, you want to avoid running out of GPU memory. If you encounter memory errors, reduce the batch size incrementally until the model trains smoothly.



Watch: How to Use Batch Inference with Ultralytics YOLO11 | Speed Up Object Detection in Python üéâ

With respect to YOLO11, you can set the batch_size parameter in the training configuration to match your GPU capacity. Also, setting batch=-1 in your training script will automatically determine the batch size that can be efficiently processed based on your device's capabilities. By fine-tuning the batch size, you can make the most of your GPU resources and improve the overall training process.

Subset Training
Subset training is a smart strategy that involves training your model on a smaller set of data that represents the larger dataset. It can save time and resources, especially during initial model development and testing. If you are running short on time or experimenting with different model configurations, subset training is a good option.

When it comes to YOLO11, you can easily implement subset training by using the fraction parameter. This parameter lets you specify what fraction of your dataset to use for training. For example, setting fraction=0.1 will train your model on 10% of the data. You can use this technique for quick iterations and tuning your model before committing to training a model using a full dataset. Subset training helps you make rapid progress and identify potential issues early on.

Multi-scale Training
Multiscale training is a technique that improves your model's ability to generalize by training it on images of varying sizes. Your model can learn to detect objects at different scales and distances and become more robust.

For example, when you train YOLO11, you can enable multiscale training by setting the scale parameter. This parameter adjusts the size of training images by a specified factor, simulating objects at different distances. For example, setting scale=0.5 randomly zooms training images by a factor between 0.5 and 1.5 during training. Configuring this parameter allows your model to experience a variety of image scales and improve its detection capabilities across different object sizes and scenarios.

Caching
Caching is an important technique to improve the efficiency of training machine learning models. By storing preprocessed images in memory, caching reduces the time the GPU spends waiting for data to be loaded from the disk. The model can continuously receive data without delays caused by disk I/O operations.

Caching can be controlled when training YOLO11 using the cache parameter:

cache=True: Stores dataset images in RAM, providing the fastest access speed but at the cost of increased memory usage.
cache='disk': Stores the images on disk, slower than RAM but faster than loading fresh data each time.
cache=False: Disables caching, relying entirely on disk I/O, which is the slowest option.
Mixed Precision Training
Mixed precision training uses both 16-bit (FP16) and 32-bit (FP32) floating-point types. The strengths of both FP16 and FP32 are leveraged by using FP16 for faster computation and FP32 to maintain precision where needed. Most of the neural network's operations are done in FP16 to benefit from faster computation and lower memory usage. However, a master copy of the model's weights is kept in FP32 to ensure accuracy during the weight update steps. You can handle larger models or larger batch sizes within the same hardware constraints.

Mixed Precision Training Overview

To implement mixed precision training, you'll need to modify your training scripts and ensure your hardware (like GPUs) supports it. Many modern deep learning frameworks, such as PyTorch and TensorFlow, offer built-in support for mixed precision.

Mixed precision training is straightforward when working with YOLO11. You can use the amp flag in your training configuration. Setting amp=True enables Automatic Mixed Precision (AMP) training. Mixed precision training is a simple yet effective way to optimize your model training process.

Pre-trained Weights
Using pretrained weights is a smart way to speed up your model's training process. Pretrained weights come from models already trained on large datasets, giving your model a head start. Transfer learning adapts pretrained models to new, related tasks. Fine-tuning a pre-trained model involves starting with these weights and then continuing training on your specific dataset. This method of training results in faster training times and often better performance because the model starts with a solid understanding of basic features.

The pretrained parameter makes transfer learning easy with YOLO11. Setting pretrained=True will use default pre-trained weights, or you can specify a path to a custom pre-trained model. Using pre-trained weights and transfer learning effectively boosts your model's capabilities and reduces training costs.

Other Techniques to Consider When Handling a Large Dataset
There are a couple of other techniques to consider when handling a large dataset:

Learning Rate Schedulers: Implementing learning rate schedulers dynamically adjusts the learning rate during training. A well-tuned learning rate can prevent the model from overshooting minima and improve stability. When training YOLO11, the lrf parameter helps manage learning rate scheduling by setting the final learning rate as a fraction of the initial rate.
Distributed Training: For handling large datasets, distributed training can be a game-changer. You can reduce the training time by spreading the training workload across multiple GPUs or machines. This approach is particularly valuable for enterprise-scale projects with substantial computational resources.
The Number of Epochs To Train For
When training a model, an epoch refers to one complete pass through the entire training dataset. During an epoch, the model processes each example in the training set once and updates its parameters based on the learning algorithm. Multiple epochs are usually needed to allow the model to learn and refine its parameters over time.

A common question that comes up is how to determine the number of epochs to train the model for. A good starting point is 300 epochs. If the model overfits early, you can reduce the number of epochs. If overfitting does not occur after 300 epochs, you can extend the training to 600, 1200, or more epochs.

However, the ideal number of epochs can vary based on your dataset's size and project goals. Larger datasets might require more epochs for the model to learn effectively, while smaller datasets might need fewer epochs to avoid overfitting. With respect to YOLO11, you can set the epochs parameter in your training script.

Early Stopping
Early stopping is a valuable technique for optimizing model training. By monitoring validation performance, you can halt training once the model stops improving. You can save computational resources and prevent overfitting.

The process involves setting a patience parameter that determines how many epochs to wait for an improvement in validation metrics before stopping training. If the model's performance does not improve within these epochs, training is stopped to avoid wasting time and resources.

Early Stopping Overview

For YOLO11, you can enable early stopping by setting the patience parameter in your training configuration. For example, patience=5 means training will stop if there's no improvement in validation metrics for 5 consecutive epochs. Using this method ensures the training process remains efficient and achieves optimal performance without excessive computation.

Choosing Between Cloud and Local Training
There are two options for training your model: cloud training and local training.

Cloud training offers scalability and powerful hardware and is ideal for handling large datasets and complex models. Platforms like Google Cloud, AWS, and Azure provide on-demand access to high-performance GPUs and TPUs, speeding up training times and enabling experiments with larger models. However, cloud training can be expensive, especially for long periods, and data transfer can add to costs and latency.

Local training provides greater control and customization, letting you tailor your environment to specific needs and avoid ongoing cloud costs. It can be more economical for long-term projects, and since your data stays on-premises, it's more secure. However, local hardware may have resource limitations and require maintenance, which can lead to longer training times for large models.

Selecting an Optimizer
An optimizer is an algorithm that adjusts the weights of your neural network to minimize the loss function, which measures how well the model is performing. In simpler terms, the optimizer helps the model learn by tweaking its parameters to reduce errors. Choosing the right optimizer directly affects how quickly and accurately the model learns.

You can also fine-tune optimizer parameters to improve model performance. Adjusting the learning rate sets the size of the steps when updating parameters. For stability, you might start with a moderate learning rate and gradually decrease it over time to improve long-term learning. Additionally, setting the momentum determines how much influence past updates have on current updates. A common value for momentum is around 0.9. It generally provides a good balance.

Common Optimizers
Different optimizers have various strengths and weaknesses. Let's take a glimpse at a few common optimizers.

SGD (Stochastic Gradient Descent):

Updates model parameters using the gradient of the loss function with respect to the parameters.
Simple and efficient but can be slow to converge and might get stuck in local minima.
Adam (Adaptive Moment Estimation):

Combines the benefits of both SGD with momentum and RMSProp.
Adjusts the learning rate for each parameter based on estimates of the first and second moments of the gradients.
Well-suited for noisy data and sparse gradients.
Efficient and generally requires less tuning, making it a recommended optimizer for YOLO11.
RMSProp (Root Mean Square Propagation):

Adjusts the learning rate for each parameter by dividing the gradient by a running average of the magnitudes of recent gradients.
Helps in handling the vanishing gradient problem and is effective for recurrent neural networks.
For YOLO11, the optimizer parameter lets you choose from various optimizers, including SGD, Adam, AdamW, NAdam, RAdam, and RMSProp, or you can set it to auto for automatic selection based on model configuration.

Connecting with the Community
Being part of a community of computer vision enthusiasts can help you solve problems and learn faster. Here are some ways to connect, get help, and share ideas.

Community Resources
GitHub Issues: Visit the YOLO11 GitHub repository and use the Issues tab to ask questions, report bugs, and suggest new features. The community and maintainers are very active and ready to help.
Ultralytics Discord Server: Join the Ultralytics Discord server to chat with other users and developers, get support, and share your experiences.
Official Documentation
Ultralytics YOLO11 Documentation: Check out the official YOLO11 documentation for detailed guides and helpful tips on various computer vision projects.
Using these resources will help you solve challenges and stay up-to-date with the latest trends and practices in the computer vision community.

Key Takeaways
Training computer vision models involves following good practices, optimizing your strategies, and solving problems as they arise. Techniques like adjusting batch sizes, mixed precision training, and starting with pre-trained weights can make your models work better and train faster. Methods like subset training and early stopping help you save time and resources. Staying connected with the community and keeping up with new trends will help you keep improving your model training skills.

FAQ
How can I improve GPU utilization when training a large dataset with Ultralytics YOLO?
To improve GPU utilization, set the batch_size parameter in your training configuration to the maximum size supported by your GPU. This ensures that you make full use of the GPU's capabilities, reducing training time. If you encounter memory errors, incrementally reduce the batch size until training runs smoothly. For YOLO11, setting batch=-1 in your training script will automatically determine the optimal batch size for efficient processing. For further information, refer to the training configuration.

What is mixed precision training, and how do I enable it in YOLO11?
Mixed precision training utilizes both 16-bit (FP16) and 32-bit (FP32) floating-point types to balance computational speed and precision. This approach speeds up training and reduces memory usage without sacrificing model accuracy. To enable mixed precision training in YOLO11, set the amp parameter to True in your training configuration. This activates Automatic Mixed Precision (AMP) training. For more details on this optimization technique, see the training configuration.

How does multiscale training enhance YOLO11 model performance?
Multiscale training enhances model performance by training on images of varying sizes, allowing the model to better generalize across different scales and distances. In YOLO11, you can enable multiscale training by setting the scale parameter in the training configuration. For example, scale=0.5 reduces the image size by half, while scale=2.0 doubles it. This technique simulates objects at different distances, making the model more robust across various scenarios. For settings and more details, check out the training configuration.

How can I use pre-trained weights to speed up training in YOLO11?
Using pre-trained weights can greatly accelerate training and enhance model accuracy by leveraging a model already familiar with foundational visual features. In YOLO11, simply set the pretrained parameter to True or provide a path to your custom pre-trained weights in the training configuration. This method, called transfer learning, allows models trained on large datasets to be effectively adapted to your specific application. Learn more about how to use pre-trained weights and their benefits in the training configuration guide.

What is the recommended number of epochs for training a model, and how do I set this in YOLO11?
The number of epochs refers to the complete passes through the training dataset during model training. A typical starting point is 300 epochs. If your model overfits early, you can reduce the number. Alternatively, if overfitting isn't observed, you might extend training to 600, 1200, or more epochs. To set this in YOLO11, use the epochs parameter in your training script. For additional advice on determining the ideal number of epochs, refer to this section on number of epochs.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 1 month ago
glenn-jocher
RizwanMunawar
UltralyticsAssistant
Y-T-G
onuralpszr
willie.maddox@gmail.com
Laughing-q
abirami-vina

Tweet

Share

Comments
 Back to top
Previous
Preprocessing Annotated Data
Next
Insights on Model Evaluation and Fine-Tuning
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How Data Augmentation Works
Real-World Applications
Data Augmentation vs. Related Concepts
Glossary
Data Augmentation
Enhance your machine learning models with data augmentation. Discover techniques to boost accuracy, reduce overfitting, and improve robustness.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Data augmentation is a critical technique in machine learning (ML) used to artificially increase the size and diversity of a training dataset. This is achieved by creating modified, yet realistic, versions of existing data samples. The primary goal is to improve the performance and robustness of AI models, particularly in computer vision (CV), by exposing them to a wider variety of conditions during training. This helps prevent overfitting, where a model learns the training data too well but fails to generalize to new, unseen data, ultimately leading to higher model accuracy.

How Data Augmentation Works
In computer vision, data augmentation involves applying a series of transformations to images. These transformations are designed to simulate real-world variations that a model might encounter after deployment. Common techniques include:

Geometric Transformations: These alter the spatial orientation of an image. Examples include random rotations, scaling, translations (shifting), cropping, and flipping (horizontal or vertical). These teach the model to be invariant to the object's position and orientation.
Color Space Transformations: These modify the color properties of an image. Adjustments to brightness, contrast, saturation, and hue make the model more resilient to changes in lighting conditions.
Advanced Techniques: More complex methods involve altering image content more significantly. These can include adding random noise (like Gaussian noise), applying blurring effects, or using methods like Mixup, which creates new images by linearly combining two existing ones, and Cutout, which randomly removes regions of an image. You can learn more about these methods in The Ultimate Guide to Data Augmentation.
Many deep learning frameworks, like PyTorch and TensorFlow, provide tools for data augmentation. Specialized libraries like Albumentations offer a vast collection of high-performance augmentation techniques and are integrated with models like Ultralytics YOLO11 to diversify training data seamlessly.

Real-World Applications
Data augmentation is a standard practice across many domains to build more reliable AI systems.

AI in Healthcare: In medical image analysis, datasets are often small due to patient privacy regulations and the rarity of certain diseases. To train a model for detecting tumors in scans, augmentation techniques like rotation, scaling, and brightness changes create a more diverse set of training examples. This helps the model accurately identify anomalies regardless of variations in imaging equipment or patient positioning, improving diagnostic reliability.
AI for Automotive: Developing robust object detection systems for autonomous vehicles requires data from countless driving scenarios. Instead of collecting data for every possible condition, augmentation can simulate different weather (e.g., adding synthetic rain or snow), lighting (day, dusk, night), and occlusions (e.g., a pedestrian partially hidden by another car). This makes the vehicle's perception system more dependable in unpredictable real-world environments.
Other significant applications include AI in manufacturing for quality control and AI in agriculture for detecting crop diseases under varying field conditions.

Data Augmentation vs. Related Concepts
It is important to distinguish data augmentation from other data-related techniques.

Synthetic Data: While both methods enhance datasets, they operate differently. Data augmentation modifies existing real data. In contrast, synthetic data generation creates entirely new, artificial data from scratch using simulations or generative models like GANs. While augmentation expands variance around observed data, synthetic data can create novel scenarios not present in the original dataset, a concept explored in this overview of synthetic data in computer vision.
Data Cleaning: Data cleaning is a part of the broader data preprocessing pipeline that focuses on identifying and correcting errors, inconsistencies, and inaccuracies in a dataset. Its goal is to improve data quality. Data augmentation, on the other hand, is about increasing data quantity and variety. A clean dataset is the ideal starting point before applying augmentation.
Transfer Learning: This technique involves using a model pre-trained on a large benchmark dataset like ImageNet and then fine-tuning it on a smaller, task-specific dataset. Data augmentation is often used during the fine-tuning stage to further improve performance and prevent overfitting on the new data.
Platforms like Ultralytics HUB streamline the entire model training process, incorporating data augmentation as a key step to help users build powerful, state-of-the-art vision AI models.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Guides
The ultimate guide to data augmentation in 2025

Abirami Vina
6 min read

February 14, 2025

Find out how image data augmentation helps Vision AI models learn better, enhance accuracy, and perform more effectively in real-world situations.


What is image data augmentation?
The importance of data augmentation in computer vision
When should you use image data augmentation?
How image data augmentation works
Key image data augmentation techniques
The role of generative AI in image data augmentation
Limitations of image data augmentation
A real-world application of image data augmentation
Key takeaways
Due to the AI boom, phenomena like robots working in factories and self-driving cars navigating streets are making headlines more often. AI is changing the way machines interact with the world, from improving medical imaging to assisting with quality control on production lines.

A large part of this progress comes from computer vision, a branch of AI that makes it possible for machines to understand and interpret images. Just like humans learn to recognize objects and patterns over time, Vision AI models like Ultralytics YOLO11 need to be trained on large amounts of image data to develop their visual understanding.

However, collecting such a vast amount of visual data isn‚Äôt always easy. Even though the computer vision community has created many large datasets, they can still miss certain variations - like images with objects in low light, partially hidden items, or things viewed from different angles. These differences can be confusing for computer vision models that have only been trained on specific conditions.

Image data augmentation is a technique that solves this problem by introducing new variations into existing data. By making changes to images, like adjusting colors, rotating, or shifting perspective, the dataset becomes more diverse, helping Vision AI models recognize objects better in real-world situations.

In this article, we‚Äôll explore how image data augmentation works and the impact it can have on computer vision applications.

What is image data augmentation?
Let‚Äôs say you are trying to recognize a friend in a crowd, but they are wearing sunglasses or standing in a shady spot. Even with these minor changes in appearance, you still know who they are. On the other hand, a Vision AI model may struggle with such variations unless it has been trained to recognize objects in different settings.

Image data augmentation improves computer vision model performance by adding modified versions of existing images to the training data, instead of collecting thousands of new images. 

Changes to images like flipping, rotating, adjusting brightness, or adding small distortions expose Vision AI models to a wider range of conditions. Instead of relying on massive datasets, models can learn efficiently from smaller training datasets with augmented images. 

__wf_reserved_inherit
Fig 1. Examples of augmented images of a car.
‚Äç

The importance of data augmentation in computer vision
Here are some of the key reasons why augmentation is essential for computer vision:

Reduces data requirements: Collecting large image datasets requires time and resources. Augmentation can be used to train models effectively without needing massive datasets.
‚Äç
Prevents overfitting: A model trained on too few examples may memorize details instead of recognizing general patterns. Adding variety through augmentation ensures Vision AI models learn in a way that applies to new and unseen data.
‚Äç
Mimics imperfect images: Images in datasets are often too perfect, but real-world photos can be blurry, obscured, or distorted. Augmenting images with noise, occlusions, or other variations makes them more realistic.
‚Äç
Enhances model robustness: Training with a variety of images helps AI handle real-world changes, making it more reliable in different environments, lighting conditions, and situations.
When should you use image data augmentation?
Image data augmentation is particularly helpful when a computer vision model needs to recognize objects in different situations but doesn‚Äôt have enough varied images. 

For example, if researchers are training a Vision AI model to identify rare underwater species that are rarely photographed, the dataset may be small or lack variation. By augmenting the images - adjusting colors to simulate different water depths, adding noise to mimic murky conditions, or slightly altering shapes to account for natural movement - the model can learn to detect underwater objects more accurately.

Here are some other situations where augmentation makes a big difference:

Balancing the dataset: Some objects may appear less often in training data, making Vision AI models biased. Augmentation helps create more examples of rare objects so the model can recognize all categories fairly.
‚Äç
Adapting to different cameras: Images can look different depending on the device. Augmentation helps Vision AI models perform well on photos with different resolutions, lighting, and quality.
‚Äç
Correcting minor labeling errors: Slight shifts, cropping, or rotations help computer vision models recognize objects correctly, even if the original labels are not perfectly aligned.
How image data augmentation works
In the early days of computer vision, image data augmentation primarily involved basic image processing techniques such as flipping, rotating, and cropping to increase dataset diversity. As AI improved, more advanced methods were introduced, such as adjusting colors (color space transformations), sharpening or blurring images (kernel filters), and blending multiple images together (image mixing) to enhance learning.

Augmentation can happen before and during model training. Before training, modified images can be added to the dataset to provide more variety. During training, images can be randomly altered in real time, helping Vision AI models adapt to different conditions.

These changes are made using mathematical transformations. For example, rotation tilts an image, cropping removes parts to mimic different views, and brightness changes simulate lighting variations. Blurring softens images, sharpening makes details clearer, and image mixing combines parts of different images. Vision AI frameworks and tools like OpenCV, TensorFlow, and PyTorch can automate these processes, making augmentation fast and effective.

Key image data augmentation techniques
Now that we've discussed what image data augmentation is, let's take a closer look at some fundamental image data augmentation techniques used to enhance training data.

Adjusting orientation and position
Computer vision models like YOLO11 often need to recognize objects from various angles and viewpoints. To help with this, images can be flipped horizontally or vertically so the AI model learns to recognize objects from different viewpoints. 

Similarly, rotating images slightly changes their angle, allowing the model to identify objects from multiple perspectives. Also, shifting images in different directions (translation) helps models adjust to small positional changes. These transformations make sure models generalize better to real-world conditions where object placement in an image is unpredictable.

__wf_reserved_inherit
Fig 2. Different orientation and position-related augmentation methods.
‚Äç

Resizing and cropping
With respect to real-world computer vision solutions, objects in images can appear at varying distances and sizes. Vision AI models have to be robust enough to detect them regardless of these differences. 

To improve adaptability, the following augmentation methods can be used:

Scaling: Resizing changes the image size while maintaining its proportions, letting AI models detect objects at different distances.
‚Äç
Cropping: This removes unnecessary parts of an image, helping the model focus on key areas and reducing background distractions.
‚Äç
Shearing: Skewing an image slightly simulates a tilted or stretched appearance, helping AI recognize objects from different angles.
These adjustments help computer vision models recognize objects even if their size or shape changes slightly.

Perspective and distortion adjustments
Objects in images can appear differently depending on the camera angle, making recognition difficult for computer vision models. To help models handle these variations, augmentation techniques can adjust how objects are presented in images. 

For instance, perspective transforms can change the viewing angle, making an object look as if it‚Äôs being seen from a different position. This allows Vision AI models to recognize objects even when they are tilted or captured from an unusual viewpoint. 

Another example is an elastic transform that stretches, bends, or warps images to simulate natural distortions so that objects appear as they would in reflections or under pressure. 

Color and lighting modifications
Lighting conditions and color differences can significantly impact how Vision AI models interpret images. Since objects can appear differently under various lighting settings, the following augmentation techniques can help handle these situations:

Brightness and contrast adjustments: Simulating different lighting conditions helps Vision AI models recognize objects in both bright and dark environments.
‚Äç
Color jittering: Randomly changing hue, saturation, and color balance makes computer vision models more adaptable to different cameras and lighting conditions.
‚Äç
Grayscale conversion: Converting images to black and white encourages Vision AI models to focus on shapes and textures rather than color.
__wf_reserved_inherit
Fig 3. Examples of augmentations related to color variations.
‚Äç

Advanced image data augmentation techniques
Till now, we've only explored augmentation techniques that modify a single image. However, some advanced methods involve combining multiple images to improve AI learning.

For example, MixUp blends two images together, helping computer vision models understand object relationships and improving their ability to generalize across different scenarios. CutMix takes this a step further by replacing a section of one image with a part of another, enabling models to learn from multiple contexts within the same image. Meanwhile, CutOut works differently by removing random parts of an image, training Vision AI models to recognize objects even when they are partially hidden or obstructed.

__wf_reserved_inherit
Fig 4. Advanced image data augmentation techniques.
‚Äç

The role of generative AI in image data augmentation
Generative AI is gaining traction across many industries and everyday applications. You‚Äôve likely encountered it in relation to AI-generated images, deepfake videos, or apps that create realistic avatars. But beyond creativity and entertainment, Generative AI plays a crucial role in training Vision AI models by generating new images from existing ones.

Rather than simply flipping or rotating pictures, it can create realistic variations - changing facial expressions, clothing styles, or even simulating different weather conditions. These variations help computer vision models become more adaptable and accurate in diverse real-world scenarios. Advanced generative AI models like GANs (Generative Adversarial Networks) and diffusion models can also fill in missing details or create high-quality synthetic images.

Limitations of image data augmentation
While data augmentation improves training datasets, there are also some limitations to consider. Here are a few key challenges related to image data augmentation:

Limited data diversity: Augmented images come from existing data and cannot introduce completely new patterns or rare perspectives.
‚Äç
Potential data distortion: Excessive transformations can make images unrealistic, potentially reducing model accuracy in real-world scenarios.
‚Äç
Increased computation: Real-time augmentation that takes place during model training can require quite a bit of processing power, slowing down training and increasing memory usage.
‚Äç
Class imbalance remains: Augmentation does not create entirely new samples, so underrepresented categories may still lead to biased learning.
A real-world application of image data augmentation
An interesting application of image data augmentation is in self-driving cars, where split-second decisions made by computer vision models like YOLO11 are crucial. The model has to be able to detect roads, people, and other objects accurately.

However, the real-world conditions that a self-driving vehicle encounters can be unpredictable. Bad weather, motion blur, and hidden signs can make Vision AI solutions in this sector complex. Training computer vision models with just real-world images is often not enough. Image datasets for the models in self-driving cars need to be diverse so that the model can learn to handle unexpected situations.

Image data augmentation solves this by simulating fog, adjusting brightness, and distorting shapes. These changes help models recognize objects in different conditions. As a result, models become smarter and more reliable. 

With augmented training, Vision AI solutions in self-driving cars adapt better and make safer decisions. More accurate results mean fewer accidents and improved navigation. 

__wf_reserved_inherit
Fig 5. An example of image data augmentation with respect to self-driving cars.
‚Äç

Self-driving cars are just one example. In fact, image data augmentation is crucial in a wide range of sectors, from medical imaging to retail analytics. Any application that relies on computer vision can potentially benefit from image data augmentation.

Key takeaways
Vision AI systems need to be able to recognize objects in different conditions, but collecting endless real-world images for training can be difficult. Image data augmentation solves this by creating variations of existing images, helping models learn faster and perform better in real-world situations. It improves accuracy, ensuring Vision AI models like YOLO11 can handle different lighting, angles, and environments.

For businesses and developers, image data augmentation saves time and effort while making computer vision models more reliable. From healthcare to self-driving cars, many industries depend on it. As Vision AI keeps evolving, augmentation will continue to be an essential part of building smarter and more adaptable models for the future.

Join our community and visit our GitHub repository to see AI in action. Explore our licensing options and discover more about AI in agriculture and computer vision in manufacturing on our solutions pages.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Guides

Exploring ensemble learning and its role in AI and ML
5 min read

Oct 9, 2025
Browse all articles
Let‚Äôs build the future
of AI together!
Begin your journey with the future of machine learning

Start for free
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Transcript


Search in video
0:00
[MUSIC PLAYING]
0:02
0:05
SPEAKER: Imagine you're a visitor from a faraway galaxy.
0:09
You're excited to go sightseeing and make new friends.
0:13
But you don't speak the language and your translator app
0:16
isn't much help.
0:17
How are you going to communicate, you wonder?
0:20
Then you have an idea.
0:22
You find a bustling local community
0:26
and listen for a few months so you can learn the language
0:30
from native speakers.
0:31
0:34
After a few months of language immersion,
0:36
you finally feel ready to communicate.
0:38
0:41
Uh oh.
0:43
What went wrong?
0:45
Clearly, your language skills are not as good
0:47
as you thought they were.
0:48
They didn't translate to the real world.
0:52
Machine learning models can have the same problem.
0:55
When you train a model on data, you're
0:57
teaching it to perform well on that specific set of data.
1:00
But this training data is just a small subset of real world data.
1:05
If the model makes high quality predictions on the training data
1:08
but much lower quality predictions on brand new data,
1:12
then the model didn't generalize.
1:15
In this case, we say that the model
1:17
has overfit the training data.
1:19
It has been tuned so closely to the specific patterns
1:22
of the data it's already seen that it can't identify patterns
1:26
in new data.
1:28
Realizing your mistake, overfitting
1:31
to the speech patterns of only a small group of people,
1:33
you begin your language studies again,
1:36
but this time you broaden your approach,
1:39
listening to a wide variety of speakers.
1:43
Your communication skills now successfully generalize
1:47
to brand new contexts.
1:48
1:51
And you land an exciting new job sharing your knowledge
1:54
with others.
1:56
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
Train




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.217
47.5k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions üöÄ
Guides
Integrations
HUB
Reference
Help
Tasks
Detect
Segment
Classify
Pose
OBB
Modes
Train
Val
Predict
Export
Track
Benchmark
Table of contents
Introduction
Why Choose Ultralytics YOLO for Training?
Key Features of Train Mode
Usage Examples
Multi-GPU Training
Idle GPU Training
Apple Silicon MPS Training
Resuming Interrupted Trainings
Train Settings
Augmentation Settings and Hyperparameters
Logging
Comet
ClearML
TensorBoard
FAQ
How do I train an object detection model using Ultralytics YOLO11?
What are the key features of Ultralytics YOLO11's Train mode?
How do I resume training from an interrupted session in Ultralytics YOLO11?
Can I train YOLO11 models on Apple silicon chips?
What are the common training settings, and how do I configure them?
Model Training with Ultralytics YOLO
Ultralytics YOLO ecosystem and integrations

Introduction
Training a deep learning model involves feeding it data and adjusting its parameters so that it can make accurate predictions. Train mode in Ultralytics YOLO11 is engineered for effective and efficient training of object detection models, fully utilizing modern hardware capabilities. This guide aims to cover all the details you need to get started with training your own models using YOLO11's robust set of features.



Watch: How to Train a YOLO model on Your Custom Dataset in Google Colab.

Why Choose Ultralytics YOLO for Training?
Here are some compelling reasons to opt for YOLO11's Train mode:

Efficiency: Make the most out of your hardware, whether you're on a single-GPU setup or scaling across multiple GPUs.
Versatility: Train on custom datasets in addition to readily available ones like COCO, VOC, and ImageNet.
User-Friendly: Simple yet powerful CLI and Python interfaces for a straightforward training experience.
Hyperparameter Flexibility: A broad range of customizable hyperparameters to fine-tune model performance.
Key Features of Train Mode
The following are some notable features of YOLO11's Train mode:

Automatic Dataset Download: Standard datasets like COCO, VOC, and ImageNet are downloaded automatically on first use.
Multi-GPU Support: Scale your training efforts seamlessly across multiple GPUs to expedite the process.
Hyperparameter Configuration: The option to modify hyperparameters through YAML configuration files or CLI arguments.
Visualization and Monitoring: Real-time tracking of training metrics and visualization of the learning process for better insights.
Tip

YOLO11 datasets like COCO, VOC, ImageNet and many others automatically download on first use, i.e. yolo train data=coco.yaml
Usage Examples
Train YOLO11n on the COCO8 dataset for 100 epochs at image size 640. The training device can be specified using the device argument. If no argument is passed GPU device=0 will be used if available, otherwise device='cpu' will be used. See Arguments section below for a full list of training arguments.

Windows Multi-Processing Error

On Windows, you may receive a RuntimeError when launching the training as a script. Add a if __name__ == "__main__": block before your training code to resolve it.

Single-GPU and CPU Training Example

Device is determined automatically. If a GPU is available then it will be used (default CUDA device 0), otherwise training will start on CPU.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.yaml")  # build a new model from YAML
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)
model = YOLO("yolo11n.yaml").load("yolo11n.pt")  # build from YAML and transfer weights

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

Multi-GPU Training
Multi-GPU training allows for more efficient utilization of available hardware resources by distributing the training load across multiple GPUs. This feature is available through both the Python API and the command-line interface. To enable multi-GPU training, specify the GPU device IDs you wish to use.

Multi-GPU Training Example

To train with 2 GPUs, CUDA devices 0 and 1 use the following commands. Expand to additional GPUs as required.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model with 2 GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[0, 1])

# Train the model with the two most idle GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[-1, -1])

Idle GPU Training
Idle GPU Training enables automatic selection of the least utilized GPUs in multi-GPU systems, optimizing resource usage without manual GPU selection. This feature identifies available GPUs based on utilization metrics and VRAM availability.

Idle GPU Training Example

To automatically select and use the most idle GPU(s) for training, use the -1 device parameter. This is particularly useful in shared computing environments or servers with multiple users.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolov8n.pt")  # load a pretrained model (recommended for training)

# Train using the single most idle GPU
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=-1)

# Train using the two most idle GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[-1, -1])

The auto-selection algorithm prioritizes GPUs with:

Lower current utilization percentages
Higher available memory (free VRAM)
Lower temperature and power consumption
This feature is especially valuable in shared computing environments or when running multiple training jobs across different models. It automatically adapts to changing system conditions, ensuring optimal resource allocation without manual intervention.

Apple Silicon MPS Training
With the support for Apple silicon chips integrated in the Ultralytics YOLO models, it's now possible to train your models on devices utilizing the powerful Metal Performance Shaders (MPS) framework. The MPS offers a high-performance way of executing computation and image processing tasks on Apple's custom silicon.

To enable training on Apple silicon chips, you should specify 'mps' as your device when initiating the training process. Below is an example of how you could do this in Python and via the command line:

MPS Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model with MPS
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device="mps")

While leveraging the computational power of the Apple silicon chips, this enables more efficient processing of the training tasks. For more detailed guidance and advanced configuration options, please refer to the PyTorch MPS documentation.

Resuming Interrupted Trainings
Resuming training from a previously saved state is a crucial feature when working with deep learning models. This can come in handy in various scenarios, like when the training process has been unexpectedly interrupted, or when you wish to continue training a model with new data or for more epochs.

When training is resumed, Ultralytics YOLO loads the weights from the last saved model and also restores the optimizer state, learning rate scheduler, and the epoch number. This allows you to continue the training process seamlessly from where it was left off.

You can easily resume training in Ultralytics YOLO by setting the resume argument to True when calling the train method, and specifying the path to the .pt file containing the partially trained model weights.

Below is an example of how to resume an interrupted training using Python and via the command line:

Resume Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("path/to/last.pt")  # load a partially trained model

# Resume training
results = model.train(resume=True)

By setting resume=True, the train function will continue training from where it left off, using the state stored in the 'path/to/last.pt' file. If the resume argument is omitted or set to False, the train function will start a new training session.

Remember that checkpoints are saved at the end of every epoch by default, or at fixed intervals using the save_period argument, so you must complete at least 1 epoch to resume a training run.

Train Settings
The training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model's performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.

Argument  Type  Default Description
model str None  Specifies the model file for training. Accepts a path to either a .pt pretrained model or a .yaml configuration file. Essential for defining the model structure or initializing weights.
data  str None  Path to the dataset configuration file (e.g., coco8.yaml). This file contains dataset-specific parameters, including paths to training and validation data, class names, and number of classes.
epochs  int 100 Total number of training epochs. Each epoch represents a full pass over the entire dataset. Adjusting this value can affect training duration and model performance.
time  float None  Maximum training time in hours. If set, this overrides the epochs argument, allowing training to automatically stop after the specified duration. Useful for time-constrained training scenarios.
patience  int 100 Number of epochs to wait without improvement in validation metrics before early stopping the training. Helps prevent overfitting by stopping training when performance plateaus.
batch int or float  16  Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).
imgsz int 640 Target image size for training. Images are resized to squares with sides equal to the specified value (if rect=False), preserving aspect ratio for YOLO models but not RTDETR. Affects model accuracy and computational complexity.
save  bool  True  Enables saving of training checkpoints and final model weights. Useful for resuming training or model deployment.
save_period int -1  Frequency of saving model checkpoints, specified in epochs. A value of -1 disables this feature. Useful for saving interim models during long training sessions.
cache bool  False Enables caching of dataset images in memory (True/ram), on disk (disk), or disables it (False). Improves training speed by reducing disk I/O at the cost of increased memory usage.
device  int or str or list  None  Specifies the computational device(s) for training: a single GPU (device=0), multiple GPUs (device=[0,1]), CPU (device=cpu), MPS for Apple silicon (device=mps), or auto-selection of most idle GPU (device=-1) or multiple idle GPUs (device=[-1,-1])
workers int 8 Number of worker threads for data loading (per RANK if Multi-GPU training). Influences the speed of data preprocessing and feeding into the model, especially useful in multi-GPU setups.
project str None  Name of the project directory where training outputs are saved. Allows for organized storage of different experiments.
name  str None  Name of the training run. Used for creating a subdirectory within the project folder, where training logs and outputs are stored.
exist_ok  bool  False If True, allows overwriting of an existing project/name directory. Useful for iterative experimentation without needing to manually clear previous outputs.
pretrained  bool or str True  Determines whether to start training from a pretrained model. Can be a boolean value or a string path to a specific model from which to load weights. Enhances training efficiency and model performance.
optimizer str 'auto'  Choice of optimizer for training. Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for automatic selection based on model configuration. Affects convergence speed and stability.
seed  int 0 Sets the random seed for training, ensuring reproducibility of results across runs with the same configurations.
deterministic bool  True  Forces deterministic algorithm use, ensuring reproducibility but may affect performance and speed due to the restriction on non-deterministic algorithms.
single_cls  bool  False Treats all classes in multi-class datasets as a single class during training. Useful for binary classification tasks or when focusing on object presence rather than classification.
classes list[int] None  Specifies a list of class IDs to train on. Useful for filtering out and focusing only on certain classes during training.
rect  bool  False Enables minimum padding strategy‚Äîimages in a batch are minimally padded to reach a common size, with the longest side equal to imgsz. Can improve efficiency and speed but may affect model accuracy.
multi_scale bool  False Enables multi-scale training by increasing/decreasing imgsz by up to a factor of 0.5 during training. Trains the model to be more accurate with multiple imgsz during inference.
cos_lr  bool  False Utilizes a cosine learning rate scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.
close_mosaic  int 10  Disables mosaic data augmentation in the last N epochs to stabilize training before completion. Setting to 0 disables this feature.
resume  bool  False Resumes training from the last saved checkpoint. Automatically loads model weights, optimizer state, and epoch count, continuing training seamlessly.
amp bool  True  Enables Automatic Mixed Precision (AMP) training, reducing memory usage and possibly speeding up training with minimal impact on accuracy.
fraction  float 1.0 Specifies the fraction of the dataset to use for training. Allows for training on a subset of the full dataset, useful for experiments or when resources are limited.
profile bool  False Enables profiling of ONNX and TensorRT speeds during training, useful for optimizing model deployment.
freeze  int or list None  Freezes the first N layers of the model or specified layers by index, reducing the number of trainable parameters. Useful for fine-tuning or transfer learning.
lr0 float 0.01  Initial learning rate (i.e. SGD=1E-2, Adam=1E-3). Adjusting this value is crucial for the optimization process, influencing how rapidly model weights are updated.
lrf float 0.01  Final learning rate as a fraction of the initial rate = (lr0 * lrf), used in conjunction with schedulers to adjust the learning rate over time.
momentum  float 0.937 Momentum factor for SGD or beta1 for Adam optimizers, influencing the incorporation of past gradients in the current update.
weight_decay  float 0.0005  L2 regularization term, penalizing large weights to prevent overfitting.
warmup_epochs float 3.0 Number of epochs for learning rate warmup, gradually increasing the learning rate from a low value to the initial learning rate to stabilize training early on.
warmup_momentum float 0.8 Initial momentum for warmup phase, gradually adjusting to the set momentum over the warmup period.
warmup_bias_lr  float 0.1 Learning rate for bias parameters during the warmup phase, helping stabilize model training in the initial epochs.
box float 7.5 Weight of the box loss component in the loss function, influencing how much emphasis is placed on accurately predicting bounding box coordinates.
cls float 0.5 Weight of the classification loss in the total loss function, affecting the importance of correct class prediction relative to other components.
dfl float 1.5 Weight of the distribution focal loss, used in certain YOLO versions for fine-grained classification.
pose  float 12.0  Weight of the pose loss in models trained for pose estimation, influencing the emphasis on accurately predicting pose keypoints.
kobj  float 2.0 Weight of the keypoint objectness loss in pose estimation models, balancing detection confidence with pose accuracy.
nbs int 64  Nominal batch size for normalization of loss.
overlap_mask  bool  True  Determines whether object masks should be merged into a single mask for training, or kept separate for each object. In case of overlap, the smaller mask is overlaid on top of the larger mask during merge.
mask_ratio  int 4 Downsample ratio for segmentation masks, affecting the resolution of masks used during training.
dropout float 0.0 Dropout rate for regularization in classification tasks, preventing overfitting by randomly omitting units during training.
val bool  True  Enables validation during training, allowing for periodic evaluation of model performance on a separate dataset.
plots bool  False Generates and saves plots of training and validation metrics, as well as prediction examples, providing visual insights into model performance and learning progression.
compile bool or str False Enables PyTorch 2.x torch.compile graph compilation with backend='inductor'. Accepts True ‚Üí "default", False ‚Üí disables, or a string mode such as "default", "reduce-overhead", "max-autotune-no-cudagraphs". Falls back to eager with a warning if unsupported.
Note on Batch-size Settings

The batch argument can be configured in three ways:

Fixed Batch Size: Set an integer value (e.g., batch=16), specifying the number of images per batch directly.
Auto Mode (60% GPU Memory): Use batch=-1 to automatically adjust batch size for approximately 60% CUDA memory utilization.
Auto Mode with Utilization Fraction: Set a fraction value (e.g., batch=0.70) to adjust batch size based on the specified fraction of GPU memory usage.
Augmentation Settings and Hyperparameters
Augmentation techniques are essential for improving the robustness and performance of YOLO models by introducing variability into the training data, helping the model generalize better to unseen data. The following table outlines the purpose and effect of each augmentation argument:

Argument  Type  Default Supported Tasks Range Description
hsv_h float 0.015 detect, segment, pose, obb, classify  0.0 - 1.0 Adjusts the hue of the image by a fraction of the color wheel, introducing color variability. Helps the model generalize across different lighting conditions.
hsv_s float 0.7 detect, segment, pose, obb, classify  0.0 - 1.0 Alters the saturation of the image by a fraction, affecting the intensity of colors. Useful for simulating different environmental conditions.
hsv_v float 0.4 detect, segment, pose, obb, classify  0.0 - 1.0 Modifies the value (brightness) of the image by a fraction, helping the model to perform well under various lighting conditions.
degrees float 0.0 detect, segment, pose, obb  0.0 - 180 Rotates the image randomly within the specified degree range, improving the model's ability to recognize objects at various orientations.
translate float 0.1 detect, segment, pose, obb  0.0 - 1.0 Translates the image horizontally and vertically by a fraction of the image size, aiding in learning to detect partially visible objects.
scale float 0.5 detect, segment, pose, obb, classify  >=0.0 Scales the image by a gain factor, simulating objects at different distances from the camera.
shear float 0.0 detect, segment, pose, obb  -180 - +180 Shears the image by a specified degree, mimicking the effect of objects being viewed from different angles.
perspective float 0.0 detect, segment, pose, obb  0.0 - 0.001 Applies a random perspective transformation to the image, enhancing the model's ability to understand objects in 3D space.
flipud  float 0.0 detect, segment, pose, obb, classify  0.0 - 1.0 Flips the image upside down with the specified probability, increasing the data variability without affecting the object's characteristics.
fliplr  float 0.5 detect, segment, pose, obb, classify  0.0 - 1.0 Flips the image left to right with the specified probability, useful for learning symmetrical objects and increasing dataset diversity.
bgr float 0.0 detect, segment, pose, obb  0.0 - 1.0 Flips the image channels from RGB to BGR with the specified probability, useful for increasing robustness to incorrect channel ordering.
mosaic  float 1.0 detect, segment, pose, obb  0.0 - 1.0 Combines four training images into one, simulating different scene compositions and object interactions. Highly effective for complex scene understanding.
mixup float 0.0 detect, segment, pose, obb  0.0 - 1.0 Blends two images and their labels, creating a composite image. Enhances the model's ability to generalize by introducing label noise and visual variability.
cutmix  float 0.0 detect, segment, pose, obb  0.0 - 1.0 Combines portions of two images, creating a partial blend while maintaining distinct regions. Enhances model robustness by creating occlusion scenarios.
copy_paste  float 0.0 segment 0.0 - 1.0 Copies and pastes objects across images to increase object instances.
copy_paste_mode str flip  segment - Specifies the copy-paste strategy to use. Options include 'flip' and 'mixup'.
auto_augment  str randaugment classify  - Applies a predefined augmentation policy ('randaugment', 'autoaugment', or 'augmix') to enhance model performance through visual diversity.
erasing float 0.4 classify  0.0 - 0.9 Randomly erases regions of the image during training to encourage the model to focus on less obvious features.
These settings can be adjusted to meet the specific requirements of the dataset and task at hand. Experimenting with different values can help find the optimal augmentation strategy that leads to the best model performance.

Info

For more information about training augmentation operations, see the reference section.

Logging
In training a YOLO11 model, you might find it valuable to keep track of the model's performance over time. This is where logging comes into play. Ultralytics YOLO provides support for three types of loggers - Comet, ClearML, and TensorBoard.

To use a logger, select it from the dropdown menu in the code snippet above and run it. The chosen logger will be installed and initialized.

Comet
Comet is a platform that allows data scientists and developers to track, compare, explain and optimize experiments and models. It provides functionalities such as real-time metrics, code diffs, and hyperparameters tracking.

To use Comet:

Example


Python

# pip install comet_ml
import comet_ml

comet_ml.init()

Remember to sign in to your Comet account on their website and get your API key. You will need to add this to your environment variables or your script to log your experiments.

ClearML
ClearML is an open-source platform that automates tracking of experiments and helps with efficient sharing of resources. It is designed to help teams manage, execute, and reproduce their ML work more efficiently.

To use ClearML:

Example


Python

# pip install clearml
import clearml

clearml.browser_login()

After running this script, you will need to sign in to your ClearML account on the browser and authenticate your session.

TensorBoard
TensorBoard is a visualization toolkit for TensorFlow. It allows you to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.

To use TensorBoard in Google Colab:

Example


CLI

load_ext tensorboard
tensorboard --logdir ultralytics/runs # replace with 'runs' directory

To use TensorBoard locally run the below command and view results at http://localhost:6006/.

Example


CLI

tensorboard --logdir ultralytics/runs # replace with 'runs' directory

This will load TensorBoard and direct it to the directory where your training logs are saved.

After setting up your logger, you can then proceed with your model training. All training metrics will be automatically logged in your chosen platform, and you can access these logs to monitor your model's performance over time, compare different models, and identify areas for improvement.

FAQ
How do I train an object detection model using Ultralytics YOLO11?
To train an object detection model using Ultralytics YOLO11, you can either use the Python API or the CLI. Below is an example for both:

Single-GPU and CPU Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

For more details, refer to the Train Settings section.

What are the key features of Ultralytics YOLO11's Train mode?
The key features of Ultralytics YOLO11's Train mode include:

Automatic Dataset Download: Automatically downloads standard datasets like COCO, VOC, and ImageNet.
Multi-GPU Support: Scale training across multiple GPUs for faster processing.
Hyperparameter Configuration: Customize hyperparameters through YAML files or CLI arguments.
Visualization and Monitoring: Real-time tracking of training metrics for better insights.
These features make training efficient and customizable to your needs. For more details, see the Key Features of Train Mode section.

How do I resume training from an interrupted session in Ultralytics YOLO11?
To resume training from an interrupted session, set the resume argument to True and specify the path to the last saved checkpoint.

Resume Training Example


Python
CLI

from ultralytics import YOLO

# Load the partially trained model
model = YOLO("path/to/last.pt")

# Resume training
results = model.train(resume=True)

Check the section on Resuming Interrupted Trainings for more information.

Can I train YOLO11 models on Apple silicon chips?
Yes, Ultralytics YOLO11 supports training on Apple silicon chips utilizing the Metal Performance Shaders (MPS) framework. Specify 'mps' as your training device.

MPS Training Example


Python
CLI

from ultralytics import YOLO

# Load a pretrained model
model = YOLO("yolo11n.pt")

# Train the model on Apple silicon chip (M1/M2/M3/M4)
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device="mps")

For more details, refer to the Apple Silicon MPS Training section.

What are the common training settings, and how do I configure them?
Ultralytics YOLO11 allows you to configure a variety of training settings such as batch size, learning rate, epochs, and more through arguments. Here's a brief overview:

Argument  Default Description
model None  Path to the model file for training.
data  None  Path to the dataset configuration file (e.g., coco8.yaml).
epochs  100 Total number of training epochs.
batch 16  Batch size, adjustable as integer or auto mode.
imgsz 640 Target image size for training.
device  None  Computational device(s) for training like cpu, 0, 0,1, or mps.
save  True  Enables saving of training checkpoints and final model weights.
For an in-depth guide on training settings, check the Train Settings section.



üìÖ
Created 1 year ago
‚úèÔ∏è
Updated 3 months ago
glenn-jocher
Laughing-q
UltralyticsAssistant
MatthewNoyce
Y-T-G
JairajJangle
jk4e
RizwanMunawar
dependabot
fcakyon
Burhan-Q

Tweet

Share

Comments
 Back to top
Previous
Ultralytics YOLO11 Modes
Next
Val
¬© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How Data Mining Works
Real-World AI and Computer Vision Applications
Data Mining vs. Related Concepts
Glossary
Data Mining
Discover how data mining transforms raw data into actionable insights, powering AI, ML, and real-world applications in healthcare, retail, and more!

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Data mining is the process of discovering patterns, correlations, and anomalies within large datasets to extract valuable and previously unknown information. It acts as a crucial exploratory step that transforms raw data into a comprehensible structure, often serving as the foundation for predictive modeling and Machine Learning (ML) tasks. By leveraging techniques from statistics, database systems, and AI, data mining helps uncover hidden insights that can inform business strategies, scientific research, and technological innovation.

How Data Mining Works
The data mining process is often structured according to frameworks like the Cross-Industry Standard Process for Data Mining (CRISP-DM). The typical stages include:

Data Collection and Integration: Gathering data from various sources, which may include structured databases, unstructured text, or images from a Data Lake.
Data Preprocessing: This involves data cleaning to handle missing or inconsistent values and data transformation to normalize or aggregate the data for analysis. Data augmentation can also be used at this stage to enrich the dataset.
Pattern Discovery and Modeling: Applying algorithms to identify patterns. Common tasks include classification, clustering (K-Means), regression, and association rule mining. This is the stage where ML algorithms are most heavily used.
Evaluation and Interpretation: Assessing the discovered patterns for their validity and usefulness. Data visualization is a key tool here, helping to make the findings understandable.
Knowledge Deployment: Integrating the discovered knowledge into operational systems, such as a recommendation engine or a fraud detection system.
Real-World AI and Computer Vision Applications
Data mining is fundamental to developing intelligent systems across many industries.

AI in Retail and Market Basket Analysis: Retailers mine vast transaction logs to discover which products are frequently purchased together. For instance, finding that customers who buy bread also often buy milk (an association rule) can inform product placement strategies, promotional bundling, and targeted advertising. This analysis of customer behavior also fuels personalized recommendation systems. Learn more about how AI is achieving retail efficiency.
Medical Image Analysis: In AI in healthcare, data mining techniques are applied to large-scale medical records and image datasets, such as the Brain Tumor dataset. By mining this data, researchers can identify patterns and correlations that link certain image features or patient demographics to diseases. This helps in building diagnostic models, like those for tumor detection, and supports organizations like the National Institutes of Health (NIH) in advancing medical science.
Data Mining vs. Related Concepts
It's important to distinguish data mining from other related data science terms.

Machine Learning (ML): While the terms are often used interchangeably, they are distinct. Data mining is a broader process of knowledge discovery from data. Machine learning is a collection of techniques and algorithms (e.g., supervised learning, unsupervised learning) that are often used within the data mining process to find patterns. In essence, ML is a tool to achieve data mining's goal.
Data Analytics: Data analytics is a wider field focused on examining datasets to draw conclusions and support decision-making. Data mining is a specific subset of data analytics that emphasizes discovering previously unknown patterns, whereas data analytics can also involve testing predefined hypotheses and creating summary reports.
Big Data: This term refers to the vast, complex, and rapidly growing datasets themselves. Data mining is the process applied to Big Data to extract value from it. The challenges of Big Data (volume, velocity, variety) often require specialized data mining tools like the Apache Hadoop ecosystem.
Deep Learning (DL): This is a specialized subfield of machine learning that uses neural networks with many layers. DL models, like the ones used in Ultralytics YOLO, can automatically perform feature extraction from raw data like images, which is a powerful capability within a data mining workflow for Computer Vision (CV). Platforms like Ultralytics HUB streamline the entire process, from managing datasets to training models.
Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Close button
Register now for
Ultralytics YOLO11
Shenzhen
Arrow right
Ultralytics logo
Products
Licensing
GitHub
117K+
Get in touch
English flagEN
‰∏≠Êñá (ÁÆÄ‰Ωì) flagZH
ÌïúÍµ≠Ïñ¥ flagKO
Êó•Êú¨Ë™û flagJA
–†—É—Å—Å–∫–∏–π flagRU
Deutsch flagDE
Fran√ßais flagFR
Espa√±ol flagES
Portugu√™s flagPT
Italiano flagIT
T√ºrk√ße flagTR
Ti·∫øng Vi·ªát flagVI
ÿßŸÑÿπÿ±ÿ®Ÿäÿ©‚Äè flagAR
Cookie Settings
By clicking ‚ÄúAccept All Cookies‚Äù, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage and assist in our marketing efforts. More info
Accept All Cookies
Cookie Settings
Back to Ultralytics Glossary
How Predictive Modeling Works
Real-World Applications
Predictive Modeling vs. Related Concepts
Tools and Platforms
Glossary
Predictive Modeling
Discover how predictive modeling leverages machine learning to forecast outcomes, optimize decisions, and drive insights across diverse industries.

Train Ultralytics YOLO models to streamline workflows across industries
Get started
Train YOLO models with Ultralytics HUB
Predictive modeling is a statistical and machine learning technique that uses historical and current data to forecast future outcomes. By identifying patterns and relationships within large datasets, these models generate predictions about unknown events. The core idea is to go beyond simply analyzing past events and instead create a practical, forward-looking forecast. This process is central to making data-driven decisions in business, science, and technology, enabling organizations to anticipate trends and behaviors proactively.

How Predictive Modeling Works
The development of a predictive model follows a structured process that transforms raw data into actionable forecasts. This workflow typically includes several key stages:

Data Collection and Preparation: The process begins with gathering relevant historical data, which serves as the training data. This is followed by a crucial data preprocessing step, where the data is cleaned, formatted, and enriched to ensure quality and consistency.
Model Selection and Training: A suitable algorithm is chosen based on the problem. Common types include classification models, which predict a category (e.g., yes/no), and regression models, which predict a numerical value. The model is then trained on the prepared dataset.
Evaluation and Tuning: The model's performance and accuracy are assessed using validation data. This often involves hyperparameter tuning to optimize its predictive power.
Deployment and Monitoring: Once validated, the model is put into production through model deployment to make real-world predictions. Continuous model monitoring is essential to ensure it remains effective over time.
Real-World Applications
Predictive modeling is applied across many industries to solve complex problems.

Predictive Maintenance in Manufacturing: In AI for manufacturing, models analyze sensor data from machinery to predict when a component is likely to fail. For instance, a model can learn the vibration patterns that precede a breakdown. This allows for proactive maintenance, significantly reducing unplanned downtime and costs. You can learn more about predictive maintenance strategies from Deloitte. Computer vision systems, powered by models like Ultralytics YOLO, can enhance this by visually inspecting equipment for defects, providing critical data to the predictive system.
Demand Forecasting in Retail: Retail companies use predictive models to forecast product demand by analyzing historical sales data, seasonality, and promotional events. This is a core component of AI in Retail that helps optimize inventory management, ensuring popular items are in stock while avoiding overstocking less popular ones.
Predictive Modeling vs. Related Concepts
It is helpful to distinguish predictive modeling from closely related terms:

Machine Learning (ML): Predictive modeling is a primary application of ML. Machine Learning provides the algorithms and methods used to build the models, while predictive modeling is the specific practice of using those models to forecast future events.
Statistical AI: This is the theoretical foundation upon which many predictive models are built. Classical methods from Statistical AI, such as linear and logistic regression, are direct forms of predictive modeling.
Anomaly Detection: While predictive modeling forecasts a likely future outcome, anomaly detection focuses on identifying unusual data points that deviate from established patterns. For example, an anomaly detection system might flag an irregular heartbeat, while a predictive model might forecast the long-term risk of heart disease based on patient history.
Tools and Platforms
Developing and deploying predictive models often involves using specialized software libraries and platforms. Popular machine learning libraries like Scikit-learn, and deep learning frameworks such as PyTorch and TensorFlow, provide the building blocks for many predictive models. Platforms like Kaggle offer datasets and environments for experimentation. For managing the end-to-end lifecycle, platforms like Ultralytics HUB provide tools to train, manage datasets, track experiments, and deploy models. Resources like Machine Learning Mastery and Towards Data Science offer further learning opportunities.

Read more in this category
Guides

From bits to qubits: How quantum optimization is reshaping AI
4 min read

Oct 17, 2025
Guides

A quick guide for beginners on how to train an AI model
5 min read

Oct 16, 2025
Events

From Dubai with insights: Key takeaways from the GDG MENA-T Summit 2025
4 min read

Oct 10, 2025
Browse all articles
Join the Ultralytics community
Join the future of AI. Connect, collaborate, and grow with global innovators

Join now
Stay ahead of the game

Get the latest news and insights on AI and machine learning ‚Äî our monthly newsletter has it all!

 Drop your email
Ultralytics logo
Locations
5001 Judicial Way
Frederick, MD 21703, USA
8 Devonshire Square
EC2M 4YJ London, UK
Cra de S. Jer√≥nimo, 15 Centro, 28014 Madrid,
Spain
5F, CR Land Tower D, 91 Kefa Road, Nanshan District, Shenzhen, China
Connect
Products
Ultralytics YOLO
Ultralytics YOLOv5
Ultralytics YOLOv8
Ultralytics YOLO11
Solutions
Ultralytics solutions
AI in agriculture
AI in automotive
AI in healthcare
AI in logistics
AI in manufacturing
AI in retail
AI in robotics
Resources
Learn
Docs
Blogs
Glossary
Changelog
Community
GitHub
Forum
Discord
Reddit
Events
YOLO Vision
Ultralytics Live Sessions
Conferences
Webinars
Podcasts
Insights
Customers
Survey
News
Licensing
Licensing
Company
About us
About Ultralytics
Ultralytics Handbook
Brand guidelines
Careers
Ultralytics shop
Partners
Ultralytics integrations
Contribute to OSS
Contact
Contact us
hello@ultralytics.com
Terms of Service
Privacy Policy
Ultralytics Enterprise Software License Terms
¬© 2025 Ultralytics Inc. All rights reserved.

Ask AI
Chat avatar
Transcript


Search in video
Introduction
0:00
so in this video here we're going to see how we  can train a custom object detection model with  
0:03
YOLOv8 from Ultralytics we're going to see how  we can set it up we're also going to see how you  
0:08
can label your data with a specific tool you can  lose use like whatever tools that you want how can  
0:13
we act like export it into the YOLOv8 format  import into a Google Colab notebook and then  
0:17
we're going to train our own custom optic texting  model with YOLOv8 so let's now jump straight into  
0:22
Google Colab notebook let's see how we can set  it up it will be down in description here under  
0:26
the video so you can use it on your own custom  data set so first of all here we're basically  
0:31
just going to set up our Nvidia SMI just to see  what GPU we're running on if you want to use the  
0:36
GPU make sure that you go up to runtime change  random type and make sure that right like have  
0:41
chosen a dpu as the hardware accelerator so  yeah we have done that now we should be able  
0:45
to call this Nvidia is a my command here we go  we can see the information about our GPU right  
0:50
now this is just like the standard GPU in a Google  Colab notebook again you can use the free version  
0:55
as well and you can run this fine and train  your own custom YOLOv8 model from Ultralytics  
1:01
so now we're just going to pip install Ultralytics  here so now we can see that we've installed  
1:05
Ultralytics in our environment now we can go down  and install YOLOv8 or basically just set that up  
1:10
so from Ultralytics we're going to import the YOLO  model there we go now we have imported the YOLO  
1:15
model from Ultralytics now we basically just need  our data set so I'm using like roboflow in this  
Ô∏è Setting Up Environment
1:20
video to go in and label my data set you can use  whatever tool that you want I have the data set  
1:24
here let me just go in shortly to show you guys  that so I just have a data set with a number of  
1:29
different cups then I've just labeled it here with  this annotation tool I'm basically just drawing  
1:34
boundary boxes around it so you can just drag the  tool draw a bounding box around the object and  
1:38
then choose whatever class that you want to like  have your object in after we've done that we can  
1:44
then go in and Export our data set so I have some  versions already generated we hit export data set  
1:49
and then we can select the format we're going to  go with the YOLOv8 format from Ultralytics then  
1:54
we can hit show downloadable code so we just get a  code snippet and we can just copy paste that into  
2:00
our Google colab notebook so I've already copy  pasted it here you first of all you just need  
2:04
to copy paste in your API key then when we run  this we act like just download our data set and  
2:09
then we'll have our data set in our Google colab  environment and then we can just directly train  
2:14
our yellow V8 model on that so now we can see that  our data set has been downloaded we should be able  
2:19
to go over into our folder we can see we have  this cup detection version two free we can see  
2:24
we get a test set validation set and also a train  set so yeah we just have some test images we also  
2:29
have the labels here so this will basically like  be the pounding boxes we have the label and then  
2:34
we also basically have the top left corner and the  bottom right corner so this is the YOLOv8 format  
2:38
from Ultralytics if you go down to the data here  I just want to make sure that we actually have  
2:42
the correct path because we're using this in a  Google colab environment and we actually need to  
2:47
directory for our image just so I'm doing to copy  paste that there we go and we also have our train
2:56
you probably also need to do this here because  we're using a Google home app environment you  
3:00
can also have your data set in a Google Drive  and then just connect to your Google Drive then  
3:04
we basically have the path to our test set train  set and also our validation set we're just going  
3:08
to save it and now we should be able to run it  directly in our Google collab notebook so now we  
3:13
have this yellow command we're going to set the  task to detect for optic detection then we also  
3:18
have this mode we set that equal to train because  we want to train our own custom YOLOv8 model now  
3:23
for update detection we can choose which of the  yellow models that we want to use in this example  
3:28
I'm going to go with the media model and now we  also just need to specify the location to our data  
3:32
set so that will be the path to our data dot yaml  file that I just showed you and then we also need  
3:38
to specify the number of epochs and image size  of the images that we actually train our model on  
3:44
so now we can just directly go in and train our  YOLOv8 model so now we can see the training has  
Ô∏è Labeling Dataset
3:48
started for a custom YOLOv8 model they're just  going pretty fast because we're using the GPU here  
3:52
in Google collabs now we can go into the metrics  that we get while our model is training just to  
3:57
verify that we actually learn whatever you want to  I usually look at the mean error position so that  
4:02
is probably like the most important parameter we  both have the mean air position of 0.50 and also  
4:07
0.50 to 0.95 in intervals of 0.05 and those should  act like be increasing over your number of epochs  
4:15
and ideally they should be close to one okay so a  custom uv8 model is now done training we can see  
4:20
that we end up at our mean out position of 0.50 at  around like 0.81 82 and we also have like a meter  
4:27
position of 0.50 to 95 of around 0.5 you can just  directly go in and download the weights from here  
4:34
so in the next video I'm going to show you how we  can export these weights and basically just create  
4:38
another instance in our own custom python script  with a yellow V8 model and then use the custom  
4:44
trained one instead of a pre-trained one and also  how we can run live inference with our own custom  
4:49
trained yellow V8 model so here we're basically  just going to plot the confusion Matrix we can  
4:54
see that it acts like does a pretty good job with  our detection so ideally we should have all the  
4:59
values here in the diagonal in our confusion  Matrix let's take a look at the train results  
5:04
so these are all the graphs from our training  results for the metrics so we can see that the  
5:07
mean air position 0.50 is increasing now we can  go ahead and take a look at the validation we can  
5:12
choose the best model that we have at like  train and we specify the data set location  
5:16
for our validation set and then it will just do  validation on images that the model hasn't trained  
5:22
on before still look fine on our validation  set even though that these images have never  
5:26
been seen by the model before let's now do the  prediction mode so I'm directly just going to  
5:31
have a follow running through all the images  that we have in our detect and predict folder  
5:36
there we go now we should be able to see all the  predictions so sometimes we made some predictions  
5:40
but here we see that we're basically detecting  like all the cops with the correct labels here  
5:44
we made some detections here we get some nice  predictions again pretty high confidence score  
5:48
and we're only using the medium model so this is  how you can train your own custom YOLOv8 object  
5:52
detection model so thanks for watching this  video here I hope to see in the next one it'd  
5:56
be really exciting to run it live on a webcam  I hope to see in that video guys bye for now
Transcript


Search in video
Introduction: An overview of the episode, highlighting the focus on effective techniques for training machine learning models.
0:00
so in this video here we're going to
0:01
talk about tips for model training we're
0:03
going to cover the different High
0:04
parameters and so on the optimizer how
0:06
you can choose the optimizer the number
0:08
of epoch early stopping and so on these
0:11
are just the best practices once we have
0:13
our data set and we want to train our
0:15
custom Yol 8 models or any other AI
0:18
models out there so let's just jump
0:20
straight into the documentation if you
0:22
go inside the guun tab up here at the
0:24
top and scroll down to the bottom we
0:26
then have this tips for model training
0:29
over here to the right we can see the
0:30
table of content so let's just go over
0:32
them and explain each individual one of
0:34
them and the most common and the best
0:37
practice that you should use for model
0:39
training so first of all let's talk
How to Train a Machine Learning Model: Learn the foundational steps in training a model from scratch, including data preparation and algorithm selection.
0:40
about how machine learning and deep
0:42
learning model acts like learns on a
0:44
high level so if you're doing update T
0:47
instance segmentation and so on we have
0:49
our images we throw it through a model
0:51
and then we have our labels as well so
0:53
you need to label your data we need to
0:54
have our ground Tru once we have our
0:56
ground truth and we have our model
0:58
predictions we can just do a comparison
1:00
and take the difference between what a
1:02
model has predicted and what it should
1:04
have predicted then we can take that
1:06
loss do some math and then we can do the
1:08
back propagation so we do the chain rule
1:10
from math back propagate and then we
1:12
basically just take a look at the loss
1:14
how is the weights each individual
1:16
weight in our Network how is it
1:18
affecting the loss and then we just do
1:20
our back propagation go all the way from
1:22
the output layer all the way down to the
1:24
input layer again we have our Optimizer
1:26
optimizing each individual weight
1:28
depending on the Optimizer that we're
1:30
using the loss and also our learning
1:32
rate so our learning rate basically just
1:34
specifies how large steps do we want to
1:36
take how much do we want to change each
1:38
individual weight in our network if you
1:41
just keep doing this over and over again
1:43
at the end our models would act like
1:45
learn the data that we have so it just
1:47
becomes better and better at predicting
1:49
what we want our model to predict so
1:51
this is a high level overview over how
1:53
machine learning and deep learning
1:55
models are trained so if you want to do
1:57
training on large data sets we need to
1:59
know what a bad size is and also how we
Batch Size and GPU Utilization: Understanding how batch size affects performance and how to utilize GPU efficiently during training.
2:01
can use the GPU in the best possible way
2:04
so it could be that you have a single
2:05
GPU but also multiple gpus so you can
2:08
actually take your model take your large
2:10
data set and train on hundreds of
2:12
different gpus out there if you have the
2:15
resources but most often one GPU is more
2:18
than enough for a lot of use cases so
2:21
the bad size is basically just how many
2:23
images do we want to take in the same
2:25
batch throw it through our model we do
2:27
our forward pass we do our calculations
2:29
with a loss at the end then we do the
2:31
back propagation and optimize our
2:32
weights so it could be that we have a
2:35
batch size of 8 16 and so on so this is
2:37
how many images we load into the GPU Ram
2:40
at the same time do all our passes back
2:43
again optimize our models and then we
2:45
just keep doing that until we have been
2:47
through our whole data set so we just
2:49
take our whole data set and then we
2:51
divide it into individual badges throw
2:53
all that through our model and then we
2:55
train our model like that and this is
2:57
also how the models are learning so
2:59
often you'll use a bad size of 8 16 32
3:02
and so on just make sure that you're not
3:04
running out of dpu ram we can also do
Subset Training: Techniques for training on smaller subsets of data when resources are limited.
3:07
subset training so just take a very
3:09
small subset of your training set make
3:11
sure that your model is act like
3:13
training in the correct way that your
3:14
high parameters so on or tune and this
3:16
is also very good just to save a ton of
3:19
time instead of just spending like days
3:21
or like hours training your model where
3:23
you could just have taking a subset and
3:25
then done some iterations and also some
3:27
tuning based on that before you train on
3:29
your full scale data set we also have
3:32
multiscale training which just means
Multi-scale Training: Discover how training on images of different sizes can enhance the model's ability to generalize effectively.
3:34
that if you have different objects at
3:36
different distances image size and so on
3:38
that we want to generalize our model
3:40
with we can also specify the scaling
3:42
parameters each of these parameters here
3:44
can be set directly once we call the
3:46
Training Method with alter lytics so we
3:49
already have videos covering all of that
3:51
with training the whole computer vision
3:53
pipeline how we can take a data set
3:55
annotate it basically just set up
3:57
everything with allytics it's just a few
3:59
lines of code and we can train our
4:00
custom models and these are all the
4:03
arguments that you can find inside the
4:05
documentation as well so right now we
4:07
can just set it to 0.5 and it will
4:08
reduce the image size by half but we can
4:11
also double it with this scaling value
4:13
here so this is really good if you want
4:15
to have taking inid scales into account
4:18
and basically just be able to detect
4:20
different objects different distances
4:22
object sizes and also various scenarios
4:26
we can do caching so this is basically
Caching Images: Speed up training by caching images to reduce data loading time.
4:27
just how we want to cast the images
4:29
before it's sent into the dpu ram so it
4:32
could be that you want to cat it on in
4:34
the Ram or just have it on disk or we
4:37
just set it to fault here as well where
4:38
it's basically just going to rely
4:40
entirely on the disk input output so
4:43
every time it's going to load in the
4:44
batch it's going to take it from dis so
4:46
if we set the cast equal to true it will
4:48
just load it into RAM instead so that
4:50
will be faster transfer speed from the
4:52
computer RAM so the CPU Ram into the dpu
4:55
ram back and forth so you don't need to
4:57
write and read from our dis every single
4:59
single time we can also do mix Precision
Mixed Precision Training: Enhance training efficiency by using lower precision computations without sacrificing accuracy.
5:02
training so it basically just means that
5:03
we both train a model for 16bit and also
5:06
32bit this is a pretty nice
5:08
visualization so we have our model here
5:10
we do our back propagation so we
5:12
calculate the gradients of our loss for
5:14
a floating Point 16bit model and also
5:17
the floating Point 32 then we do our
5:20
Optimizer here or optimization step with
5:22
the floating Point 32 model so we make
5:24
sure that we don't lose any accuracy or
5:26
any Precision once we're doing the
5:28
optimization steps for a model and then
5:30
we can always go down to our floating
5:32
Point 16 weights again once we do our
5:35
forward passes so this is also a very
5:37
good way to do it and we can just set
5:40
amp equal to true in our training so
5:42
that enables automatic mix Precision you
5:45
can use pretrain mats so instead of
5:46
initializing every single weight
Using Pretrained Weights: Leverage pretrained models to reduce training time and improve accuracy for specific tasks.
5:49
randomly to start with we can use
5:50
pre-train models and this is probably
5:52
the best thing to do in most scenarios
5:54
unless you have a very large scale data
5:56
set you have a lot of processing power
5:58
and also a lot of time so probably take
6:00
a few days and so on to train everything
6:02
from scratch on thousands of images
6:04
compared to if you're just using
6:05
pre-trained weights from the Coco data
6:07
set you can just fine-tune it on your
6:09
own data set and only require a few
6:12
hundred images and then you have a model
6:14
within an hour or so you're good to go
6:17
you can use it in your own applications
6:19
and projects so we can also take a look
Other Techniques for Handling Large Datasets: Additional methods for efficiently managing and processing large datasets during training.
6:21
at some other techniques to consider
6:23
when handling a large data set could be
6:25
a learning rate schedule you start with
6:27
a Higher Learning rate basically just to
6:28
learn more with your model to start with
6:31
and then it will decrease over time so
6:32
we basically just make sure that we're
6:34
not overfitting but we don't want our
6:36
model to learn as much in later stages
6:39
of our EPO could also be distributed
6:41
training so if you have multiple dpus
6:44
available we also have some tips for the
6:46
number of EPO that you need to train for
Tips on Number of Epochs for Model Training: Guidelines for determining the optimal number of epochs to train your model.
6:48
normally you can just go with 300 EPO to
6:51
start with then you can always change it
6:53
depending on the high parameters how
6:54
your losses are converging the meaner
6:57
position and so on when you're looking
6:58
at that you can always Implement early
Early Stopping: A method to prevent overfitting by stopping training when performance stops improving.
7:01
stopping so that just means that if we
7:03
take a look at the graph we just stop
7:05
once our model is not improving any
7:07
longer so often we will take a look at
7:09
the error we don't want a model to
7:11
overfit so if we don't stop the model
7:12
training at some point a model would act
7:14
like overfit to our training data and it
7:17
won't be able to generalize well to our
7:19
validation set and also test set and we
7:21
don't want that when we take our model
7:23
and put it into production so we want
7:26
our models to generalize we can do the
7:28
early stopping both to save time time
7:29
but also dpu resources as well so this
7:33
is a really good technique to implement
7:35
so it doesn't really matter how many EPO
7:37
it will stop once the model is not
7:39
learning any longer also best practices
7:42
and tips for choosing between cloud and
Best Practices for Cloud and Local Training: Explore the pros and cons of training models on cloud versus local machines, helping you choose the best setup.
7:44
local training depending on what you
7:47
have available the scale of your data
7:49
set and so on and also if you want to be
7:51
able to have greater control
7:52
customizations and so on gole Cola
7:54
notebooks are a very good option if you
7:56
just want to train some smaller models
7:58
could also just that you want to have
8:00
everything local with your own data then
8:02
you can just train it on your own dpus
8:05
just make sure that you have a dpu
8:06
available when you're training locally
8:08
or it will take a very long time
8:10
depending on the number of images that
8:12
you have we also have the optimizers
8:14
here the most standard ones is the atam
Optimizers for Model Training: Learn about different optimizers and how they impact model convergence and performance.
8:16
optimizer and the most common used one
8:19
there's different variations and all of
8:21
that for pretty much all of them out
8:22
here but this atom Optimizer is pretty
8:25
awesome and it combines both the
8:27
benefits of SGD which is so ftic
8:29
gradient descent and RMS prop you can go
8:32
and read more about it if that's
8:33
interesting but you should probably just
8:35
go with the atom Optimizer in most cases
8:39
and this is also the default Optimizer
8:40
that we're using to train Al lytics
8:42
optimizers because again it is efficient
8:45
and it generally requires less tuning so
8:47
you don't really have to tune the
8:48
learning rate too much and also all the
8:50
momentums and so on it just works out of
8:53
the box so these are some tips for model
Conclusion and Summary: A recap of the main points, summarizing best practices for training machine learning models efficiently.
8:55
training and also just best practices so
8:58
make sure that you take these into
8:59
account make sure that you know what
9:01
each of these individual ones means
9:03
because it can save you a ton of time it
9:05
basically just makes sure that you train
9:07
the best models out there for your own
9:09
problems in your own projects and
9:11
applications so I hope you learn a ton
9:13
definitely go and check it out play with
9:15
it in a Google collab notebook once
9:17
you're training your models try out
9:18
different optimizers Implement early
9:20
stopping try to play around with number
9:22
of EPO and so on but also just to
9:24
specify the correct bad size if you want
9:26
to use caching mix precision and so on
9:29
what that means it's also good practices
9:32
to do so thanks a lot for watching this
9:34
video here guys make sure to check out
9:35
all the other videos that we have on the
9:37
channel we have pretty much everything
9:39
out there covering every single aspect
9:41
of it and also inside the documentation
9:44
so thanks a lot for watching I hope to
9:45
see you guys in one of the upcoming
9:47
videos until then Happy learning

Search
Write
Sign up

Sign in



Academy Team
Academy Team
Academy Team provides current and detailed information about IT technologies and their impact on our lives. Here, you can find articles, analyses, and comments on the latest developments, innovations, and future trends in IT, artificial intelligence, and software development.

Follow publication

Model Optimization Techniques for YOLO Models
Dr. Fatih Hattatoglu
Dr. Fatih Hattatoglu

Follow
13 min read
¬∑
Jul 3, 2025
184




Computer vision technologies are revolutionizing various fields today, from autonomous vehicles and security systems to medical imaging and industrial automation. Among several models, YOLO (You Only Look Once) stands out for its balance between speed and accuracy. This article will delve into the importance of YOLO models in the field of computer vision, their working principles, and optimization techniques for real-time applications.

Press enter or click to view image in full size

The Importance and Advantages of YOLO Models in Computer Vision
YOLO (You Only Look Once) offers a single-stage approach to object detection tasks, providing both high speed and acceptable accuracy. Compared to traditional two-stage models (e.g., the R-CNN series), YOLO has several key advantages:

Speed: YOLO processes images in a single network pass, making it ideal for real-time applications. For instance, FPS (Frames Per Second) values are critical in scenarios like autonomous driving or video analytics.

Simplicity: The single-stage architecture facilitates easier implementation and optimization of the model.

Overall Performance: YOLO is flexible in detecting different object classes and sizes, making it a versatile solution.

Community Support: As an open-source framework, YOLO is continually improved by a large developer community.

The Necessity of Model Optimization: Efficiency and Speed for Real-Time Applications
Press enter or click to view image in full size

In real-time applications (e.g., object detection, security cameras, real-time decision-making systems), low latency and high FPS are critical. However, high-accuracy models often require more computational resources and time due to their complex architectures. Therefore, optimizing YOLO models is essential to strike a balance between speed and accuracy. Optimization is important for the following reasons:

Resource Constraints: Models running on low-power hardware, such as mobile devices or embedded systems, must be efficient in terms of energy and memory.

Speed Requirements: Real-time applications demand at least 30 FPS or higher speeds.

Generalization Capability: Optimization enhances the model‚Äôs ability to generalize better across different datasets and scenarios.

YOLO‚Äôs Working Principle
YOLO divides an image into grid cells (SxS) and calculates object classification, bounding box predictions, and confidence scores for each cell. The key steps of the model are:

Image Division: The image is divided into an SxS grid.

Predictions: Each cell produces multiple bounding boxes and class probabilities.

Non-Maximum Suppression (NMS): Overlapping boxes are filtered, and the best predictions are selected.

Loss Function: The model optimizes classification, localization, and confidence score losses.

This single-stage approach makes YOLO fast and efficient, but without optimization, balancing high accuracy and speed can be challenging.

Key Concepts of Model Optimization


Model optimization encompasses all the techniques used to improve the performance of a model. Key concepts include:

Balance Between Speed and Accuracy: Smaller models (e.g., YOLOv11n) are faster but may experience a loss in accuracy. Larger models (e.g., YOLOv11x) are more accurate but slower.

Computational Efficiency: Reducing the number of model parameters or utilizing hardware acceleration saves energy and time. However, care must be taken to optimize this without reducing the model‚Äôs prediction power.

Generalization: Techniques like data augmentation and hyperparameter tuning can be applied to ensure the model performs well across different datasets and scenarios.

YOLO Model Optimization Methods
Various methods are used to optimize YOLO models. These methods aim to improve the model‚Äôs speed, accuracy, and generalization capacity. The goal is to find the optimum point that maximizes speed without compromising accuracy.

1. Comparison of Different YOLO Versions
The different versions of YOLO (ranging from smaller to larger, such as YOLOv11n, YOLOv11s, YOLOv11m, YOLOv11l, YOLOv11x) vary in size and complexity. The evaluation parameters, mean Average Precision (mAP) and Frames Per Second (FPS), are used to assess accuracy and speed, respectively. The mAP metric indicates how close the model is to a perfect score of 1, and a higher FPS value means the model is faster. The following table shows a comparison of YOLOv11 models using the A100 GPU in Google Colab. The mAP (expressed as a percentage) and FPS values for the models were compared at a resolution of 640x640. The table also indicates how much improvement was achieved in mAP compared to the previous model.

Press enter or click to view image in full size

Models with the highest FPS can be preferred due to their speed, but as indicated in the table, they may have lower mAP values. Models with FPS values above 30 are considered viable options. According to the table, models with the highest mAP but with FPS values ranking 2nd or 3rd (if they are close to each other) can also be considered. Based on the table, the YOLOv11m model is deemed the optimum model. Although there is only a 0.2 mAP point difference between it and the next best model, it outperforms it by 20 FPS points, making it a better option.

2. Comparison of Image Size
The image size directly affects the balance between speed and accuracy in a model. For instance, when comparing 320x320 and 640x640 input sizes:

320x320: Higher FPS (e.g., ~120 FPS for YOLOv8n), but accuracy loss in detecting smaller objects.

640x640: Higher mAP (e.g., ~37.3% for YOLOv8n), but FPS drops (~80 FPS).

As a recommendation, the appropriate resolution should be chosen based on application requirements. For real-time applications, 320x320 is preferable.

The table below shows the results from a study conducted with the YOLOv8l model and a T4 GPU.

Press enter or click to view image in full size

Although the mAP value for the model with an image size of 640 is reported as 83%, the FPS value is close to 30, even with the use of a T4 GPU. High FPS enables faster detection. An FPS value above 30 is desirable for models. In this case, a value close to the limit (35 FPS) was achieved, which is not ideal. Therefore, the model with an image size of 320 would be a more preferable option in this scenario.

3. Use of Half Precision
Half Precision (FP16) is a number format used in YOLO and other deep learning models. This format represents floating-point numbers with lower precision (16-bit) and uses less memory. This provides performance improvement and more efficient memory usage, especially during GPU-accelerated computations.

YOLO and Half Precision:

Deep learning models like YOLO can work faster and more efficiently by using half precision (FP16), particularly for GPU-based acceleration. Using half precision in YOLO models provides a speed boost, especially on platforms like CUDA and TensorRT.

Benefits:

Faster Training and Testing: The model runs faster because computations are performed with fewer bits.

Lower Memory Consumption: Less memory is used, which is particularly beneficial for large models and datasets.

More Efficient GPU Utilization: Specifically, Tensor Cores (special hardware units found in NVIDIA GPUs) are optimized for FP16 operations.

In summary, Half Precision (FP16) is a technique used to improve the performance of models like YOLO. It offers speed and efficiency but may result in some accuracy loss. In tasks like image processing and object detection, where minor accuracy loss is generally not critical, FP16 is preferred. Half-precision (FP16) arithmetic reduces memory usage and computation time. When used with TensorRT on NVIDIA GPUs, FP16 can increase FPS by 20‚Äì30%, with minimal accuracy loss (0.5‚Äì1% mAP reduction).

Below, the results obtained with the YOLOv8l model and T4 GPU are shown.

Press enter or click to view image in full size

According to the table, the mAP values can be considered the same for both models. However, in some studies, better results have been observed when half_precision=True. In this case, since the mAP values are the same for both, the decision should be based on FPS. As the model with half_precision=False achieves faster FPS, it would be a better choice for this study.

4. Hyperparameter Tuning
Press enter or click to view image in full size

Hyperparameter tuning optimizes the training process of a model. The important hyperparameters are listed below. These parameters are typically represented in the code as follows:

yolo train data=coco8.yaml model=yolo11n.pt epochs=100 batch=16 workers=8 lr0=0.002 momentum=0.9 weight_decay=0.0005 warmup_epochs=3 warmup_momentum=0.8 warmup_bias_lr=0.1 optimizer=AdamW patience=30
Workers: The number of threads used for data loading. More workers (e.g., 8 or 16) reduce the data preprocessing time and speed up the training process. However, a higher number of workers can increase the GPU/CPU load, especially on systems with limited memory. Recommendation: Choose a value between 4‚Äì16 depending on hardware capacity.

Batch Size: The number of images processed simultaneously during training. Larger batch sizes (e.g., 16 or 32) stabilize gradient estimates and can improve generalization capacity, but they require more GPU memory. Smaller batch sizes (e.g., 4 or 8) use less memory but may result in noisy gradients. Recommendation: A value between 8‚Äì32 should be selected according to the GPU memory capacity.

Epochs and Patience: Epochs refer to the number of times the dataset will be fully processed (e.g., 50‚Äì300). Patience is used for early stopping; if the model‚Äôs performance does not improve over a set period (e.g., 50 epochs), the training is stopped. This helps prevent overfitting and reduces training time. Recommendation: A patience of 20‚Äì50 epochs and a total epoch count of 100‚Äì300 may be suitable.

Learning Rate: Determines how quickly the model‚Äôs weights are updated. A high learning rate (e.g., 0.01) leads to fast learning but can cause instability. A low learning rate (e.g., 0.001) is more stable but converges slower. Recommendation: A starting value between 0.001‚Äì0.01, generally optimized with cosine or linear learning rate scheduling.

Momentum: Stabilizes the speed and direction of gradient descent. High momentum (e.g., 0.9) considers past gradients, resulting in smoother optimization. Recommendation: A value between 0.8‚Äì0.9 is generally effective.

Weight Decay: Limits the growth of weights, helping to prevent overfitting. For example, a small value like 0.0005 can improve the model‚Äôs generalization. Recommendation: Fine-tune between 0.0001‚Äì0.001.

Warmup Epochs, Warmup Momentum, Warmup Bias LR: Used to stabilize the model at the start of training with a low learning rate. Warmup epochs (e.g., 3‚Äì5) reduce the model‚Äôs sensitivity to unstable gradients at the beginning. Warmup momentum and bias learning rate ensure more controlled weight and bias updates. Recommendation: 3‚Äì5 warmup epochs, 0.8 warmup momentum, and 0.1 warmup bias LR are appropriate starting values.

Optimizer: Optimizers like SGD (Stochastic Gradient Descent), Adam, or AdamW influence the model‚Äôs convergence speed and performance. AdamW, which includes weight decay, may deliver better generalization results. Recommendation: AdamW is frequently preferred for models like YOLOv8.

Get Dr. Fatih Hattatoglu‚Äôs stories in your inbox
Join Medium for free to get updates from this writer.

Enter your email
Subscribe
Rather than using all of these parameters, it is better to refer to successful cases in the literature for optimal settings.

Application and Impact

Speed and Efficiency: Larger batch sizes and an appropriate number of workers speed up training. For example, increasing the batch size from 8 to 16 can reduce the training time by 20‚Äì30%.

Accuracy: Tuning the learning rate and weight decay can increase mAP by 1‚Äì2%. For example, reducing the learning rate from 0.01 to 0.002 can improve generalization by reducing overfitting.

Resource Usage: Lower worker counts and smaller batch sizes make it feasible to train on devices with limited GPU memory.

Press enter or click to view image in full size

Recommendations

Automated Tuning: Tools like grid search or Optuna automate hyperparameter optimization, making it easier to find the best combination.

Trial and Error: Ideal hyperparameters vary for each dataset and hardware. Testing should be done with small-scale experiments (e.g., 10 epochs).

Hardware Compatibility: If GPU memory is limited, lower batch sizes (4‚Äì8) and fewer workers (4) should be preferred.

5. TensorRT Tools
NVIDIA‚Äôs TensorRT library is a powerful tool for optimizing YOLO models. TensorRT uses the following techniques:

Layer Fusion: Combines unnecessary layers in the model.
Kernel Optimization: Performs hardware-specific optimizations.
INT8 Quantization: Converts the model to 8-bit integer arithmetic, increasing speed.
TensorRT can increase FPS by up to 50% for YOLOv8n (e.g., from 80 FPS to 120 FPS).

6. Model Architecture Modification
Model architecture modification is a powerful method to optimize YOLO models in terms of speed, accuracy, and computational efficiency. This process aims to enhance performance in real-time applications by simplifying or making the model more efficient. It refers to the process of disabling or modifying certain stages in the architecture of YOLO models.

Press enter or click to view image in full size

Modifying YOLO‚Äôs Architecture

Modifying YOLO‚Äôs architecture typically includes the following approaches:

Model Pruning: Removing unnecessary neurons, layers, or connections in the model reduces the number of parameters. This reduces the model size and speeds up inference time. For example, structured or unstructured pruning techniques can be used. Structured pruning removes an entire layer or filter, while unstructured pruning creates a sparse model by zeroing out specific weights.
Quantization: Converting the model‚Äôs weights from full precision (FP32) to lower precision formats (FP16 or INT8) reduces the computational load and memory usage. This is especially effective for embedded systems or low-power devices. INT8 quantization can significantly increase speed with minimal impact on accuracy.
Backbone Optimization: YOLO‚Äôs backbone (e.g., CSPDarknet) can be replaced with a lighter architecture. Lightweight backbones such as MobileNetV3, EfficientNet, or ShuffleNet reduce computational complexity and facilitate mobile device deployment.
Neck and Head Modifications: The neck (e.g., PANet) and head sections of YOLO can be optimized with fewer layers or more efficient merging techniques. For instance, a lighter feature merging structure could be used instead of FPN (Feature Pyramid Network).
Layer Fusion: Combining multiple layers into a single layer reduces computational steps. This is particularly effective when used with tools like TensorRT.
Depth and Width Adjustments: Reducing the model‚Äôs depth (number of layers) or width (number of channels) decreases the model‚Äôs complexity. For example, YOLOv8n provides a lighter structure with fewer layers and channels compared to YOLOv8x.
Knowledge Distillation: A smaller model (student) is trained by learning from a larger, higher-performance model (teacher). This approach helps increase the accuracy of the smaller model while maintaining its speed.
Advantages and Challenges of Model Architecture Modifications

Advantages:

Reduced computational load and faster inference times.

Capability to run on low-power devices (e.g., Jetson Nano, Raspberry Pi).

Reduced memory usage, which is critical for embedded systems.

Challenges:

Risk of accuracy loss, especially with aggressive pruning or quantization.

Testing and fine-tuning modifications can be time-consuming.

Hardware-specific optimizations (e.g., for TensorRT) require additional expertise.

Press enter or click to view image in full size

Analysis and Comments

Structured Pruning: The number of parameters was reduced by 25%, and FPS increased by 18% (from 80 to 95). However, a 0.5% loss in mAP was observed. This option is suitable for low-power devices.

INT8 Quantization: This technique provided the highest FPS increase (37%, from 80 to 110), and the model size was reduced by half. The accuracy loss was minimal (0.8%). Ideal for embedded systems.

MobileNetV3 Backbone: While the speed increased by 25% (from 80 to 100), a more significant accuracy loss (1.4%) was observed. Recommended for mobile devices.

Layer Fusion (TensorRT): Layer fusion with TensorRT resulted in the highest FPS (120) and minimal accuracy loss (0.3%). However, it requires hardware-specific optimizations.

Knowledge Distillation: This method preserved accuracy while providing a slight speed increase. Learning from larger models can improve generalization capacity.

Depth Reduction: Reducing the number of layers increased speed by 31%, but accuracy dropped by 1.1%. Suitable for applications that balance speed and accuracy.

Application Recommendations

Real-Time Applications: Layer fusion with TensorRT or INT8 quantization is most suitable for scenarios requiring high FPS (e.g., autonomous driving).

Low-Power Devices: MobileNetV3 backbone or pruning is ideal for devices like Jetson Nano.

Accuracy-Priority Scenarios: Knowledge distillation offers a slight speed increase while maintaining accuracy, particularly effective for complex datasets.

Testing and Fine-Tuning: The effects of modifications vary depending on the dataset and hardware. Each modification should be tested on the target application and hardware.

In conclusion, this approach is a process that can be performed by professional users. Particularly, users who specialize in CNNs might prefer these methods.

7. Data Augmentation
Press enter or click to view image in full size

Data augmentation enhances the model‚Äôs generalization capacity. Common data augmentation techniques include:

Geometric Transformations: Rotation, scaling, flipping.

Color Manipulations: Changes in brightness, contrast, saturation.

Mosaic Augmentation: Combining multiple images to create richer datasets.

For example, mosaic augmentation can increase YOLOv8‚Äôs mAP by 2‚Äì3%.

Importance of Optimization Processes
Optimization processes are critical for the following reasons:

Training Time: Models with fewer parameters and optimized hyperparameters speed up training.
Resource Usage: Lightweight models can run on low-power devices.
Prediction Speed: High FPS is achieved for real-time applications.
Summary of Optimization Techniques Used for Performance Improvement
In summary, the optimization techniques used to improve the performance of YOLO models discussed in this article are:

Model Selection: Choosing the appropriate YOLO version for the application (e.g., YOLOv11n for low-power devices).
Image Size Adjustment: Selecting the appropriate resolution for balancing speed and accuracy.
Half-Precision: Enhancing memory and computational efficiency.
Hyperparameter Optimization: Speeding up the training process and improving generalization capacity.
TensorRT: Optimized inference for hardware acceleration.
Model Architecture Modification: Designing lighter and faster models.
Data Augmentation: Enhancing the model‚Äôs generalization capacity.
Conclusion
YOLO models stand out in the field of Computer Vision for their balance of speed and accuracy. However, optimization is crucial for real-time applications. Techniques such as comparing different YOLO versions, adjusting image size, using half-precision, optimizing hyperparameters, leveraging TensorRT, modifying model architecture, and applying data augmentation improve YOLO‚Äôs performance. These optimizations reduce training time, minimize resource usage, and increase prediction speeds, making YOLO a more effective solution for real-world applications. In the future, YOLO‚Äôs impact in the computer vision field will continue to grow with new versions and advanced optimization techniques.

Computer Vision
Data Science
Yolo
Model Optimization
Yolov11
184



Academy Team
Published in Academy Team
235 followers
¬∑
Last published Jul 3, 2025
Academy Team provides current and detailed information about IT technologies and their impact on our lives. Here, you can find articles, analyses, and comments on the latest developments, innovations, and future trends in IT, artificial intelligence, and software development.


Follow
Dr. Fatih Hattatoglu
Written by Dr. Fatih Hattatoglu
471 followers
¬∑
3 following
AI Instructor | Author | Dr. Data Scientist | SQL & Statistics & Tableau | Project Director | Data Storytelling |


Follow
No responses yet

Write a response

What are your thoughts?

Cancel
Respond
More from Dr. Fatih Hattatoglu and Academy Team
Full Steps to Use n8n Platform with Docker Desktop for Free
Dr. Fatih Hattatoglu
Dr. Fatih Hattatoglu

Full Steps to Use n8n Platform with Docker Desktop for Free
The concept of Agent has now become a major topic in the IT industry. All topics in the IT field are being reshaped around this concept‚Ä¶
Jul 21
32
1
Which Data Architecture Should I Choose for My Workplace?‚Ää‚Äî‚ÄäA Data Engineer‚Äôs Approach
Dr. Fatih Hattatoglu
Dr. Fatih Hattatoglu

Which Data Architecture Should I Choose for My Workplace?‚Ää‚Äî‚ÄäA Data Engineer‚Äôs Approach
In today‚Äôs world, data has become one of the most valuable assets for organizations, playing a crucial role in making strategic decisions‚Ä¶
Apr 21
209
5
Deploying Your Streamlit Project with a Free AWS Account
Dr. Fatih Hattatoglu
Dr. Fatih Hattatoglu

Deploying Your Streamlit Project with a Free AWS Account
In this article, we embark on a journey through one of the most exhilarating and popular topics in technology: application deployment or‚Ä¶
Mar 5, 2024
136
n8n Platformunu Docker Desktop ile Free kullanmak i√ßin t√ºm Adƒ±mlar
Dr. Fatih Hattatoglu
Dr. Fatih Hattatoglu

n8n Platformunu Docker Desktop ile Free kullanmak i√ßin t√ºm Adƒ±mlar
Agent kavramƒ± artƒ±k tam olarak IT sekt√∂r√ºn√ºn ana g√ºndemi olmaya ba≈üladƒ±. IT alanƒ±ndaki t√ºm ba≈ülƒ±klar bu kavram etrafƒ±nda yeniden‚Ä¶
Jul 21
11
See all from Dr. Fatih Hattatoglu
See all from Academy Team
Recommended from Medium
A Step-by-Step Guide: Detecting Anomalies in Video Using Computer Vision
Sajid Khan
Sajid Khan

A Step-by-Step Guide: Detecting Anomalies in Video Using Computer Vision
From Simple frame differencing to advanced deep learning, how to build accurate, real-time anomaly detection pipelines for videos

Aug 29
11
Image Basics: RGB, Grayscale, and Histograms in Computer Vision
Python in Plain English
In

Python in Plain English

by

Muhammad Syaoki Faradisa

Image Basics: RGB, Grayscale, and Histograms in Computer Vision
In computer vision, everything starts with pixels. Before we jump into fancy models, we need a solid grasp of RGB (color), grayscale‚Ä¶

Aug 22
1
Prepare Your Datasets for YOLO26
Zain Shariff
Zain Shariff

Prepare Your Datasets for YOLO26
YOLO26 is set to release in late October. Here is how to prepare your datasets for the release.
Oct 5
2
Understanding Video Processing Basics: Vision Transformers & Temporal Dynamics
Artificial Intelligence in Plain English
In

Artificial Intelligence in Plain English

by

Kushagra Pandya

Understanding Video Processing Basics: Vision Transformers & Temporal Dynamics
In a world flooded with videos‚Ää‚Äî‚Ääfrom TikToks to security footage‚Ää‚Äî‚Äälet‚Äôs explore how machines learn to ‚Äúwatch‚Äù and make sense of moving‚Ä¶

Sep 15
1
Fine-Tuning YOLO Models with automated data labelling pipeline
Bootcamp
In

Bootcamp

by

Anubhav

Fine-Tuning YOLO Models with automated data labelling pipeline
From Zero Labels to Production: The YOLO-World and Gemini Data Loop

Oct 9
4
I wasted months running slow LLMs before learning this
AI Advances
In

AI Advances

by

Nikhil Anand

I wasted months running slow LLMs before learning this
Why your LLM is running at just 10% of its potential speed

Oct 10
724
10
See more recommendations
Help

Status

About

Careers

Press

Blog

Privacy

Rules

Terms

Text to speech
Transcript


Search in video
Intro
0:00
And now on a single video, we're getting
0:02
more than 200 frames per second. And you
0:04
can see now the system in action where
0:06
I'm running these four six cameras at
0:08
the same time. And I'm able to get at
0:10
least 60 frames per second per camera on
0:13
just a single machine. Hi, welcome to
Overview
0:15
this new video. This now we're going to
0:16
see how to increase the object detection
0:18
speed with YOLO by four or five times.
0:20
In this video, we're going to see eight
0:22
different key factors that affect the
0:24
object detection speed. also what you
0:26
need to change to increase the speed and
0:28
also we're going to see some practical
0:30
implementations in action. So I'm going
0:31
to take different videos and we're going
0:33
to see the nonoptimized versions against
0:35
the optimized versions. You need to
0:38
follow this if you're getting a very low
0:40
frame rate. So if you're not able to get
0:42
many frames per second when doing object
0:44
detection with YOLO. Also if you need
0:46
assistant that can analyze and perform
0:49
audio detection on a very large volume
0:51
of data. So if you have thousands and
0:53
thousands of hours to process for your
0:55
project with object detection then you
0:57
absolutely need to optimize the
0:59
performance otherwise you will waste a
1:01
lot of resources because it's not
1:02
optimized or in any case it will take a
1:05
very long time to perform the detection
1:08
on all this data and also it's essential
1:11
if you need to perform object detection
1:13
in real time in order to process all the
1:16
frames that the camera gives to your
1:18
system but also if you need to run
1:21
detection multiple cameras. So if you
1:23
have like CCTV systems and you have
1:26
multiple camera, if you don't optimize
1:28
the system, you can't process everything
1:31
on a single machine. So we're going to
1:33
see how with this system, we can have a
1:36
single m machine that can process 10 or
1:38
plus cameras in real time. So this is
1:41
huge. And also this is essentially if
1:44
you have an edge device like Nvidia
1:46
Jetson, they are very limited in power.
1:48
So you need to optimize your system to
1:50
the maximum to get a decent performance
1:52
to perform the detection in real time
1:54
and more. So this is only some of the
1:56
things that will be useful but anyway
1:58
it's good to optimize the system for
2:01
saving cost and having always better
2:03
performance. If this is good for you
2:05
let's go. Before moving further I want
2:07
to let you know that all the sources and
2:08
a dedicated course to increase the speed
2:10
of object detection will be inside the
2:12
AI vision academy. In addition, we
2:15
courses on detecting objects and more.
2:17
And also, you will have access to a
2:19
community where you can interact with
2:21
other members. You can ask support for
2:23
your project and you can get direct
2:24
access to me if you want to ask any
2:26
question. Let's start first with the
2:27
choice of the YOLO model. You should be
2:29
familiar about this knowing that there
2:31
are different YOLO models that we can
2:33
use. We have small models that have low
2:36
precision but very high speed. And then
2:38
we have bigger model that have higher
2:40
precision but lower speed. And here we
YOLO Models
2:42
can see a chart from Ultraex with the
2:44
different versions of YOLO. We have the
2:46
nano small, medium, large, and extra
2:48
large. Where the nano version, it's only
2:52
2.6 million parameters and the extra
2:54
large version is 56.9 million
2:57
parameters. So there is a huge gap
2:59
between them. With the nano version that
3:02
has the average precision of
3:04
39.5 where the extra large version is
3:07
54.7. So this needs to make you think
3:10
that despite the big difference in the
3:12
size, there is not a huge difference in
3:14
performance. Mostly I recommend to go
3:17
with the standard version which is the
3:19
medium version which is YOLO 11 medium
3:22
or the version that it's available at
3:24
the moment. Let's quickly compare them.
3:26
By default using the nonoptimized ptor
3:28
version first the YOLO 11 Nano we're
3:30
getting around 54 55 frames per second
3:34
but it is using nonoptimized version.
3:36
This will make a big difference when
3:38
it's optimized. And now the extra large
3:40
version. So if we put YOLO X here, we
3:42
have almost half of the speed with the
3:45
YOLO XLAR version. Second factor is the
Image Size
3:47
image size because uh doesn't matter the
3:50
size of your image. Let's say that we
3:52
have this image 6,000 by 4,000 and we
3:55
want to perform the texture on this one.
3:57
When we give this image to the model, by
3:59
default, it will always shrink the image
4:01
to a very small size 640x 640 pixels.
4:05
this shrinking of the image, passing a
4:07
very large image when it's not necessary
4:09
can affect the speed. So I recommend
4:11
always to not give a very high
4:13
resolution image but keeping to around
4:16
like HD size only 1,280 by 720. Uh there
4:20
is no need to give much bigger image
4:22
unless you have a complex project but
4:24
you will perform some more advanced
4:26
detection. That's another case but in
4:29
general give give a small image because
4:31
that's what the model will be using. If
4:34
you give a larger image, there will be
4:36
all the overhead and processing to
4:38
shrink the image and that's also
4:40
something that it's affecting the speed.
Core
4:42
Let's start now getting to the core.
4:44
Even if it's not yet there, later we
4:46
will get to the real important what
4:48
really makes a difference. This is
4:50
already a starting point. And if you're
4:52
not familiar with this, it's better that
4:54
you understand this. We have two options
4:56
to run the model. Mostly we can run it
4:59
on the CPU or the GPU, the graphic card.
5:02
There is a huge difference from the CPU
5:05
versus the GPU where with a CPU, even a
5:08
good one or a decent one, you will get
5:10
only a few frames per second. In this
5:13
case, I was getting five frames per
5:15
second with my CPU Ryzen 5
5:18
5,600. And with the graphic card, we can
5:22
get 100 frames per sec per second. So,
5:24
we can get at least 30x speed increase.
5:28
But if we optimize the model with the
5:30
graphic card, we get even much more than
5:32
that. So you see here later we will see
5:34
some other example where we get much
5:36
better performance. But if you have
5:39
tried the difference from CPU versus
5:41
GPU, you will see that another factor
Hardware Balance
5:44
when building your own system is to find
5:47
the hardware balance to avoid
5:48
bottleneck. You get bottleneck when
5:50
there is a component that is limiting
5:52
the speed of another component. For
5:54
example, if you have a very powerful
5:57
Nvidia graphic card which can allow you
5:59
to run 10 or more cameras at the same
6:02
time, but you have a very low power CPU
6:05
which is not enough to handle all the
6:08
processes to handle all these video
6:10
streams, then you will not get full use
6:13
of the graphic card because like the CPU
6:15
is the hardware bottleneck in your case.
6:17
Or if for example you don't have enough
6:19
RAM memory then the RAM will limit the
6:22
amount of videos that you can process at
6:24
the same time because it cannot open
6:27
just them all at the same time because
6:29
there is not enough memory. And this is
6:32
quite common more than I imagine I had
6:34
some clients that had some problem with
6:36
running the systems to a certain frames
6:38
per second. They needed to process
6:40
multiple cameras at the same time. they
6:42
had like a huge graphic card but then
6:44
they had like very old like 10 years old
6:47
CPU that wasn't able to handle much of
6:50
that and I'm surprised like even some
6:53
expert when they develop some complex
6:55
software then there are this mistakes so
6:57
that's why I'm putting this right here
Model Optimization
6:59
in this video then fifth we have the
7:01
model optimization which is essential to
7:04
increase dramatically the speed of your
7:06
detection and this depending on the
7:09
platform where we run this we have many
7:12
platforms and now here I'm showing you
7:13
like the most popular that I've tested.
7:16
Normally we have the model in PyTorch
7:18
which is the one of the most common deep
7:21
learning library that we use for
7:22
computer vision and all the model that
7:25
you can download online mainly like the
7:27
YOLO model are in PyTorch format.
7:29
PyTorch is not optimized for speed. In
7:33
this case we need to convert this model
7:35
into an optimized framework. For
7:37
example, we have the open veno framework
7:39
that works with Intel CPU which will be
7:43
will be good to be used when you're
7:45
working with CPUs or if we're working
7:48
with graphic card, I recommend to go
7:50
with the Nvidia graphic cards and we
7:51
have the framework tensor RT. So, we
7:54
need to convert the model from the
7:55
PyTorch version to the tensor RT version
7:58
and this will make a huge change
8:00
dramatic improvement in the speed of our
8:03
detection. Let's now compare the medium
8:05
version. So now we have the YOLO medium
8:08
by PyTorch. You see we're getting around
8:10
45 frames a second. Let's try the same
8:13
but in tensor RT format. Now instead of
8:15
PyTorch I converted already this model.
8:17
So we go engine which is the tensor RT
8:19
version. If we run this already with
8:21
this small change we're already doubling
8:23
the speed. So we're getting around 100
8:25
plus frames a second just with
8:27
converting the model and doing zero
8:29
optimization in the code. One of the
Linear vs Batch
8:31
main factors that will dramatically
8:33
increase the speed in object detection
8:35
is to process everything in batches.
8:37
Let's now compare a linear nonoptimized
8:40
system versus a more optimized system.
8:42
This is normally what we have. We grab
8:45
the frame, we perform object detection.
8:48
If we want, we perform object tracking.
8:50
We display the frame and we do this in a
8:52
loop. So it's a video. It gives us an
8:55
illusion that it's a video. But a video
8:57
is nothing more than a frame after frame
8:59
in a loop. Why this linear way is not
9:02
optimized? Because after we grab the
9:05
frame, while we are performing object
9:06
detection, the frame grabbing is waiting
9:09
for us to get through everything
9:12
detection tracking and display the
9:13
frame. How could this be more optimized?
9:15
It could be more optimized if instead of
9:19
doing one one one grabbing one frame,
9:21
one detection, one tracking, we can do
9:24
this in batches. So we grab 32 frames
9:27
for example. We give the 32 frames in
9:30
batch to the object detection model and
9:32
the object detection model can perform
9:34
detection on the 32 frames. Why is this
9:36
much faster? Because there is a lot of
9:39
overhead in moving a image from the disk
9:42
to the graphic card and it's a lot of
9:45
waste and also detecting only a single
9:48
image and detecting with the optimized
9:50
model 32 images. There is not a big
9:53
difference. It definitely doesn't take
9:54
32 times. So there is a huge increase in
9:57
performance by doing everything in
9:59
batches. So an idea of optimized model
10:02
will be grab 32 frames and give that to
10:05
the object detection and then process
10:07
trafficing and display frame. This will
10:09
make of course already some big
10:11
improvement. But I want to show here
10:13
even a more optimized model that I
10:15
created and we're going to see the
10:16
comparison that we will get from one to
10:19
the other. And it's multi-threading. a
10:21
multi-threading that process everything
10:23
in batches. So how will this work? It's
Multithreading
10:26
something like this. We have a camera
10:28
thread where it's going to grab frames
10:32
no stop. It's going to put everything
10:34
into a frame buffer. So each time it can
10:37
grab frames, it grab frames and it's
10:39
works autonomously. At the same time, we
10:41
have a different thread. It's a thread
10:44
that it's performing detection from the
10:46
frame buffer memory. We're going to get
10:48
all the frames whenever we have frames
10:50
available and we're going to perform
10:52
detection. We're going to perform the
10:53
detection in batches. In this case, like
10:55
by default, I saw that 32 images at the
10:58
same time is one that gives the best
10:59
performance. We perform 32 frames
11:02
detection in batches. And then this is a
11:05
continuous loop. So it's separate from
11:07
the camera thread. So they work
11:09
autonomously. And this is going to put
11:12
everything into a detection buffer. In
11:14
this way we can already show everything
11:17
that we have if we want. If we also need
11:19
tracking which most of the case will be
11:21
necessary for the projects. If we need
11:23
to analyze or uh track some objects then
11:27
everything will move into a tracking
11:29
thread and it will be the same operation
11:31
the tracking thread autonomously. We
11:32
track everything based on the detection
11:34
and then we save everything in a
11:36
tracking buffer and then we have to do
11:39
nothing more than just display
11:40
everything that we have in a tracking
11:42
buffer. This will make a huge
11:44
difference. Let me show you what we have
11:46
now with this. Oh, let me use another
11:48
video for this one. So, standard YOLO
11:50
medium PyTorch. Let's run this one. We
11:52
get around 45 frames a second. Let's now
11:56
run the standard model with PyTorch
11:58
again, but into multi-threading. Now,
12:00
I'm using a different code. I created
12:02
this code to run object detection and
12:04
tracking into multi-threading. So, for
12:07
everything runs a separate thread. If
12:10
you want to get this code with also the
12:12
course that explained everything,
12:13
everything will be inside the AI vision
12:15
academy link below in the description.
12:18
And now let's run this one. So we're
12:20
going to use again the PyTorch model
12:21
standard model and we're going to use
12:23
batch size one. So we're going to use
12:25
only one image per time. Let's run this
12:28
with the same model PyTorch. We're more
12:30
or less probably getting the same. It's
12:32
not able to process more than this.
12:34
Maybe a couple of frames more per
12:35
second. We have around stable 48 frames.
12:38
Let's now use still PyTorch model but
12:41
increasing the batch size. So instead of
12:44
using just batch size one, let's try for
12:46
example batch size 16. We're using a
12:49
nonoptimized PyTorch model and we're
12:51
getting around 200 frames a second just
12:53
because we are processing everything
12:55
into batches. Let me see how we're using
12:58
the GPU resources. So GPU usage is
13:01
around
13:02
71%, CPU is around 55%. And let's now go
13:08
and increase this even more. So increase
13:10
the batch size to 32. And let's use an
13:12
optimized model. So instead of using the
13:15
PyTorch model, we're going to use the
13:17
engine model. So the tensor RT format.
13:20
So EO 11 medium batch 32. That's how we
13:23
set the model. And now on a single
13:26
video, we're getting more than 200
13:28
frames per second. we and at the same
13:30
time I using the GPU only around 60% and
13:34
CPU 50%. This means that I'm able to
13:38
detect with 200 frames per second not
13:41
using the full power of this machine.
13:43
This means that I can add also other
13:46
videos at the same time and also so push
13:50
this machine to the maximum to get 400
13:53
plus frames a second in this case just
13:54
with a single machine. We're also going
13:56
to test that. But before testing that,
13:58
let me let me explain like the last
14:00
point of this video. The eighth point,
Floating Points
14:02
the last but not the least important
14:04
factor is convert the model into
14:06
floating point. We have three different
14:07
floating points. We have the FL 32 which
14:10
is the modify torch model that we use by
14:12
default. Mostly that's used for
14:14
training. Then we have 16 which is
14:16
called the hard precision model. This
14:18
doesn't mean that you get hard precision
14:20
or half accurus. It means that instead
14:22
of float 32 instead of taking 32 bits of
14:25
memory the each value inside the deep
14:28
learning model will take 16 bits. So
14:31
it's half of the size and this is
14:33
normally used for inference because you
14:35
don't see much of a difference in the
14:37
accuracy. The accuracy is almost the
14:39
same but the model is much smaller and
14:42
inside the memory of the GPU and it's
14:44
also much faster. There is another one
14:46
which is int 8 which will have less
14:48
precision and this is normally used when
14:51
you have edge devices or like CPU
14:53
devices that are very limited in power.
14:56
So keep in mind that choosing the right
14:58
floating point for your model. It's
15:00
another requirement when you need to
15:01
optimize the model to the maximum. And
15:04
now let's finally test everything with
15:07
also multiple cameras at the same time.
15:10
And you can see now the system in action
15:11
where I'm running these four six cameras
15:14
at the same time. and I'm able to get at
15:16
least 60 frames per second per camera on
15:19
just a single machine. But most
15:21
important is that I'm not even using
15:23
most of GPU. I'm using only 60% because
15:26
CPU is the maximum. So this means that
15:28
in my case, this should be a mistake if
15:31
the system was designed for this. The
15:33
CPU is the bottleneck for this system.
15:36
It means that if I had a more powerful
15:38
CPU, I could get many more frames per
15:41
second. So could run many more cameras
15:42
getting 60 frames a second. If you only
15:45
need to get 30 frames a second or even a
15:47
bit less because that's what necessary
15:49
for vehicle tracking or people detection
15:51
and tracking then you can run 10 15
15:54
cameras at the same time on just a
15:56
single machine which is incredible. And
15:59
this is just a desktop machine. If you
16:00
run on GPU server then you can get much
16:05
much more on a single machine. So this
16:07
will dramatically decrease the cost of
16:09
your system if you're renting graphic
16:12
cards. I hope that you enjoyed this
16:13
video that you got a lot of valuable
16:15
information from it. Let me know if you
16:16
have any question about increasing the
16:18
speed for object action below in the
16:20
comments. If you want the sources, if
16:22
you get want to get in touch with me and
16:24
Viva community, the links of the academy
16:26
is down below in the description. This
16:28
is all for now. See you in the next
16:30
video.
Transcript


0:00
are you tired of super slow AI models
0:03
frustrated by poor accuracy and yes I
0:06
know most courses at their only teach
0:08
you how to use the YOLO model but what
0:10
if I tell you there's something more
0:12
introducing the first ever course that
0:14
doesn't just teach you how to use YOLO
0:16
but empowers you to Super its
0:18
performance I will teach you the art of
0:19
AI acceleration like you've never seen
0:21
in any course before beforehand hi my
0:24
name is prento I Got My THD in AI years
0:27
ago when I did My THD I modified YOLO to
0:30
become faster and more accurate at the
0:32
same time I have used all the YOLO
0:34
sequences since version 2 until now the
0:36
YOLO V8 in this Grand breaking course
0:38
you'll learn first how to turbance your
0:41
model speed in both TPU and CPU using
0:44
tensor RT and open FAL model
0:45
quantisation your model will be like a
0:47
well-tuned racing car second achieve the
0:50
Perfect Blend of speed and accuracy
0:52
through architectural modification while
0:54
it's true that in some use cases
0:55
achieving both may seem like a challenge
0:57
the exciting part is that there are
0:59
conos applications where we can improve
1:01
speed and accuracy simultaneously the
1:03
models will be Swift and precise which
1:05
is desperately needed for many real
1:07
world cases based on our experiment we
1:09
can increase the accuracy up to 20 m
1:12
points and boost the speed up to four
1:13
times so don't just setle for one or the
1:16
other let's go for the perfect balance
1:18
next dive into the art of hyperparameter
1:20
tuning and data augmentation to create
1:22
models that outperform the competition
1:24
in terms of robustness lastly are you
1:27
struggling to get enough data in your
1:28
data set say colot to Pudo labeling your
1:31
secret weapon for effortlessly enriching
1:33
your data set more deta means better
1:35
accuracy this master class has been
1:37
taught as premium paid workshops a
1:39
couple of times that are
1:40
enthusiastically attended by industrial
1:42
professionals researchers and students
1:45
what do they say let's find
1:48
[Music]
1:49
out now it is time for you to take
1:52
advantage of it wait you might be
1:54
thinking I'm a beginner fear not I have
1:56
got you covered you'll receive a crystal
1:58
clear breakdown of your 8's architecture
2:00
a deep dive into convolutional neuron
2:02
networks and a guide on how to install
2:05
yo8 I taught this course using Google
2:07
collab notebook to insure a hasslefree
2:09
experience no messy installation and the
2:12
code runs immediately regardless of your
2:14
operating system get ready to embark on
2:16
a transformative AI Journey join me in
2:19
the YOLO f8 performance Improvement
2:20
Master Class en roll now and unleash the
2:24
full potential of Yol ly8
Skip to main content
How to increase accuracy of yolov8 model on crack detection... : r/deeplearning

r/deeplearning
Current search is within r/deeplearning

Remove r/deeplearning filter and expand search to all of Reddit
Search in r/deeplearning
Log In

Expand user menu
Skip to NavigationSkip to Right Sidebar

Back
Go to deeplearning
r/deeplearning
‚Ä¢
2y ago
Imlegion-123

How to increase accuracy of yolov8 model on crack detection...
Hello everyone i don't know if this is the place to ask this question but i am actually stuck.

I am trying to train a yolov8 model for crack detection on roads and the precision I'm getting is not more than 85%. What can i do and what layers can be added to this please help...


Upvote
1

Downvote

2
Go to comments


Share
Add your reply
Sort by:

Best

Search Comments
Expand comment search
Comments Section
gimcel
‚Ä¢
2y ago
Hey! I just started applying a YOLO model (v5) for object detection. But maybe I can give some advice, despite my limited experience :-)

I think your model performance sounds ok, but thats highly dependant on your data. In general you could try the following things to get a few percent more performance:

Hyperparameter tuning (number of epochs, batch size, optimizer ...)

Try different YOLOv8 models (like v8l or v8x), maybe your model is too small.

Add more and diverse training data (pre-training is also generally a good idea). Check in which cases YOLO fails to detect your cracks, maybe some cracks are too small, maybe there are cracks that look totally different from your training data, maybe you have a lot of false positives, maybe there are features that are look similar to cracks?

Check your labels. There are some guidelines what you should avoid during your labelling (e.g., missing and not labelling instances in your training data is bad, bounding boxes that are too large is bad, etc.)

Add background images, meaning images where no crack (or any class you want to detect) is present. You could also try to reduce the number of classes you want to detect. In my application, the model performance improved by a few percent, just by adding background images!

Check the literature and see what kind of methods (data augmentation, special loss functions, different yolo versions etc.) they are using. Maybe you can get more specialized advise there.

Here are some useful links I used to set up my first models: https://docs.ultralytics.com/yolov5/tutorials/tips_for_best_training_results/?h=training

https://wandb.ai/iankelk/YOLOv5/reports/Search-and-Rescue-Augmentation-and-Preprocessing-on-Drone-Based-Water-Rescue-Images-With-YOLOv5--VmlldzoxOTk4MTI2

https://www.kaggle.com/code/vbookshelf/basics-of-yolo-v5-balloon-detection

Good luck! If you solve your problem, I would be interested to learn what you did! Maybe your experience helps me (and others) too.


u/Imlegion-123 avatar
Imlegion-123
OP
‚Ä¢
2y ago
Thank you for your generous input... I'll surely dm you once i solve this. Thanks again

Related Answers Section
Related Answers
Best architectures for image recognition
Applications of deep learning in healthcare
Differences between CNNs and RNNs
Use cases of generative adversarial networks
Latest trends in unsupervised deep learning
New to Reddit?
Create your account and connect with a world of communities.


Continue With Phone Number
By continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy.
More posts you may like
Is my aim bad and how can I improve it? (Read Description)
r/VALORANT icon
r/VALORANT
‚Ä¢
2y ago
Is my aim bad and how can I improve it? (Read Description)
2 upvotes ¬∑ 12 comments
Help with Detectron2 Instance Segm Model
r/computervision icon
r/computervision
‚Ä¢
2y ago
Help with Detectron2 Instance Segm Model
r/computervision - Help with Detectron2 Instance Segm Model
3
1 upvote ¬∑ 7 comments
I left the printer alone for a while but a few asked about my setting so her if any one has any idea on how I can improve my ulimaker slicer settinging please don't fear and reach out please
r/anycubic icon
r/anycubic
‚Ä¢
3y ago
I left the printer alone for a while but a few asked about my setting so her if any one has any idea on how I can improve my ulimaker slicer settinging please don't fear and reach out please
r/anycubic - I left the printer alone for a while but a few asked about my setting so her if any one has any idea on how I can improve my ulimaker slicer settinging please don't fear and reach out please
4 upvotes ¬∑ 6 comments
Hey guys... I want to train my AI machine..and use it to predict some results.. How do i get started.. I have no clue.. How will i create the machine..Where to start? Any leads..?
r/pythontips icon
r/pythontips
‚Ä¢
3y ago
Hey guys... I want to train my AI machine..and use it to predict some results.. How do i get started.. I have no clue.. How will i create the machine..Where to start? Any leads..?
6 comments
Help with QC for CF V3 Panda
r/RepTimeQC icon
r/RepTimeQC
‚Ä¢
2y ago
Help with QC for CF V3 Panda
r/RepTimeQC - Help with QC for CF V3 Panda
8
1 upvote ¬∑ 7 comments
Need some help with configuration of Cobra Neo
r/anycubickobra icon
r/anycubickobra
‚Ä¢
3y ago
Need some help with configuration of Cobra Neo
2 upvotes ¬∑ 25 comments
Is there a way to speed up the scan using an HM-10 BLE module
r/arduino icon
r/arduino
‚Ä¢
2y ago
Is there a way to speed up the scan using an HM-10 BLE module
4 upvotes ¬∑ 1 comment
I have adhd, forgive me if this is easy to find info....
r/BearableApp icon
r/BearableApp
‚Ä¢
2y ago
I have adhd, forgive me if this is easy to find info....
7 upvotes ¬∑ 1 comment
How to improve fps in my game?
r/Unity3D icon
r/Unity3D
‚Ä¢
2y ago
How to improve fps in my game?
12 comments
How to progress this build?. Poison Blade Vortex
r/PathOfExileBuilds icon
r/PathOfExileBuilds
‚Ä¢
2y ago
How to progress this build?. Poison Blade Vortex
1 comment
Help with FS-CT6B transmitter
r/diydrones icon
r/diydrones
‚Ä¢
3y ago
Help with FS-CT6B transmitter
4 upvotes ¬∑ 3 comments
Help/tips for safe operation of new AM5 platform parts?
r/pchelp
‚Ä¢
2y ago
Help/tips for safe operation of new AM5 platform parts?
3 upvotes ¬∑ 4 comments
View Post in
Êó•Êú¨Ë™û
–†—É—Å—Å–∫–∏–π
Portugu√™s
‰∏≠Êñá (ÁÆÄ‰Ωì)
Fran√ßais
Community Info Section
r/deeplearning
Join
Deep Learning
Public
Top Posts
Reddit
reReddit: Top posts of June 18, 2023
Reddit
reReddit: Top posts of June 2023
Reddit
reReddit: Top posts of 2023
Reddit Rules
Privacy Policy
User Agreement
Accessibility
Reddit, Inc. ¬© 2025. All rights reserved.

Collapse Navigation
Skip to main content
[D]How to Improve YOLO v8 model performance ? : r/MachineLearning


r/MachineLearning
Current search is within r/MachineLearning

Remove r/MachineLearning filter and expand search to all of Reddit
Search in r/MachineLearning
Log In

Expand user menu
Skip to NavigationSkip to Right Sidebar

Back
r/MachineLearning icon
Go to MachineLearning
r/MachineLearning
‚Ä¢
2y ago
Ordinary_Run_2513

[D]How to Improve YOLO v8 model performance ?
Discussion
Hi everyone! I'm working on a model using YOLO v8x to detect regions on identity cards, but it struggles with identifying address regions. This issue seems to stem from insufficient data. Would it be advisable to incorporate additional data containing addresses(other documents instead of identity card) to enhance the model's accuracy in detecting address regions?


Upvote
4

Downvote

11
Go to comments


Share
u/modal-labs avatar
modal-labs
‚Ä¢
Promoted

What if scaling to 1000 GPUs was as easy as adding @app.function? Modal makes it reality. Free $30 compute included.
Sign Up
modal.com
Thumbnail image: What if scaling to 1000 GPUs was as easy as adding @app.function? Modal makes it reality. Free $30 compute included.
Add your reply
Sort by:

Best

Search Comments
Expand comment search
Comments Section
arm2armreddit
‚Ä¢
2y ago
usually the rule of thumb: more data, better results


u/InternationalMany6 avatar
InternationalMany6
‚Ä¢
2y ago
‚Ä¢
Edited 2y ago
Sure, big data often leads to better insights, but it's not just about quantity, mate. Gotta ensure quality and relevance too! Just piling up data ain't the answer.


Ordinary_Run_2513
OP
‚Ä¢
2y ago
it's a geometry task. it figuring out which part of the image has the region based on the position/shape/texture of that part of the image.

u/StephaneCharette avatar
StephaneCharette
‚Ä¢
2y ago
This issue seems to stem from insufficient data.

How did you determine this? For example, another very common problem is people don't correctly size their neural network. If the address regions are too small, it could explain why detection behaves poorly. See this video: https://www.youtube.com/watch?v=m3Trxxt9RzE

a model using YOLO v8x

You understand that version of YOLO is both slower and less precise than the earlier versions of YOLO, right? Take a look at the results between YOLOv4 / YOLOv4-tiny and the later equivalent configurations. Here is one video where I compare some of these results: https://www.youtube.com/watch?v=JSgDs0XXz8M

Would it be advisable to incorporate additional data containing addresses(other documents instead of identity card)

How many images do you have now? How many different types of "identify cards" do you have now? Are these flatbed scanner images, or is this people holding their cards in their hand, thus resulting in rotation issues, different zoom lighting, etc?

My first thought is you should review your existing annotations. See the "review" functions in DarkMark (https://www.ccoderun.ca/darkmark/Summary.html#DarkMarkReview) to start. If the review and the other things I mention above check out, then see if anything can be done to make detection easier. For example, in the DarkPlate project (https://github.com/stephanecharette/DarkPlate#darkplate) we crop the plates to match the network dimensions, allowing us to more easily detect and read the individual characters. Can you do the same?

And lastly...as you wrote, add more training images.


u/pm_me_your_smth avatar
pm_me_your_smth
‚Ä¢
2y ago
You understand that version of YOLO is both slower and less precise than the earlier versions of YOLO, right?

Do you have a source for this? I couldn't find any benchmarks which included v8 (I assume it's because the model's too recent)


u/InternationalMany6 avatar
InternationalMany6
‚Ä¢
2y ago
‚Ä¢
Edited 2y ago
Actually, YOLOv8 is designed to outperform its predecessors in both speed and accuracy, thanks to improvements in neural network architecture and training techniques. Check out the latest updates and benchmarks from the official YOLO GitHub page for more accurate details!

Ordinary_Run_2513
OP
‚Ä¢
2y ago
I possess a collection of 120 images. Among these, approximately 30 images contain addresses. The used data contains two distinct types of identity cards. my process involves rotating the images to their proper orientation and they aren't scanned.


yogidrink
‚Ä¢
2y ago
I tried some fine tuning training with a a few hundred aswell and it didnt work out well. Worked great with more data though. Only 30 annotations isnt a lot


2 more replies
u/StephaneCharette avatar
StephaneCharette
‚Ä¢
2y ago
The general rule of thumb is 1000 image samples per class. 30 examples of addresses is clearly not enough. https://www.ccoderun.ca/programming/darknet_faq/#how_many_images

Related Answers Section
Related Answers
Best practices for feature engineering in ML
Impact of transfer learning on small datasets
Ethical considerations in AI development
Role of explainable AI in model transparency
Use cases of generative adversarial networks
New to Reddit?
Create your account and connect with a world of communities.


Continue With Phone Number
By continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy.
More posts you may like
Use of ADetailer with YOLO Models
r/StableDiffusion icon
r/StableDiffusion
‚Ä¢
2y ago
Use of ADetailer with YOLO Models
2 upvotes ¬∑ 4 comments
UNH YOLO & Thesis
r/wallstreetbets icon
r/wallstreetbets
‚Ä¢
5mo ago
UNH YOLO & Thesis
r/wallstreetbets - UNH YOLO & Thesis
192 upvotes ¬∑ 107 comments
[D] Improve model performance on the fly. How to approach this?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
2y ago
[D] Improve model performance on the fly. How to approach this?
2 comments
How to quickly improve YOLOv8 model for object detection on drone?
r/computervision icon
r/computervision
‚Ä¢
2y ago
How to quickly improve YOLOv8 model for object detection on drone?
8 upvotes ¬∑ 29 comments
factorio-analytics - a package for testing and analyzing prepared blueprints
r/factorio icon
r/factorio
‚Ä¢
2y ago
factorio-analytics - a package for testing and analyzing prepared blueprints
89 upvotes ¬∑ 8 comments
Best alternative to attention so far?
r/LocalLLaMA icon
r/LocalLLaMA
‚Ä¢
2y ago
Best alternative to attention so far?
38 upvotes ¬∑ 17 comments
[D] Anyone using smaller, specialized models instead of massive LLMs?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
10d ago
[D] Anyone using smaller, specialized models instead of massive LLMs?
97 upvotes ¬∑ 52 comments
Enhancing Game Development Efficiency with the Game Analyzer Tool
r/gameenginedevs icon
r/gameenginedevs
‚Ä¢
2y ago
Enhancing Game Development Efficiency with the Game Analyzer Tool
r/gameenginedevs - Enhancing Game Development Efficiency with the Game Analyzer Tool
12 upvotes ¬∑ 6 comments
[SDXL] Simplified explanation of aesthetics score
r/StableDiffusion icon
r/StableDiffusion
‚Ä¢
2y ago
[SDXL] Simplified explanation of aesthetics score
44 upvotes ¬∑ 14 comments
[D] Want to move away from coding heavy ML but still want to complete the PhD
r/MachineLearning icon
r/MachineLearning
‚Ä¢
1y ago
[D] Want to move away from coding heavy ML but still want to complete the PhD
78 upvotes ¬∑ 53 comments
[D] How should I respond to reviewers when my model is worse than much larger models?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
4mo ago
[D] How should I respond to reviewers when my model is worse than much larger models?
55 upvotes ¬∑ 16 comments
[D] ML Engineers, what's the most annoying part of your job?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
9mo ago
[D] ML Engineers, what's the most annoying part of your job?
97 upvotes ¬∑ 118 comments
[D] What ML Concepts Do People Misunderstand the Most?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
10mo ago
[D] What ML Concepts Do People Misunderstand the Most?
215 upvotes ¬∑ 191 comments
[D] Advice needed for Fine Tuning Multimodal Language model
r/MachineLearning icon
r/MachineLearning
‚Ä¢
7d ago
[D] Advice needed for Fine Tuning Multimodal Language model
8 upvotes ¬∑ 8 comments
[D]How do you track and compare hundreds of model experiments?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
1mo ago
[D]How do you track and compare hundreds of model experiments?
32 upvotes ¬∑ 33 comments
[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
2mo ago
[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything?
92 upvotes ¬∑ 56 comments
[D] AI Engineer here- our species is already doomed.
r/MachineLearning icon
r/MachineLearning
‚Ä¢
5mo ago
[D] AI Engineer here- our species is already doomed.
45 comments
Seeking Advice for Fine-Tuning YOLOv8 to Detect Small Anomalies in Bulk Products
r/computervision icon
r/computervision
‚Ä¢
2y ago
Seeking Advice for Fine-Tuning YOLOv8 to Detect Small Anomalies in Bulk Products
r/computervision - Seeking Advice for Fine-Tuning YOLOv8 to Detect Small Anomalies in Bulk Products
8 upvotes ¬∑ 13 comments
[D] Need to train a model for a client whilst proving I never saw the data
r/MachineLearning icon
r/MachineLearning
‚Ä¢
5mo ago
[D] Need to train a model for a client whilst proving I never saw the data
54 upvotes ¬∑ 34 comments
[D] Had an AI Engineer interview recently and the startup wanted to fine-tune sub-80b parameter models for their platform, why?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
5mo ago
[D] Had an AI Engineer interview recently and the startup wanted to fine-tune sub-80b parameter models for their platform, why?
167 upvotes ¬∑ 82 comments
[P] Give me your one line of advice of machine learning code, that you have learned over years of hands on experience.
r/MachineLearning icon
r/MachineLearning
‚Ä¢
24d ago
[P] Give me your one line of advice of machine learning code, that you have learned over years of hands on experience.
91 upvotes ¬∑ 58 comments
Should we not use data-testid for element identification?
r/QualityAssurance
‚Ä¢
2y ago
Should we not use data-testid for element identification?
21 upvotes ¬∑ 57 comments
CNN issues
r/learnmachinelearning
‚Ä¢
2y ago
CNN issues
r/learnmachinelearning - CNN issues
2
50 upvotes ¬∑ 26 comments
[D] Who do you all follow for genuinely substantial ML/AI content?
r/MachineLearning icon
r/MachineLearning
‚Ä¢
5mo ago
[D] Who do you all follow for genuinely substantial ML/AI content?
154 upvotes ¬∑ 39 comments
[D] Laptop Suggestion for PhD in ML for Robotics
r/MachineLearning icon
r/MachineLearning
‚Ä¢
2mo ago
[D] Laptop Suggestion for PhD in ML for Robotics
21 comments
Related discussion
Best Yo Yos
View Post in
Êó•Êú¨Ë™û
Fran√ßais
Ti·∫øng Vi·ªát
‰∏≠Êñá (ÁπÅÈ´î)
‰∏≠Êñá (ÁÆÄ‰Ωì)
–†—É—Å—Å–∫–∏–π
Community Info Section
r/MachineLearning
Join
Machine Learning
Beginners -> /r/mlquestions or /r/learnmachinelearning , AGI -> /r/singularity, career advices -> /r/cscareerquestions, datasets -> r/datasets

Show more
Public
Top Posts
Reddit
reReddit: Top posts of August 11, 2023
Reddit
reReddit: Top posts of August 2023
Reddit
reReddit: Top posts of 2023
Reddit Rules
Privacy Policy
User Agreement
Accessibility
Reddit, Inc. ¬© 2025. All rights reserved.

Collapse Navigation
Skip to main content
Training a YOLO model for the first time : r/computervision


r/computervision
Current search is within r/computervision

Remove r/computervision filter and expand search to all of Reddit
Search in r/computervision
Log In

Expand user menu
Skip to NavigationSkip to Right Sidebar

Back
r/computervision icon
Go to computervision
r/computervision
‚Ä¢
7mo ago
techhgal

Training a YOLO model for the first time
Help: Project
I have a 10k image dataset. I want to train YOLOv8 on this dataset to detect license plates. I have never trained a model before and I have a few questions.

should I use yolov8m pr yolov8l?

should I train using Google Colab (free tier) or locally on a gpu?

following is my model.train() code.

model.train( data='/content/dataset/data.yaml',
epochs=150, imgsz=1280,
batch=16,
device=0,
workers=4,
lr0=0.001,
lrf=0.01,
optimizer='AdamW',
dropout=0.2,
warmup_epochs=5,
patience=20,
augment=True,
mixup=0.2,
mosaic=1.0,
hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,
scale=0.5,
perspective=0.0005,
flipud=0.5,
fliplr=0.5,
save=True,
save_period=10,
cos_lr=True,
project="/content/drive/MyDrive/yolo_models",
name="yolo_result" )

what parameters do I need to add or remove in this? also what should be the values of these parameters for the best results?

thanks in advance!


Upvote
17

Downvote

25
Go to comments


Share
u/Segments_ai avatar
Segments_ai
‚Ä¢
Promoted

The multi-sensor labeling platform for robotics and autonomous vehicles.
The multi-sensor labeling platform for robotics and autonomous vehicles.
The multi-sensor labeling platform for robotics and autonomous vehicles.
The multi-sensor labeling platform for robotics and autonomous vehicles.
The multi-sensor labeling platform for robotics and autonomous vehicles.
The multi-sensor labeling platform for robotics and autonomous vehicles.
The multi-sensor labeling platform for robotics and autonomous vehicles.
segments.ai
Learn More
Add your reply
Sort by:

Best

Search Comments
Expand comment search
Comments Section
Time-Bicycle5456
‚Ä¢
7mo ago
What are your inference/accuracy requirements?

If you have gpu then the answer is obvious

Start with default parameters first, then try to either tweak the hyperparameters manually or with some tuning libraries


u/Reasonable_Bat_6533 avatar
Reasonable_Bat_6533
‚Ä¢
7mo ago
Do tou mind elaborating on #1 please and its impact on his parameters?


Time-Bicycle5456
‚Ä¢
7mo ago
If you have strict real-time requirements then go with the model with less parameters, probably the n model could be fine. If you need more accuracy then one might naturally choose a model with more parameters. However, you've mentioned license plate detection, which represents just one class and thus an easier challenge probably; that said I would start with the n model (faster training, inference, experimentation) I don't understand the second part of your question. Do you mean how the hyperparameters impact the training?

Key-Mortgage-1515
‚Ä¢
7mo ago
you can also use m version with Kaggle dual GPU options for free.


u/SadAdeptness1863 avatar
SadAdeptness1863
‚Ä¢
7mo ago
Congratulations on starting a new journey!!

I would suggest:

start with smaller models(n or m)... then use the larger ones(l or x) cuz if you see the documentation of ultralytics... on the latest iteration i.e. yolov12... there is a 2.1% increase in mAP from Yolov10n and similarly for other models....

Yes.. I think kaggle is better than colab if u r on the free tier... if you've a good laptop or pc(like 8-12gb Vram) you can run it locally...

you should first start with default parameters and see how the model perfoms on your dataset then try to fine-tune it later on...

BTW... there are plenty of notebooks on kaggle you can directly clone it into your account and run it on your dataset....

u/StephaneCharette avatar
StephaneCharette
‚Ä¢
7mo ago
Another option is to use Darknet/YOLO which will give you both faster and more precise results. See DarkPlate: https://github.com/stephanecharette/DarkPlate#darkplate I have tutorials on the Darknet/YOLO YouTube channel. For example: https://www.youtube.com/watch?v=jz97_-PCxl4


u/SmartPercent177 avatar
SmartPercent177
‚Ä¢
7mo ago
Why were you downvoted?


u/StephaneCharette avatar
StephaneCharette
‚Ä¢
7mo ago
The YOLO field is controversial. A commercial company came around a few years ago, tried to take over the "YOLO" name, and released a product that was both slower and less precise than the original Darknet/YOLO.

Because they have lots of money (look at their monthly and yearly license fees) the free and fully open-source Darknet/YOLO project cannot compete with their marketing. I don't even have to name them, and I'm sure 99% of people know which corporation I'm talking about.

They keep increasing the "YOLO" version numbers. People unfortunately assume that the higher the number, the better it is. Meanwhile, Darknet/YOLO has focused on prediction quality, training speed, and inference speed. I have videos on the Darknet/YOLO YouTube channel showing training a full network in 89 seconds, and obtaining speeds of 1000 FPS for inference. And Darknet/YOLO "Slate" V4 was released a few weeks ago with support for AMD GPU, meaning you can train on AMD or NVIDIA.

Unfortunately, when I post on Reddit, my posts are usually downvoted by the fan-boys of this company.

For more information on Darknet/YOLO, see https://www.ccoderun.ca/programming/yolo_faq/

Lots of example videos in the FAQ showing the results you can expect to get from Darknet/YOLO.


1 more reply
u/Vangi avatar
Vangi
‚Ä¢
7mo ago
Ultralytics fanboys

u/imperfect_guy avatar
imperfect_guy
‚Ä¢
7mo ago
do you have a simple pip install for the darknet yolo? Many people dont have sudo access to their machines, and thats why cannot use this repo


u/StephaneCharette avatar
StephaneCharette
‚Ä¢
7mo ago
A simple install? Yes. As documented in the readme, a simple sudo dpkg --install is all that is required to install it, like any other normal Debian package. (It also builds for Windows.)

You understand "pip" is a python tool, right?

So no, there is no "pip install". If you don't have a C++ compiler or OpenCV installed as part of your linux distro, and you don't have sudo permissions, then you cannot build it. Ask the owner of the computer to install the required packages -- which are clearly stated in the readme -- and then you can build it locally for your account. It will run very well locally without having to install it for every user.


8 more replies
u/shopify avatar
shopify
‚Ä¢ Official
‚Ä¢
Promoted

Level up your business with AI-powered tools for productivity, marketing, and creativity.
Learn More
shopify.com
Thumbnail image: Level up your business with AI-powered tools for productivity, marketing, and creativity.
u/rodeee12 avatar
rodeee12
‚Ä¢
7mo ago
You just want to detect the license plate ? or You want extract text from it as well ?


techhgal
OP
‚Ä¢
7mo ago
detect and then extract the text as well


u/Crimson-knight11 avatar
Crimson-knight11
‚Ä¢
7mo ago
My suggestion will be to see if the model can detect license plates at imgsz=640. If it can then go with it as it will be a lot faster to inference and you will not run out of memory during training. Process the detection result and get the bbox coordinates as xyxyn format. This provides the bboxes with values 0-1 range. You can then extract the section from the full size image and run your text extraction logic to that. I have used this method in a different use case with very good results.

u/AdShoddy6138 avatar
AdShoddy6138
‚Ä¢
7mo ago
You have a bulk dataset i would prefer using no augmentations at first, even if required go with the default ones (but first try without it), next go with the default img size i.e. 640, the inference will be way faster and accuracy will be more or less the same (higher img size is used when dealing with small/tiny objects)


u/AdShoddy6138 avatar
AdShoddy6138
‚Ä¢
7mo ago
Also cut down the epochs to lets say 25 or modify script to stop early based on some criteria, moreover use the n or m version only the larger network brings a very little increment to accuracy that also is only claimed in the paper, going with the base version is the best choice

Related Answers Section
Related Answers
Best practices for YOLO model training
How to use YOLO for object detection
Overview of YOLO in machine learning
Summary of RF-DETR object detection method
Applications of computer vision in healthcare
New to Reddit?
Create your account and connect with a world of communities.


Continue With Phone Number
By continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy.
More posts you may like
Custom YOLO model
r/computervision icon
r/computervision
‚Ä¢
4d ago
Custom YOLO model
r/computervision - Custom YOLO model
71 upvotes ¬∑ 28 comments
Best option to run YOLO models on the go?
r/computervision icon
r/computervision
‚Ä¢
10mo ago
Best option to run YOLO models on the go?
11 upvotes ¬∑ 20 comments
YOLo v11 Retraining your custom model
r/computervision icon
r/computervision
‚Ä¢
7mo ago
YOLo v11 Retraining your custom model
13 upvotes ¬∑ 15 comments
Decrease false positives in yolo model?
r/computervision icon
r/computervision
‚Ä¢
1y ago
Decrease false positives in yolo model?
17 upvotes ¬∑ 15 comments
Is YOLO still the state-of-art for Object Detection in 2025?
r/computervision icon
r/computervision
‚Ä¢
6mo ago
Is YOLO still the state-of-art for Object Detection in 2025?
62 upvotes ¬∑ 23 comments
Is there a better alternative to YOLO from Ultralytics?
r/computervision icon
r/computervision
‚Ä¢
10mo ago
Is there a better alternative to YOLO from Ultralytics?
29 upvotes ¬∑ 24 comments
Car Damage Detection with custom trained YOLO model (https://github.com/suryaremanan/Damaged-Car-parts-prediction-using-YOLOv8/tree/main)
r/computervision icon
r/computervision
‚Ä¢
9mo ago
Car Damage Detection with custom trained YOLO model (https://github.com/suryaremanan/Damaged-Car-parts-prediction-using-YOLOv8/tree/main)
r/computervision - Car Damage Detection with custom trained YOLO model (https://github.com/suryaremanan/Damaged-Car-parts-prediction-using-YOLOv8/tree/main)
22 upvotes ¬∑ 19 comments
Made a CV model which detects Smoke and Fire suing yolov8, any feedback?
r/computervision icon
r/computervision
‚Ä¢
3d ago
Made a CV model which detects Smoke and Fire suing yolov8, any feedback?
r/computervision - Made a CV model which detects Smoke and Fire  suing yolov8, any feedback?
73 upvotes ¬∑ 17 comments
Fine-tuning a fine-tuned YOLO model?
r/computervision icon
r/computervision
‚Ä¢
7mo ago
Fine-tuning a fine-tuned YOLO model?
11 upvotes ¬∑ 6 comments
Is the Yolo model good with low resolution images?
r/computervision icon
r/computervision
‚Ä¢
9mo ago
Is the Yolo model good with low resolution images?
3 upvotes ¬∑ 5 comments
YOLO - do you crop your images before training?
r/computervision icon
r/computervision
‚Ä¢
1y ago
YOLO - do you crop your images before training?
14 upvotes ¬∑ 12 comments
Suggestions on using YOLO v12 for a small-scale project for a startup
r/computervision icon
r/computervision
‚Ä¢
8mo ago
Suggestions on using YOLO v12 for a small-scale project for a startup
10 upvotes ¬∑ 18 comments
Alternative to Ultralytics/YOLO for object classification
r/computervision icon
r/computervision
‚Ä¢
2mo ago
Alternative to Ultralytics/YOLO for object classification
21 upvotes ¬∑ 28 comments
YOLO training: How to create diverse image dataset from Videos?
r/computervision icon
r/computervision
‚Ä¢
3mo ago
YOLO training: How to create diverse image dataset from Videos?
5 upvotes ¬∑ 9 comments
YOLO downloading the yolo11n model automatically when using GPU in training
r/computervision icon
r/computervision
‚Ä¢
6mo ago
YOLO downloading the yolo11n model automatically when using GPU in training
4 upvotes ¬∑ 2 comments
YOLO alternatives for cracks detection
r/computervision icon
r/computervision
‚Ä¢
7mo ago
YOLO alternatives for cracks detection
12 upvotes ¬∑ 11 comments
YOLOv11 | Should I Use a Pre-Trained Model or Train from Scratch for this experiment?
r/computervision icon
r/computervision
‚Ä¢
8mo ago
YOLOv11 | Should I Use a Pre-Trained Model or Train from Scratch for this experiment?
3 upvotes ¬∑ 4 comments
Which camera to use for real time YOLO processing?
r/computervision icon
r/computervision
‚Ä¢
9mo ago
Which camera to use for real time YOLO processing?
5 upvotes ¬∑ 8 comments
Poker Hand Detection and Analysis using YOLO11
r/computervision icon
r/computervision
‚Ä¢
10mo ago
Poker Hand Detection and Analysis using YOLO11
r/computervision - Poker Hand Detection and Analysis using YOLO11
119 upvotes ¬∑ 12 comments
Should i use YOLO or OPENCV for face detection.
r/computervision icon
r/computervision
‚Ä¢
1mo ago
Should i use YOLO or OPENCV for face detection.
15 upvotes ¬∑ 25 comments
Yolo and sort alternatives for object tracking
r/computervision icon
r/computervision
‚Ä¢
2mo ago
Yolo and sort alternatives for object tracking
r/computervision - Yolo and sort alternatives for object tracking
28 upvotes ¬∑ 18 comments
Fine tuning a YOLO pose without bounding boxes
r/computervision icon
r/computervision
‚Ä¢
1y ago
Fine tuning a YOLO pose without bounding boxes
6 upvotes ¬∑ 7 comments
Is Object Detection with Frozen DinoV3 with YOLO head possible?
r/computervision icon
r/computervision
‚Ä¢
24d ago
Is Object Detection with Frozen DinoV3 with YOLO head possible?
5 upvotes ¬∑ 5 comments
How to merge different datasets for YOLO11 model
r/computervision icon
r/computervision
‚Ä¢
8mo ago
How to merge different datasets for YOLO11 model
5 upvotes ¬∑ 2 comments
how to achieve this overlapping effect in p5.js?
r/generative icon
r/generative
‚Ä¢
8mo ago
how to achieve this overlapping effect in p5.js?
r/generative - how to achieve this overlapping effect in p5.js?
2 upvotes ¬∑ 4 comments
Related discussion
Yo-Kai Watch Tips
View Post in
Hindi
Êó•Êú¨Ë™û
–†—É—Å—Å–∫–∏–π
Fran√ßais
Community Info Section
r/computervision
Join
Computer Vision
Computer Vision is the scientific subfield of AI concerned with developing algorithms to extract meaningful information from raw images, videos, and sensor data. This community is home to the academics and engineers both advancing and applying this interdisciplinary field, with backgrounds in computer science, machine learning, robotics, mathematics, and more. We welcome everyone from published researchers to beginners!

Show more
Public
Reddit Rules
Privacy Policy
User Agreement
Accessibility
Reddit, Inc. ¬© 2025. All rights reserved.

Collapse Navigation
Skip to main content
first time training YOLO for object detection : r/computervision


r/computervision
Current search is within r/computervision

Remove r/computervision filter and expand search to all of Reddit
Search in r/computervision
Log In

Expand user menu
Skip to NavigationSkip to Right Sidebar

Back
r/computervision icon
Go to computervision
r/computervision
‚Ä¢
1y ago
deuteriumo

first time training YOLO for object detection
Help: Project
hello i am currently working on a school project and i am confused

my goal is to detect common road objects (e.g., pedestrians, vehicles) using a car-mounted camera. i will be using a pre-trained YOLOv8s model, accuracy n inference time is important

i have the cityscapes dataset (5k images, converted to YOLO bounding boxes format) and found the Udacity Driving Dataset (19k images). I aligned the classes, but Udacity has fewer (e.g., trucks and buses are combined as "trucks").

i will also collect my own dataset bcos there are vehicles here locally that are not in the available datasets

how can i approach this properly?

should I train the model on Cityscapes first, or my own dataset directly?

can I train on Cityscapes first, then add my own dataset for the new classes?

is it okay to combine the cityscape and the udd then use it as a whole?

i read about ‚Äòfreezing‚Äô of layers but honestly i really dont get it that much. i am still learning also how and what fine tuning is.., im really new to this help is appreciated thank u so much in advance!


Upvote
4

Downvote

6
Go to comments


Share
u/vast_ai avatar
vast_ai
‚Ä¢
Promoted

Vast.ai stretches your budget further, so you can take your models farther.
View More
cloud.vast.ai
Thumbnail image: Vast.ai stretches your budget further, so you can take your models farther.
Add your reply
Sort by:

Best

Search Comments
Expand comment search
Comments Section
TEX_flip
‚Ä¢
1y ago
Important thing: if you train on dataset A, then on B, the model will forget anything about A. This is called catastrophic forgetting and it's an open problem in AI. You have to combine all the datasets and align the classes if you want to train that model


Specialist-Artist664
‚Ä¢
1y ago
isn't this also fine tune? for example my have 2 dataset one has (a)general traffic sign label(square one, circle one ex) maybe 2 or 3 class, the other one (b)specific traffic sign(speed limit, stop sign, ex ) maybe 15 or 20 classes. if i using first a before b ,will my model be more successfully than just b.


TEX_flip
‚Ä¢
1y ago
If you start from a pretrained model it's always a fine tune. But a fine tune doesn't mean that the model will be better, it means that it will take less time to train (if the data are in a similar domain). If you don't fine tune you eventually end up more or less to the same performance.

If you first train on A then on B it will take approximately the same time than just training on B to pair the performance. This is true if A and B are in the same domain, but in your case A and B are slightly different so it will take less time to train just on B.


1 more reply
u/JustSomeStuffIDid avatar
JustSomeStuffIDid
‚Ä¢
1y ago
You will have to combine the datasets including your own into a single dataset if you want to keep the classes from all of them.

u/jayemcee456 avatar
jayemcee456
‚Ä¢
1y ago
‚Ä¢
Edited 1y ago
Focus on your data set, it‚Äôs the most important part of your exercise.

Things to consider:

Balanced classes - if you have too many instances of a single class the model will learn that more and less of the other classes

Tight bounding boxes - don‚Äôt leave air gaps between the object and box

Train with the image size that you will run inference on

Consistent bounding boxes rules, ie don‚Äôt cut of parts of the object in one example and include it in another example. I‚Äôve seen this throw off models really bad

Lots of images, think 10k or more.

Combine your data sets into a single DS, retraining with sequential DS will slowly skew you away from the previous data set

Best way to think of training a model is just like fitting a line regression to a group of data points. If you change the data points the line will fit to the new DS. If you combine all the data into a single DS the regression is more likely to fit better

Spending time with the data set is the least sexy part of AI but it‚Äôs the most important. Models can learn anything you give it, even garbage, so spend your time making sure all your data is clean. Think 80/20 with respect to effort, 80% of your time will probably be just reviewing your DS

Related Answers Section
Related Answers
Best practices for training YOLO models
How to train YOLO for object detection
YOLO vs RF-DETR for object detection
How YOLO works for computer vision tasks
Using Roboflow for YOLO dataset preparation
New to Reddit?
Create your account and connect with a world of communities.


Continue With Phone Number
By continuing, you agree to our User Agreement and acknowledge that you understand the Privacy Policy.
More posts you may like
Is YOLO still the state-of-art for Object Detection in 2025?
r/computervision icon
r/computervision
‚Ä¢
6mo ago
Is YOLO still the state-of-art for Object Detection in 2025?
62 upvotes ¬∑ 23 comments
Training a YOLO model for the first time
r/computervision icon
r/computervision
‚Ä¢
7mo ago
Training a YOLO model for the first time
17 upvotes ¬∑ 25 comments
Did yall see the new SOTA realtime object detection? I just learned about it. YOLO has not been meaningfully dethroned in so long.
r/computervision icon
r/computervision
‚Ä¢
1y ago
Did yall see the new SOTA realtime object detection? I just learned about it. YOLO has not been meaningfully dethroned in so long.
155 upvotes ¬∑ 30 comments
Which is the best object detection model in yolo ( for real time human detection using Jetson nano)?
r/computervision icon
r/computervision
‚Ä¢
1y ago
Which is the best object detection model in yolo ( for real time human detection using Jetson nano)?
1 upvote ¬∑ 2 comments
How yolo or other object detection model handle images of different sizes ?
r/computervision icon
r/computervision
‚Ä¢
1y ago
How yolo or other object detection model handle images of different sizes ?
8 upvotes ¬∑ 5 comments
I compared the object detection outputs of YOLO, DETR and Fast R-CNN models. Here are my results üëá
r/computervision icon
r/computervision
‚Ä¢
10mo ago
I compared the object detection outputs of YOLO, DETR and Fast R-CNN models. Here are my results üëá
r/computervision - I compared the object detection outputs of YOLO, DETR and Fast R-CNN models. Here are my results üëá
22 upvotes ¬∑ 38 comments
Rust + YOLO: Using Tonic, Axum, and Ort for Object Detection
r/computervision icon
r/computervision
‚Ä¢
8mo ago
Rust + YOLO: Using Tonic, Axum, and Ort for Object Detection
r/computervision - Rust + YOLO: Using Tonic, Axum, and Ort for Object Detection
25 upvotes ¬∑ 17 comments
Is Object Detection with Frozen DinoV3 with YOLO head possible?
r/computervision icon
r/computervision
‚Ä¢
24d ago
Is Object Detection with Frozen DinoV3 with YOLO head possible?
5 upvotes ¬∑ 5 comments
I am trying to make Yolov8 model 1622 class object detection.
r/Ultralytics icon
r/Ultralytics
‚Ä¢
1y ago
I am trying to make Yolov8 model 1622 class object detection.
r/Ultralytics - I am trying to make Yolov8 model 1622 class object detection.
3 upvotes ¬∑ 5 comments
YOLO, Faster R-CNN and DETR Object Detection | Comparison (Clearer Predict)
r/computervision icon
r/computervision
‚Ä¢
10mo ago
YOLO, Faster R-CNN and DETR Object Detection | Comparison (Clearer Predict)
r/computervision - YOLO, Faster R-CNN and DETR Object Detection | Comparison (Clearer Predict)
29 upvotes ¬∑ 20 comments
Fun with YOLO object detection and RealSense depth powered 3D bounding boxes!
r/computervision icon
r/computervision
‚Ä¢
12d ago
Fun with YOLO object detection and RealSense depth powered 3D bounding boxes!
r/computervision - Fun with YOLO object detection and RealSense depth powered 3D bounding boxes!
169 upvotes ¬∑ 29 comments
Real-time Abandoned Object Detection using YOLOv11n!
r/computervision icon
r/computervision
‚Ä¢
1mo ago
Real-time Abandoned Object Detection using YOLOv11n!
r/computervision - Real-time Abandoned Object Detection using YOLOv11n!
773 upvotes ¬∑ 44 comments
Best object detection model for non real time applications?
r/computervision icon
r/computervision
‚Ä¢
7mo ago
Best object detection model for non real time applications?
r/computervision - Best object detection model for non real time applications?
9 upvotes ¬∑ 8 comments
Looking for Object Detection Models Similar to YOLOv11n for Commercial Use
r/computervision icon
r/computervision
‚Ä¢
8mo ago
Looking for Object Detection Models Similar to YOLOv11n for Commercial Use
15 upvotes ¬∑ 5 comments
Object detection via Yolo11 on mobile phone [Computer vision]
r/computervision icon
r/computervision
‚Ä¢
5mo ago
Object detection via Yolo11 on mobile phone [Computer vision]
r/computervision - Object detection via Yolo11 on mobile phone [Computer vision]
62 upvotes ¬∑ 29 comments
Should i use YOLO or OPENCV for face detection.
r/computervision icon
r/computervision
‚Ä¢
1mo ago
Should i use YOLO or OPENCV for face detection.
15 upvotes ¬∑ 25 comments
Picking the right camera for real-time object detection
r/computervision icon
r/computervision
‚Ä¢
8mo ago
Picking the right camera for real-time object detection
6 upvotes ¬∑ 18 comments
YOLO alternatives for cracks detection
r/computervision icon
r/computervision
‚Ä¢
7mo ago
YOLO alternatives for cracks detection
12 upvotes ¬∑ 11 comments
How to improve YOLOv11 detection on small objects?
r/computervision icon
r/computervision
‚Ä¢
18d ago
How to improve YOLOv11 detection on small objects?
r/computervision - How to improve YOLOv11 detection on small objects?
15 upvotes ¬∑ 31 comments
Best Practices for Monitoring Object Detection Models in Production ?
r/computervision icon
r/computervision
‚Ä¢
9mo ago
Best Practices for Monitoring Object Detection Models in Production ?
17 upvotes ¬∑ 23 comments
Yolo and sort alternatives for object tracking
r/computervision icon
r/computervision
‚Ä¢
2mo ago
Yolo and sort alternatives for object tracking
r/computervision - Yolo and sort alternatives for object tracking
28 upvotes ¬∑ 18 comments
Poker Hand Detection and Analysis using YOLO11
r/computervision icon
r/computervision
‚Ä¢
10mo ago
Poker Hand Detection and Analysis using YOLO11
r/computervision - Poker Hand Detection and Analysis using YOLO11
119 upvotes ¬∑ 12 comments
Alternative to Ultralytics/YOLO for object classification
r/computervision icon
r/computervision
‚Ä¢
2mo ago
Alternative to Ultralytics/YOLO for object classification
21 upvotes ¬∑ 28 comments
YOLO training: How to create diverse image dataset from Videos?
r/computervision icon
r/computervision
‚Ä¢
3mo ago
YOLO training: How to create diverse image dataset from Videos?
5 upvotes ¬∑ 9 comments
A begginer on abaqus, need help on my first dynamic analysis Ever.
r/fea
‚Ä¢
1y ago
A begginer on abaqus, need help on my first dynamic analysis Ever.
1 upvote ¬∑ 7 comments
View Post in
–†—É—Å—Å–∫–∏–π
‰∏≠Êñá (ÁÆÄ‰Ωì)
Fran√ßais
Êó•Êú¨Ë™û
Community Info Section
r/computervision
Join
Computer Vision
Computer Vision is the scientific subfield of AI concerned with developing algorithms to extract meaningful information from raw images, videos, and sensor data. This community is home to the academics and engineers both advancing and applying this interdisciplinary field, with backgrounds in computer science, machine learning, robotics, mathematics, and more. We welcome everyone from published researchers to beginners!

Show more
Public
Top Posts
Reddit
reReddit: Top posts of September 25, 2024
Reddit
reReddit: Top posts of September 2024
Reddit
reReddit: Top posts of 2024
Reddit Rules
Privacy Policy
User Agreement
Accessibility
Reddit, Inc. ¬© 2025. All rights reserved.

Collapse Navigation