Transcript


Search in video
Introduction
0:13
Hey everyone, today we're going to walk through  training a YOLO model inside Google Colab. I'll  
0:18
go through the process of gathering a data set  training the model in Colab and then running it  
0:23
locally on your PC with a custom Python script.  We'll show how to train a candy detection model  
0:29
that can detect and identify different popular  types of candies. You can follow along and use  
0:34
my candy data set as an example or you can  bring your own data set. We'll work inside  
0:39
a Google Colab notebook I wrote for training  YOLO models. It works with YOLO11, YOLOv8,  
0:44
and YOLOv5. if you want to train your models  on a local PC instead, I wrote an article that  
0:49
shows how to do so. You can find it in the video  description below. All right let's get started! 
Gather Images
0:57
The first step to training a custom model  is to create a data set. A good starting  
1:02
point for a proof of concept model  is 100 to 200 images. The training  
1:06
images should show the objects in a  variety of backgrounds, perspectives,  
1:10
and lighting conditions that are similar to what  the model will see in the actual application.
1:17
The best way to build a data set is to  manually take pictures of your objects  
1:20
in a variety of conditions.  For my candy detection model,  
1:24
I took pictures of candy in various locations  around my house using my phone and a webcam.  
1:30
You can also use online sources like Roboflow  Universe, Kaggle, or Google's Open Images V7 data  
1:36
set to find pre-labeled images of your objects. For tips and best practices on building an  
1:42
effective data set see my video on how to capture  and label training data. Once you finished taking  
1:49
pictures or downloading them from the internet,  move all of the images to a single folder on your  
1:54
computer. I'll create a folder called YOLO in  my Documents folder, and another folder inside  
2:02
that called candy-images. Then I'll move all the  photos from my phone into the candy images folder.
2:12
Now that you've got your training images gathered  and saved on your PC, it's time to label them.  
Label Images
2:17
We'll use an open-source tool called Label Studio  to label our images. I like Label Studio because  
2:23
it offers a simple workflow while providing  advanced features like automated labeling. Best  
2:28
of all it's free and open source. Because I like  Label Studio, I reached out to them and asked if  
2:34
they would sponsor this video and they agreed to  it. So, this video is sponsored by HumanSignal,  
2:39
the team that developed Label Studio. If you  want to learn more about their tool, click the  
2:43
first link in the video description below. To install Label Studio we first need  
2:49
to install Anaconda, which is a tool for  creating and managing Python environments.  
2:54
Go to anaconda.com/download and click the  skip registration button and then download  
2:59
download the package for your OS. When it's  finished downloading, run the installer and  
3:04
click through the installation steps you can  use the default options for installation.
3:12
Once it's installed, run Anaconda Prompt by  searching for it on the Start bar and clicking  
3:17
on it. This opens a command window that we'll  run our commands from. Let's create a new Python  
3:22
environment for installing Label Studio and the  other libraries we need for running YOLO models. 
3:27
Create a new environment by issuing "conda  create --name yolo-env1 python=3.12" .
3:41
When it's finished creating the environment  activate it by issuing "conda activate yolo-env1".
3:49
Now we can install label Studio by  issuing "pip install label-studio".  
3:55
Depending on your internet connection,  it'll take a few minutes to install.
4:03
Once it's installed start Label Studio  by issuing "label-studio start".
4:16
After a few seconds, Label Studio will start in  a new web browser window. Label Studio runs on  
4:21
a server that's hosted locally on your computer,  and you interact with it through your web browser.  
4:26
Even though it's in a web browser, all the  accounts files and label data you create are  
4:30
stored entirely on your computer and not on the  web (unless you want to set it up that way). To  
4:36
start with, you'll need to make an account for  your local Label Studio server. Again, this is  
4:40
all hosted on your local computer so you aren't  actually creating a web account. Click the sign  
4:46
up link and then enter a fake email address and  password. I'll use fake@fake.com and fakepassword.  
4:54
Click create account to continue. You'll enter  the project page for Label Studio click the Create  
5:02
Project button to start a new project. Enter a  project name like candy images and then click  
5:08
the data import tab. On this screen, you can  drag and drop images from your folder on your  
5:13
computer into the web browser to import them. If  you try and import more than a 100 images at once,  
5:19
you'll get an error about exceeding a max  number of files. But, you can get around it  
5:23
by just making sure you only import 100 images at  a time, and then going to the next 100 and so on.
5:34
If you have more than a thousand images to  label, it's strongly recommended to link a  
5:38
local storage folder or cloud account to the  project. See the Label Studio documentation on  
5:44
adding project storage for more information on  how to do so. Finally, click the Labeling Setup  
5:51
tab and then click the object detection option.  This opens a configuration page for the labeling  
5:57
interface. To start with, delete the airplane  and car labels. In the Add Label Names window,  
6:03
type your own class names. I'm labeling  pictures of candy, so I'll add each of the  
6:07
candy brands like AirHeads, Nerds, Skittles,  and so on with a enter after each line. Once  
6:16
all the classes are typed out, click Add.  Click Save to finish creating the project.
6:27
Okay, we're ready to start labeling! Click  the first image in the list to open the  
6:31
labeling interface. Create a new bounding  box by clicking a class at the bottom of  
6:36
the window and clicking a dragging a  box around the object in the image,  
6:40
like this Starburst for example. You can click  the box and adjust it as needed. You can also  
6:46
use class hotkeys to create a new box. So for  example I can press one to create a bounding  
6:52
box for the Airheads in this in this image, and  I can press eight to label the peanut M&M's.
7:01
Once you've labeled all the objects in the image,  click Submit and then click the next image in the  
7:05
list. On this image, let's zoom in a little to  draw tighter bounding boxes around the objects.  
7:12
We'll start with these gummy worms and get a  nice tight bounding box. Okay Three Musketeers,  
7:19
Snickers, and Milky Way. Okay, and  then when you're done with the image,  
7:25
click Submit to save the labels. Continue working through each image  
7:31
if you need tips on best practices for labeling  images, check out my object detection labeling  
7:36
video. Labeling takes a while, so grab a drink  and put on some music while you're working.
7:46
Once you've labeled all your images,  it's time to export them. Click the  
7:49
project name at the top of the window and  then click the Export button. Scroll down  
7:55
and select the YOLO export option and then  click Export. Label Studio will package  
8:01
the images and label data into a zip file  and download it through your web browser.
8:09
The files are organized in a directory structure  required for training YOLO models, with the images  
8:14
and an images folder and the labels in a labels  folder. It also generates a classes.txt labelmap  
8:22
file which lists all the classes in your data  set. Let's move that downloaded file back  
8:29
into the YOLO folder in Documents and rename  it to data.zip. We'll upload this file to the  
8:37
Google Colab training notebook later. Now, we're ready to start training our  
8:41
YOLO model. We'll train our model in Google  Colab, a service from Google that allows  
Train YOLO11 Model
8:48
you to write and run Python code in your web  browser on a free GPU. Click the Colab link  
8:53
in the video description below to get started. This is the Colab notebook where we'll upload  
9:00
our data set and train our custom YOLO  model. First, go to Runtime and Change  
9:04
Runtime Type and make sure that a T4 GPU  is selected. Click Save then click Connect.
9:13
Once it's connected, scroll down and click  play on this !nvidia-smi block. You can just  
9:18
click Run Anyway on that window that appears.  Verify that it reports a Tesla T4 or similar  
9:24
GPU is available. We've already gathered  our images so, we can scroll past Step 1. 
9:30
Next, we'll upload our data.zip file into this  Colab instance. Click the folder icon on the left  
9:36
and then drag and drop your data.zip file in into  the sidebar. This little blue circle will show  
9:43
the upload progress. You can also upload data to  Google Drive and copy it from there, which goes  
9:48
a little faster. See these instructions on how to  do so. If you got your data set from a different  
9:55
source like Roboflow Universe, make sure it has  the correct folder structure shown here. Again,  
10:00
all the images are in the images folder, the  labels are in the labels folder, and there should  
10:05
be a classes.txt file that lists every class. You  can just ignore this notes file. You may have to  
10:12
move the files around or create the classes  file manually to match this data structure.
10:19
If you just want to test the training  process on a pre-made data set,  
10:22
you can use my candy data set or my coin data  set. Click this block here to download it.  
10:30
Next unzip the data set by  clicking this code block.
10:36
Now that the files are set up, we're ready to  split them into train and validation folders.  
10:41
The train folder will contain the actual  images that are used to train the model,  
10:44
while the validation folder has images that are  used to check the model's performance after each  
10:49
training epoch. I wrote a Python script that  automatically creates the folder structure  
10:54
required for training and will randomly move  90% of the data set to the train folder and  
10:58
10% to the validation folder. Run this code  block to download and execute the script.
11:07
When script finishes, all the images  will be in a data folder where the  
11:11
train and validation images and labels are  each in their respective folders. Next,  
11:18
we'll install the Ultralytics library,  which is the Python library we'll be  
11:21
using to train the YOLO model. Click  play to run the pip install command.
11:29
Before we can run training, we need  to create a training configuration  
11:33
file. This file sets the location of the  train and validation folders and defines  
11:37
the model's classes. Here's an example of  what it looks like. Run the next code block  
11:44
to automatically generate the data.yaml  config file. When it finishes running,  
11:49
it'll display the contents of the file.  Double-check that everything looks correct.
11:55
Okay, we're finally ready to run training! But  there's a few important parameters we need to  
12:00
decide on first. We need to pick which YOLO  model we want to use. There's a variety of  
12:05
YOLO architectures and model sizes. YOLO11 is the  latest architecture and has the best performance,  
12:11
but YOLOv8 and YOLOv5 are more mature and have  broader support on hardware platforms. Model sizes  
12:17
range from nano to extra large. I made a quick  YouTube video comparing YOLO model performance  
12:24
on a Raspberry Pi 5 and a laptop equipped with an  RTX 4050 GPU. You can watch this video to get a  
12:31
sense of the frame rates and accuracy you'll  get with different YOLO models. Generally,  
12:36
you want to choose a model that's small enough to  meet the speed requirements for your application,  
12:40
while still being accurate enough to detect your  objects. If you aren't sure which model to use,  
12:46
the YOLO11s model is a good starting point. For  my candy model, I'll use YOLO11s. Next, the number  
12:54
of epochs sets how long the model will train  for. If your data set has less than 200 images,  
12:59
a good starting point is 60 epochs. If  your data set has more than 200 images,  
13:03
a good starting point is 40 epochs. Resolution is  another parameter we can tweak, but generally it's  
13:09
safe to use the default resolution of 640x 640.  However, if you want your model to run faster or  
13:15
if you know you'll be working with low resolution  images, try using a lower resolution like 480x480.
13:24
With the parameters decided, it's time to run  training. I've got the notebook set up to train  
13:28
a yolo11s model for 60 epochs at a 640x 640  resolution. You can change the parameters  
13:35
of this command if you want to try something  different. Click play to start training. The  
13:41
training algorithm will parse the images  in the train and validation directories  
13:45
and then start training the model. At the end  of each training epoch, the script runs the  
13:50
model on the validation data set and reports the  resulting mAP, precision, and recall. mAP is a  
13:57
general measure of model accuracy. As training  continues, the mAP should generally increase  
14:03
with each epoch. Training will end once it goes  through the specified number of epochs. Depending  
14:10
on how big your data set is and the number of  epochs used, training can take anywhere from 5  
14:15
minutes to a few hours. Be careful though, because  the Colab instance will time out and disconnect  
14:20
after about 5 hours. If you want more time  for training, consider upgrading to Colab Pro.
14:32
When training finishes, the best  model weights will be saved in  
14:36
runs/detect/train/weights/best.pt. The results.png  file shows how metrics like loss, precision,  
14:48
recall, and mAP progressed over each training  epoch. Let's make that a little bigger. Generally,  
14:54
mAP should increase and then begin to  flatten out by the end of training.
15:03
Now that the model's finished training,  let's test it! Run this first code block  
15:07
to perform inference on all the  images in the validation folder.
15:14
Then, run the next code block to display the  results for the first 10 images. The model  
15:19
should draw a box around each object of interest  in each image. My model is doing a pretty good  
15:24
job of identifying each of the types of candy in  these images. Looks like it's getting them mostly  
15:31
right. If your model isn't very accurate, there's  a couple things you can try. First, double-check  
15:37
your data set to make sure there's no label  errors. You can also try increasing the number of  
15:42
training epochs or using a larger model. Okay, now  we've got a trained model, so what can we do with  
15:49
it? Drawing boxes on images is great, but it isn't  very useful in itself. It's also not very helpful  
15:55
to just run the model inside a Colab notebook, so  let's download and run it locally on our computer  
16:01
with a custom Python script. First zip and  download the trained model by running this code  
16:07
block. It will rename the model to my_model.pt,  move it to a folder named my_model, and zip it.  
16:14
It also zips the training results in case you want  to reference them later. Then you can download the  
16:20
my_model.zip file from the sidebar. Now we'll  walk through instructions for setting it up on  
Deploy Model on PC
16:26
your PC. Once the download is finished, move the  downloaded zip file back into the YOLO folder that  
16:34
we were working in and extract it. We'll work  inside this extracted folder to run our model.
16:46
Go back to the Anaconda Prompt window. If it's  still running Label Studio, press CTRL+C a couple  
16:52
times to stop it. Or, if you've already closed  the window, you can re-open it, re-activate the  
17:00
environment by issuing "conda activate yolo-env1"  and then copy the path to the my_model folder and  
17:10
do "cd" and enter the path to the folder to move  into it. We'll use the Ultralytics library to run  
17:18
inference with our model. Install it by issuing  "pip install ultralytics". This also installs  
17:25
other important libraries like OpenCV-Python,  Numpy, and PyTorch, so it may take a few minutes.
17:36
If you have an NVIDIA GPU, you can install the GPU  enabled version of PyTorch by going to the PyTorch  
17:42
Get Started page at pytorch.or/get-started-locally  and scrolling down, selecting your operating  
17:50
system, and the latest version of CUDA. Copy the  installation command and paste it in your Anaconda  
17:58
Prompt window. Make sure to add "--upgrade"  after install so that it upgrades the existing  
18:06
installation. Hit enter to run the command. This  automatically installs the CUDA and cuDNN drivers  
18:13
needed for running inference on GPU. They're  pretty big drivers, so it may take a while.
18:23
Now we're ready to run the model! I wrote a custom  Python script that shows how to load the model,  
18:29
run inference on an image, and parse  the inference results. The intent for  
18:33
this script is to show you how to work  with Ultralytics YOLO models in Python,  
18:37
and it can be used as a starting point  for other applications. Download the  
18:41
script by issuing "curl -o yolo_detect.py  https://www.ejtech.io/yolo_detect.py" .
18:58
You can run the script by entering the path to  your model and an image source. For example,  
19:03
to run inference on a USB camera  at 1280x720 resolution, I can issue  
19:08
"python yolo_detect.py --model my_model.pt  --source usb0 --resolution 1280x720".
19:24
All right, so your camera view will appear and  the model will draw boxes around the different  
19:29
objects of interest in each frame. So as you  can see my, candy model doesn't do too bad  
19:35
at detecting different types of candy here. And  since it's a YOLO11s model, it runs pretty fast  
19:40
on my NVIDIA 4050 graphics card. So, not  bad! You can press Q to quit the program.
19:52
You can also run the model on a video file  image or folder of images. For example,  
19:57
I'll move a video file into  the folder and run the script  
20:00
on it by issuing "python yolo_detect.py  --model my_model.pt --source test1.mov" .
20:15
Again, a window will appear  showing detection results.
20:23
To see a full description of how to use  yolo_detect.py, see the readme file in  
20:28
the Train-YOLO-Models GitHub repository  linked in the video description below.
Conclusion
20:35
Okay, congratulations! You successfully  trained a YOLO model. Next, see if you  
20:40
can extend the application beyond just  drawing boxes on the screen and counting  
20:44
the number of objects. As an example, I  created a candy calorie counter app that  
20:50
will tell you the number of calories and the  amount of sugar and a handful of candy. You  
20:54
can check out the source code for it in the  video description below. I'll be adding more  
20:59
examples to the GitHub repository that show  cool ways you can use these YOLO models. Also,  
21:05
keep an eye on my channel for more guides, like  a guide showing how to run YOLO models on the  
21:09
Raspberry Pi. Thanks so much for watching,  and as always good luck with your projects!
21:25
[Music]
Transcript


Search in video
0:00
we have a new member in the YOLO family
0:02
we have YOLO V12 so in this video here
0:05
we're going to see how we can train on a
0:07
custom data set run inference and so on
0:09
we're basically just going to go through
0:10
a guide a Google cab notebook we're
0:12
going to talk about some of the new
0:13
things with yolo V2 and also compare to
0:16
the previous models so just jump
0:18
straight into it I've jumped into the
0:20
GitHub repo here for YOLO V12 this is
0:22
not from Alo litics but is it is based
0:25
on Ultra ltic so still the same license
0:28
everything you still can use it from the
0:30
allytics package and all that if we
0:32
scroll a bit further down we can see we
0:33
have these comparisons where we have the
0:36
Benchmark results on the Coco data set
0:39
so if we're taking a look at it we can
0:40
see that this is act like a new
0:41
state-ofthe-art model and just from The
0:43
Benchmark results here it act like looks
0:46
significantly better compared to the
0:48
other YOLO models let's compare to v11
0:50
and V V8 which is the one that I have
0:52
the best performance on so we can see
0:55
just that compared with Yol V 8 both 11
0:58
and also 12 is significant iFly faster
1:01
like significantly faster almost like
1:03
two times faster running inference we
1:06
get comparable results and so on but
1:08
again we get a bit faster INF speed with
1:11
the new Yol 12 model but again we also
1:13
get a bit better accuracy there's one
1:16
downside with the new model and for
1:18
right now it acts like requir GPU
1:21
because it has this attention Centric
1:23
real-time optic detectors so using
1:25
something called flash attention uh
1:27
which is actually just a technique from
1:29
the last language model and so on the
1:30
attention mechanism where it basically
1:32
just learns what to pay attention to in
1:34
the image so right now it's only support
1:36
on on GPU I'm not sure when it will be
1:39
available on
1:40
CPU but yeah again right now you can use
1:42
it on GPU Google collab and so on as
1:44
we're going to run it through you can
1:46
see the paper you can see they also have
1:47
a hog and face space here it has some
1:50
different updates we're going to run
1:51
through and use the robo flow a tutorial
1:54
that we have right now we're going to
1:55
use cotton data set and set up a whole
1:57
pipeline it will also be supported with
1:59
also fell soon and then we can just use
2:01
that directly out of the box from their
2:03
framework is still based on the
2:04
framework now so it will still be the
2:06
exact same thing but if you just updated
2:08
from Alo litic own repository then it's
2:11
not supported as we speak right now but
2:13
will be in the next few days we have the
2:16
Nano small medium large and extra large
2:18
model the exact same one as all the
2:20
other YOLO models that we have here you
2:22
can see the installation steps you can
2:23
also basically just go in and do the
2:24
installation steps that I'm going to
2:26
show you in a second we can run
2:27
validation training prediction X orted
2:30
and we also have a demo with an app from
2:33
gradio so this is pretty much everything
2:35
that I wanted to show you guys let's now
2:36
jump inside at Google collab notebook
2:38
and get to do some real work first all
2:42
here you can use the Fe GPU resources on
2:45
Google collab right now I'm using the
2:46
guide from roofflat definitely check
2:48
them out as well they have the roofflat
2:50
block GitHub repo and then we also have
2:52
the Google cab notebook here they read
2:55
about they basically just write about a
2:57
lot of different stuff here with the
2:59
description
3:00
what it is what are the different
3:01
comparisons and so on they also
3:03
compareed to other iterations where we
3:05
have this window attention crisscross
3:07
attention actual attention so this is
3:08
some of new stuff coming into the YOLO
3:11
V2 model that uses the tension mechanism
3:14
so basically just has some mechanism
3:16
some features that go in and learns
3:18
basically just as the human so we pay
3:19
attention to something like we're not
3:21
looking at every single thing when we
3:23
look with our eyes we pay specific
3:25
attention to specific things so right
3:27
now I'm looking into the camera so my
3:29
eyes more Focus or I pay more attention
3:31
to the camera compared to all the other
3:33
things in my surroundings so this is
3:36
what you can go and read about you can
3:37
also read about the paper and so on and
3:40
we can see the graphs here as well first
3:42
of all you can set up your rlow API key
3:44
it's going to be very easy once it
3:46
integrated into Al litics and so on as
3:48
well but we need a TPU if you go inside
3:50
the runtime you can change the runtime
3:52
type and just use one of the free GPU
3:54
resources from Google collab I've
3:57
already put my API key in if you go into
3:59
inside the API key tab here to the left
4:02
you can just put it in there and you can
4:03
extract it in Google collab in this way
4:06
I already ran a command here because I'm
4:08
running some been running some test and
4:09
so on I can just rerun here we can run
4:12
Nvidia SMI I'm also going to do a
4:14
comparison where we're going to compare
4:16
all the different versions so v11 V12 V8
4:20
and so on where we have them side by
4:21
side to see if it actually like achieves
4:24
better performance because one thing is
4:26
The Benchmark result we need to test out
4:28
our models in real world
4:30
situations different types of data sets
4:32
and so on but Robo flow is also very
4:35
good for
4:36
that right now we can then just import
4:39
our home directory then we set up our
4:41
dependencies so from the YOLO WE 12 get
4:44
our repo that we was just in we're going
4:46
to just basically just P install that
4:48
going to use rlow and also supervision
4:51
for our annotations the other thing here
4:53
is Flash attention so we need this flash
4:55
detention here and this is why it's only
4:58
working on dpu for now
5:00
and it's used for accelerating attention
5:02
based computation via optimized Cuda
5:04
kernel so this is why it's running
5:06
faster compared to the YOLO 11 model and
5:09
the other ones as well we pip install it
5:12
you have to run this one here we go in
5:14
get an example from RO with uh with the
5:17
dog example you guys have probably seen
5:19
that before somewhere so this is how we
5:21
can run inference so import You2 from
5:24
allytics we import YOLO so everything is
5:26
based on the altic framework so you can
5:28
run the exact same commands exact same
5:30
argument and so on it's just a model
5:31
that is swapped out so it will also be
5:33
available in allytics in no time right
5:36
now let's just run it let's see the
5:38
results so we get our results from our
5:40
model we do our inference we take our
5:42
predictions convert them into um
5:45
supervision class we have our box
5:47
annotator label annotator we can copy
5:49
our image just the original image and we
5:51
annotate our bounding boxes and also our
5:54
labels then we plot our image and we get
5:57
our predictions here so we have the
5:59
person here here pu from rlow his
6:02
popular dog here handbag backpack and
6:04
also a car here in the background so
6:06
it's really nice that it's picking up
6:07
the car in the background as well we get
6:10
a handbag here probably because of like
6:12
this this one here wouldn't really say
6:15
that that's a handbag but again we can
6:16
also go in and use a confidence score
6:19
threshold now we can go and take a data
6:21
set from the roof flow Universe I'm just
6:24
going to open this one here this the
6:25
example that they're using the notebook
6:27
you can go in and use whatever you can
6:29
have your own projects as well if you go
6:31
inside your project tab label your
6:33
custom data sets I have whole pipelines
6:35
whole tutorials and so on covering how
6:37
we can generate our own data set how can
6:39
we connect to a camera how can we
6:40
connect to a video and then just extract
6:42
frames from that then take those images
6:44
upload it into Ro flow platform do to
6:47
annotations they have a lot of different
6:48
cool also annotation tools as well I
6:50
have a very popular video on that topic
6:53
so definitely to check that out if you
6:54
don't want to do like manual labeling we
6:56
just throw in our data set prompt what
6:59
we wanted to take in our image and it's
7:01
going to come up with the bounding boxes
7:04
so rlow very cool platform setting up
7:06
the whole computer vision pipeline we
7:08
can train our models in here we can
7:10
export then we can do whatever and then
7:11
we can also use the inference SDK so we
7:14
have the whole computer vision pipeline
7:16
so we have a data set here we can
7:18
basically just go up and and grab the
7:20
the URL and then download that model
7:23
automatically but let's now go inside
7:25
the universe here so I'll just going to
7:27
search for any project we can also just
7:29
go back here so we go inside the rooff
7:32
Flow app and we can go back into uh the
7:35
universe so universe is down here at the
7:37
bottom once you are at your main page
7:40
there is all the different projects that
7:41
we have if you want to do self-driving
7:43
gaming manufacturing Agriculture and
7:45
also Sports you can search for specific
7:48
projects up here to have tons of public
7:50
available data sets that you can grab
7:52
out of the box let's scroll down a bit
7:55
here I think this PCB holes is a pretty
7:57
nice one it has around like images we
8:00
can go inside our data set we have
8:02
already trained models which is doing
8:04
very good at solving this specific
8:06
problem if we go inside our data set we
8:09
just grab the URL there we go we go back
8:12
into
8:14
here need to export it into ov8 format
8:17
again this is just also L
8:19
format and then we're just going to
8:21
download the data set this is how easy
8:23
it is to pull any data set from the roof
8:26
flow platform from the universe you can
8:28
also grab your own data in the exact
8:30
same way so if you go over here to the
8:33
left we have our PCB holds 4 test train
8:36
and validation split and we also have
8:37
our data yl file so let's see what
8:40
classes we have we basically just have
8:42
one class which is going to detect
8:45
missing
8:47
holes now we should have everything we
8:49
can just set our different path to our
8:52
data yl file our test train and
8:54
validation split this is pretty much
8:56
just everything that we have to do
8:58
missing holes so this is our data yl
9:01
file now we have everything we can go in
9:03
and train our models in the exact same
9:05
way as any of the other YOLO
9:08
versions there we go we have a model we
9:10
create an instance y v 12 small you can
9:13
also just specify n medium large and so
9:16
on for pulling the different models
9:18
let's just go in and use a a small model
9:21
this is actually like going to um this
9:23
is act like going to change this is
9:25
actually like going to to to train them
9:26
from scratch but you can replace them by
9:28
any model here so this is if you want to
9:31
just if you have a very large Data
9:33
Center and you want to train your own
9:34
custom models with random initialized
9:36
weights we can do it this way here but
9:38
we just want to fine-tune an already
9:40
pre-trained model so the pre-trained
9:42
model was how we was running inference
9:44
up here at the
9:47
top so now can just go in and hit start
9:51
train if you want to run inference you
9:53
can do it in the exact same way here I'm
9:54
just going to grab the code nipp
9:57
it there we go we can pull in a new
9:59
model and then instead of train we can
10:02
just call predict we can throw in our
10:05
video so this is what I did before just
10:07
did some runs so I can go and do
10:09
comparisons of YOLO YOLO 11 YOLO V2 on
10:12
different videos so predict here we can
10:15
just specify um our road so we copy a
10:20
path there we go we can specify that we
10:23
want to save it we set save set that
10:26
equal to true and then it's going to
10:27
save a video file so while it's act like
10:30
training here we can see that now we get
10:32
some different predictions Epoch one to
10:34
EP 100 so we're training for 100 ugs
10:37
this is probably a bit too much let's
10:38
just cancel it and only go for
10:40
50s because we only have 100 images in
10:43
this data set there we go while training
10:47
here let's grab up some videos that I
10:49
just ran through so I have this example
10:52
here this is a video covering the YOLO
10:54
11 model or the 12 the new 12 model and
10:57
this is the Nano version of of it then
11:00
also did the exact same thing with the
11:02
YOLO 11
11:07
model and we can just run the results
11:09
here on the sideline so just initially
11:12
it looks very similar when I'm looking
11:14
at the cars maybe we get some more
11:16
detections from the 11 model but again
11:20
it should run faster with optimized Cuda
11:22
kernels for different dpus and so on the
11:24
good thing about the other y models is
11:26
that they run on CPU directly out of the
11:28
box
11:29
yeah I think it's it's very similar
11:31
results here at least for the nanom
11:33
models I was also running through the
11:35
the medium model just to see if we get
11:36
more predictions so this one here is the
11:39
YOLO 11
11:41
model let's grab this one this is the
11:43
YOLO 11 model now we start to get some
11:45
cars up here in the background and also
11:47
a bit further away so once they start to
11:50
come around this region here and this
11:52
region we actually have detections with
11:54
the Nano model or this is the medium
11:57
model let me close all these these and
12:00
let's grab the last one here which is
12:01
the new 12 model with medium size so I
12:05
think the medium size it's it's
12:06
definitely better with the YOLO 11 model
12:09
definitely and even like YOLO V8 might
12:11
be better than YOLO V 11 at least on
12:13
smaller
12:15
objects so just initially here might be
12:17
that it's a bit faster but just because
12:19
we have a new version in the YOLO family
12:21
doesn't mean that that's the best model
12:24
especially like if they just run it on
12:26
the benchmarks because it depends on the
12:27
gpus that they using
12:29
the Cuda versions and all that because
12:31
they're using this flash tension which
12:32
is optimized for Cuda kernels so I'm not
12:35
really seeing better results with the
12:36
new yolo2 model let's see we're probably
12:39
going to do a pretty good job on the
12:40
training because it's a relative easy
12:43
data set but make sure that you test out
12:45
your data you test out your projects on
12:47
various different models just from the
12:50
initial results that I'm seeing here
12:51
because we we can't use like
12:53
CPU it doesn't look as good with the
12:56
larger models I I think the yolow 11 and
12:59
the YOLO V8 model is still is still like
13:01
better compared to this one here have to
13:02
play around with it like way more use it
13:04
for different projects different use
13:06
cases and so on but initially here we
13:08
might get some speed UPS um I've been
13:10
running some different speed UPS maybe
13:11
need some more optimization and so on
13:13
but I'm not really seeing it's that much
13:14
faster compared to the Yello 11 model so
13:18
we'll see in the future when I get to
13:19
run more test but this is how we can
13:21
train the model and it's very important
13:23
that you train it it's so easy to train
13:25
like it's just a single line of code and
13:27
if you want to train it on a YOLO 11
13:29
model we basically just copy paste the
13:31
exact same thing paste it in here we
13:34
change it this is only called Jolo 11 we
13:36
have a small model you can run it here
13:38
and then you can run like three
13:39
different models you go grab a coffee
13:43
probably more than one coffee because it
13:45
would probably take an hour or something
13:46
like that so you can grab a few coffees
13:48
you can come back and then you have
13:49
trained trained a ton of different
13:52
models basically so that's very
13:54
important now it's done training we get
13:56
our evaluation so Precision recall
13:59
pretty much one me position is also
14:01
pretty much one so this is almost like a
14:03
perfect model very good to use directly
14:06
out of the box so now we can also go in
14:09
and do evaluation could be that we have
14:11
more classes and so on then it makes
14:13
sense to go in and look at the confusion
14:14
Matrix so it's not going to like create
14:16
one for that right now if you go inside
14:18
our run folder or it
14:21
is the reason why I got it is because I
14:24
have like train two we didn't finish the
14:26
first training we converted it to uh
14:29
Epoch 5050 or 50 EPO so yeah this is our
14:32
results we get our train we get our
14:34
train to if you want to grab the custom
14:36
model of your train we just go in here
14:39
right click download the best. PT model
14:41
and you can use it in your own custom
14:43
Pyon script application and project have
14:47
our results PNG as always we can see the
14:49
training metric so the recall Precision
14:51
how it's going up over the number of EPO
14:53
and we can see here that it's pretty
14:55
much enough with 50 EPO some of the
14:57
losses are still going down but the
14:59
Precision recall meor precision and so
15:01
on is pretty much perfect so this is
15:04
very important to run through I'll just
15:06
run through it here if you use the C of
15:08
notebook it'll be down in the
15:09
description so you can just grab it run
15:11
these blocks of code and you can create
15:12
like a computer vision model use rlow
15:15
for setting up the whole pipeline the
15:17
data and so on you don't really need to
15:19
write any code it's just copy paste and
15:21
run through some commands now we have
15:24
computer vision we have missing hole
15:26
let's go in and grab the best trained
15:28
model so train to
15:31
there we go you can see the mean add
15:33
positions pretty much perfect we can
15:35
also plot it directly when we're using
15:39
um supervision metrics so this is very
15:41
good if you're creating report if you're
15:43
student if you're researcher and so on
15:46
supervision like it's very good for
15:47
visualization plotting graphics and all
15:52
that now you can run inference with the
15:54
best Mar model again I need to change it
15:56
out here you can also just if you
15:57
download to your own local machine just
15:59
drop it into your directory and you can
16:01
just specify the relative path as
16:03
well there we go and now let's just run
16:06
through some
16:07
examples we have a missing hole here
16:10
detected perfectly but again we also
16:12
expect that with the model results that
16:14
we are seeing so this is how easy it is
16:16
again we can just run it again because
16:18
it's going to grab a random image or at
16:20
least it should yep it is so yeah we're
16:23
basically just running through random
16:24
images
16:25
here and detecting missing holes here we
16:28
have two missing holes holds in our PCB
16:31
soldering this is how easy it is to use
16:33
the new YOLO V12 model it's the new
16:36
family member but again it's just as
16:39
easy training any of the other models if
16:41
not easier because you can use CPU you
16:44
don't have all these different things
16:45
that you need to take into
16:46
account just wanted to cover this new
16:48
video here going to cover it more we
16:50
need to test it on some more real world
16:52
examples see how does it act like work
16:54
is it better than YOLO 11 just because
16:56
we have a new version doesn't just is
16:59
not just equal to a better model even if
17:02
it's showing it on the Benchmark result
17:04
sometimes I'm seeing like very good
17:05
Benchmark results but when you get it
17:07
into Run in real world projects it
17:09
doesn't even get anywhere near those
17:12
results hope you learned ton of this
17:14
video here definitely going to check it
17:16
out we'll have it available with
17:17
allytics you can use Robo flow very cool
17:19
platform to have the hole in there I'm
17:21
creating a bunch of tutorials covering
17:23
both platforms as well this is what you
17:25
need if you're a computer vision
17:26
engineer or not even a computer vision
17:29
year if you just want to create computer
17:30
vision projects do detections with
17:32
cameras videos and so on hope you
17:35
learned on hope you see you guys in one
17:37
of the upcoming videos until then Happy
17:40
training
Transcript


Search in video
Intro
0:00
hey my name is Felipe and welcome to my channel  in this video we are going to train an object   detector using yolo V8 and I'm going to walk you  step by step through the entire process from how  
0:11
to collect the data you need in order to train an  object detector how to annotate the data using a  
0:16
computer vision annotation tool how to structure  the data into the exact format you need in order  
0:21
to use yolo V8, how to do the training and  I'm going to show you two different ways to   do it; from your local environment and also from  a Google collab and how to test the performance  
0:30
ofthea model you trained so this is going to be  a super comprehensive step-by-step guide of   everything you need to know in order to train  an object detector using yolo v8 on your own  
0:40
custom data set so let's get started so let's  start with this tutorial let's start with this  
Data collection
0:45
process and the first thing we need to do is to  collect data the data collection is the first step  
0:50
in this process remember that if you want to train  an object detector or any type of machine learning  
0:56
model you definitely need data, the algorithm, the specific algorithm you're going to use in this  
1:02
case yolo V8 is very very important but the data  is as important as the algorithm if you don't have  
1:08
data you cannot train any machine learning model  that's very important so let me show you the data  
1:14
I am going to use in this process these are some  images I have   downloaded and which I'm going to use in order  to train this object detector and let me show  
1:24
you a few of them these are some images of alpacas  this is an alpaca data set I have downloaded for  
1:31
today's tutorial and you can see these are all  images containing alpacas in different postures  
1:37
and in different situations right so this is  exactly the data I am going to use in this process  
1:43
but obviously you could use whatever data set you  want you could use exactly the same data set I am  
1:49
going to use or you can just collect the data  yourself you could just take your cell phone or  
1:55
your camera or whatever and you can just take the  pictures the photos the images you are going  
2:00
to use you can just do your own data collection  or something else you could do is to just use a  
2:07
a publicly available data set so let  me show you this data set this is the open image   dataset version 7 and this is a dataset which is  publicly available and you can definitely use it  
2:17
in order to work on today's tutorial in order to  train the object detector we are going to train  
2:22
on todays tutorial so let me show you how it looks  like if I go to explore and I select detection  
2:30
uh you can see that I'm going to unselect all  these options you can see that this is a huge  
2:37
data set containing many many many many many  many many many categories I don't know how  
2:43
many but they are many this is a huge data set  it contains millions of images, hundreds of  
2:51
thousands if not millions of annotations thousands  of categories this is a super super huge data set  
2:58
and you can see that you have many many different  categories now we are looking at trumpet and you   can see these are different images with trumpets  and from each one of these images we have a  
3:07
bounding box around the trumpet and if I show you  another one for example we also have Beetle and in  
3:15
this category you can see we have many different  images from many different type of beetles so   this is another example or if I show you this one  which is bottle and we have many different images  
3:27
containing bottles for example there you can see  many different type of bottles and in all cases we  
3:33
have a bounding box around the bottle and I could show you I don't know how many examples because   there are many many many different categories  so remember the first step in this process is  
3:43
the data collection this is the data I am going  to to use in this project which is a dataset   of alpacas and you can use the exact same data  I am using if you want to you can use the same  
3:54
data set of alpacas or you can just collect your  own data set by using your cell phone your camera  
3:59
or something like that or you can also download  the images from a publicly available dataset  
4:05
for example the open images dataset version 7. if you  decide to use open images dataset version 7 let  
4:12
me show you another category which is alpaca this  is exactly from where I have downloaded all of the  
4:19
images of alpacas so if in case you decide to use  this publicly available data set I can provide you  
4:25
with a couple of scripts I have used in order to  download all this data in order to parse through  
4:33
all the different annotations and to  format this data in the exact format we need  
4:38
in order to work on today's tutorial so in case  you decide to use open image data set I am going  
4:44
to give you a couple of scripts which are going to  be super super useful for you so that's that's all  
4:51
I can say about the data collection remember you  need to collect data if you want to train an object   detector and you have all those different ways  to do it and all these different categories and  
5:00
all these different options so now let's move on  to the next step and now let's continue with the  
Data annotation
5:06
data annotation you have collected a lot of images  as I have over here you have a lot of images which  
5:12
you have collected yourself or maybe you have  downloaded this data from a publicly available   data set and now it's the time to annotate this  data set maybe you were lucky enough when you were  
5:24
creating the dataset and maybe this data set you  are using is already annotated maybe you already   have all the bounding boxes from all of your  objects from all your categories maybe that's  
5:34
the case so you don't really need to annotate your  data but in any other case for example if you were  
5:41
using a custom data set, a dataset you have collected  yourself with your own cell phone your camera and  
5:47
so on something you have collected in that case  you definitely need to annotate your data so in  
5:54
order to make this process more comprehensive in  order to show you like the entire process let me   show you as well how to annotate data so we are  going to use this tool which is CVAT this is a  
6:07
labeling tool I have used it many many times in  many projects I would say it's one of my favorite  
6:12
tools I have used pretty much absolutely all  the object detection computer vision related  
6:19
annotation tools I have used maybe I haven't used  them all but I have used many many of them and if  
6:25
you are familiar with annotation tools you would  know that there are many many of them and none of  
6:31
them is perfect I will say all of the different  annotation tools have their advantages and their  
6:36
disadvantages and for some situations you prefer  to use one of them and for other situations it's   better to use another one CVAT has many advantages  and it also has a few disadvantages I'm not saying  
6:48
it's perfect but nevertheless this is a tool I  have used in many projects and I really really   like it so let me show you how to use it you  have to go to cvat.ai and then you select try  
6:59
for free there are different pricing options  but if you are going to work on your own or or  
7:04
in a very small team you  can definitely use the free version so I have  
7:10
already logged in this is already logged into my  account but if you don't have an account then you  
7:16
will have to create a new one so you  you're going to see like a sign up page and you  
7:22
can just create a new account and then you can  just logged in into that account so once you are  
7:29
logged into this annotation tool you need to  go to projects and then create a new one I'm  
7:37
going to create a project which is called alpaca  detector because this is the project I am going   to be working in and I'm going to add a label  which in my case is going to be only one label  
7:48
which is alpaca and then that's pretty much all  submit and open I have created the project it has  
7:57
one label which is alpaca remember if your project  has many many different labels add all the labels   you need, and then I will go here which is create  a new task I am going to create a new annotation  
8:10
task and I'm going to call this task something  like alpaca detector annotation task zero zero one  
8:21
this is from the project alpaca detector and this  will take all the labels from that project now  
8:28
you need to upload all the images you are going to  annotate so in my case I'm obviously not going to  
8:34
annotate all the images because you can see these  are too many images and it doesn't make any sense  
8:41
to annotate all these images in this video These  are 452 images so I'm not going to annotate them  
8:47
all but I'm going to select a few in order to show  you how exactly this annotation tool works and how  
8:54
exactly you can use it in your project also in my  case as I have already as I have downloaded these  
9:01
images from a publicly available data set from  the open images dataset version 7 I already  
9:06
have the annotations I already have all the  bounding boxes so in my case I don't really need   to annotate this data because I already have the  annotations but I'm going to pretend I don't so  
9:17
I can just label a few images and I can show you  how it works so now I go back here and I'm just  
9:22
going to select something like this many images  right yeah I'm just going to select this many  
9:28
images I'm going to open these images and then  I'm going to click on submit and open right so  
9:35
this is going to create this task and at the same  time it's going to open this task so we can start  
9:40
working on our annotation process okay so this is  the task I have just created I'm going to click  
9:46
here in job number and this and the job number  and this will open all the images and now I'm  
9:52
going to start annotating all these images so we  are working on an object detection problem so we  
9:57
are going to annotate bounding boxes we need to  go here and for example if we will be detecting  
10:04
many different categories we would select what  is the category we are going to label now and and that's  
10:10
it in my case I'm going to label always the same  category which is alpaca so I don't really need to  
10:15
do anything here so I'm going to select shape  and let me show you how I do it I'm going to   click in the upper left corner and then in the  bottom right corner so the idea is to enclose the  
10:27
object and only the object right the idea is to  draw a bonding box around the object
10:34
you only want to enclose this object  and you can see that we have other animals in the  
10:39
back right we have other alpacas so I'm just going  to label them too and there is a shortcut which is  
10:45
pressing the letter N and you can just create  a new bounding box so that's another one this  
10:52
is another one this is another alpaca and this is  the last one okay that's pretty much all so once  
11:02
you're ready you can just press Ctrl s that's  going to save the annotations I recommend you   to press Ctrl S as often as possible because it's  always a good practice so now everything is saved  
11:14
I can just continue to the next image now we are  going to annotate this alpaca and I'm going to do  
11:19
exactly the same process I can start here obviously  you can just start in whatever corner you want   and I'm going to do something like this okay  this image is completely annotated I'm going to  
11:30
continue to the next image in this case I am going  to annotate this alpaca too. this is not a real  
11:37
alpaca but I want my object detector to be able  to detect these type of objects too so I'm going  
11:44
to annotate it as well this is going to be a very  good exercise because if you want to work  
11:50
as a machine learning engineer or as a computer  visual engineer annotating data is something   you have to do very often, actually training  machine learning models is something you have to  
11:59
do very often so usually the data annotation is  done by other people, right, it is done by annotator s 
12:07
there are different  services you can hire in order to annotate data  
12:14
but in whatever case whatever service you use  it's always a very good practice to annotate  
12:21
some of the images yourself right because if  you annotate some of the images yourself   you are going to be more familiar with the data  and you're also going to be more familiar on how  
12:32
to instruct the annotators on how to annotate this  particular data for example in this case it's not  
12:39
really challenging we just have to annotate these  two objects but let me show you there will be  
12:46
other cases because there will be always situations  which are a little confusing in this case it's not  
12:51
confusing either I have just to I have to label  that object but for example a few images ago  
12:58
when we were annotating this image if an annotator  is working on this image that person is going  
13:06
to ask you what do I do here should I annotate  this image or not right if an annotator is working  
13:14
on this image and the instructions you provide  are not clear enough the person is going  
13:20
to ask you hey what do I do here should I annotate  this image or not is this an alpaca or not so for  
13:27
example that situation, another situation will be  what happened here which we had many different   alpacas in the background and some of them for  example this one is a little occluded so there  
13:37
could be an annotator someone who ask you hey do  you want me to annotate absolutely every single  
13:43
alpaca or maybe I can just draw a huge bonding box  here in the background and just say everything  
13:49
in the background is an alpaca it's something that  when an annotator is working on the images they  
13:57
are going to have many many different questions  regarding how to annotate the data and they are   all perfect questions and very good questions  because this is exactly what's about I mean when  
14:08
you are annotating data you are defining exactly  what are the objects you are going to detect right  
14:14
so um what I'm going is that if you annotate some  of the images yourself you are going to be more  
14:20
familiar on what are all the different situations  and what exactly is going on with your data so you  
14:27
are more clear in exactly what are the objects  you want to detect right so let's continue this  
14:33
is only to show a few examples this is another  situation in my case I want to say that both of  
14:39
them are alpacas so I'm just going to say  something like this but there could be another person   who says no this is only one annotation  is something like this right I'm just going  
14:48
to draw one bonding box enclosing both of them  something that and it will be a good criteria I  
14:54
mean it will be a criteria which I guess it would  be fine but uh whatever your criteria would be
15:01
you need one right you need a criteria so while you  are annotating some of the images is that you  
15:07
are going to further understand what exactly is  an alpaca what exactly is the object you want to  
15:15
consider as alpaca so I'm just going to continue  this is another case which may not be clear but  
15:23
I'm just going to say this is an alpaca this  black one which we can only see this part and   we don't really see the head but I'm going to  say it's an alpaca anyway  
15:32
this one too this one too this one too also this  is something that always happens to me when I am  
15:42
working when I am annotating images that I am more  aware of all the diversity of all these images for  
15:50
example this is a perfect perfect example because  we have an alpaca which is being reflected on a  
15:56
mirror and it's only like a very small  section of the alpaca it's only like a  
16:01
very small uh piece of the alpacas face so what  do we do here I am going to annotate this one too  
16:08
because yeah that's my criteria but another person  could say no this is not the object I want to detect  
16:15
this is only the object I want to detect and maybe  another person would say no this is not an alpaca   alpacas don't really apply makeup on them this is  not real so I'm not going to annotate this image  
16:26
you get the idea right there could be many different  situations and the only way you get familiar   with all the different type of situations  is if you annotate some of the images yourself  
16:36
so now let's continue in my case I'm going  to do something like this  
16:42
because yeah I would say the most important  object is this one and then other ones are like...  
16:48
yeah it's not really that important if we detect  them or not okay so let's continue this is very  
16:55
similar to another image I don't know how many I have  selected but I think we have only a few left
17:04
I don't know if this type of animals are natural... I'm very surprised about this like  
17:10
the head right it's like it has a lot of  hair over here and then it's completely   hairless the entire body I mean I don't know I'm  surprised maybe they are made like that or maybe  
17:21
it's like a natural alpaca who cares who cares...  let's continue so we have let's see how many  
17:28
we have only a few left so let's continue uh let's  see if we find any other strange situation which  
17:35
we have to Define if that's an alpaca or not so  I can show you an additional example also when you  
17:41
are annotating you could Define your bounding box  in many many different ways for example in this  
17:46
case we could Define it like this we could Define  it like this I mean we could Define it super super  
17:52
fit to the object something like this super super  fit and we could enclose exactly the object or we  
18:00
could be a little more relaxed right for example  something like this would be okay too and if we want  
18:06
to do it like this it will be okay too right you  don't have to be super super super accurate you  
18:12
could be like a little more relaxed and it's  going to work anyway uh now in this last one
18:22
and that's pretty much all  and this is the last one okay
18:29
I'm going to do something like this now I'm  going to take this I think this is also alpaca  
18:34
but anyway I'm just going to annotate this part  so that's pretty much all, I'm going to save and  
18:41
those are the few images I have selected in order  to show you how to use this annotation tool so  
18:46
that's pretty much all for the data annotation and  remember this is also a very important step this  
18:53
is a very important task in this process because  if we want to train an object detector we need  
18:59
data and we need annotated data so this is a very  very important part in this process remember this  
19:05
tools cvat this is only one of the many many  many available image annotation tools, you  
19:13
can definitely use another one if you want it's  perfectly fine it's not like you have to use this   one, at all, you can use whatever annotation tool  you want but this is a tool I think it's very easy  
19:24
to use I like the fact it's very easy to use it's  also a web application so you don't really need  
19:29
to download anything to your computer you can  just go ahead and use it from the web that's also  
19:34
one of its advantages so yeah so this is a  tool I showed you in this video how to use in order  
19:42
to train this object detector so this is going  to be all for this step and now let's continue   with the next part in this process and now that  we have collected and annotated all of our data  
Dataset structure
19:54
now it comes the time to format this data to  structure this data into the format we need  
20:01
in order to train an object detector using yolo V8  when you're working in machine learning and you're  
20:06
training a machine learning model every single  algorithm you work with it's going to have its own  
20:12
requirements on how to input the data that's going  to happen with absolutely every single algorithm  
20:17
you will work with it's going to happen with yolo  with all the different YOLO versions and it's  
20:23
going to happen with absolutely every single  algorithm you are working with so especially yolov8
20:29
needs the data in a very specific format so  I created this step in this process so we can  
20:38
just take all the data we have generated all the  images and all the annotations and we can convert  
20:43
all these images into the format we need in order  to input this data into yolo V8 so let me show  
20:50
you exactly how we are going to do that if you  have annotated data using cvat you have to go to  
20:57
tasks and then you have to select this option and  it's export task data set it's going to ask you  
21:06
the export format so you can export this data into  many different formats and you're going to choose  
21:13
you're going to scroll all the way down and you're  going to choose YOLO 1.1 right then you can also  
21:20
save the images but in this case it's not really  needed we don't really need the images we already   have the images and you're just going to click ok  now if you wait a few seconds or a few minutes if  
21:31
you have a very large data set you are going to  download a file like this and if I open this file   you are going to see all these different files  right you can see we have four different files so  
21:40
actually three files and a directory and if I open  the directory this is what you are going to see  
21:46
which is many many different file names and if I  go back to the images directory you will see that  
21:52
all these images file names they all look pretty  much the same right you can see that the file name  
21:59
the structure for this file name looks pretty  much the same as the one with as the ones we have  
22:05
just downloaded from cvat so basically the way  it works is that when you are downloading this  
22:11
data into this format into the YOLO format every  single annotation file is going to be downloaded  
22:18
with the same name as the image you have annotated  but with a different extension so if you have an  
22:26
image which was called something.jpg then The  annotation file for that specific image will be  
22:33
something.txt right so that's the way it works  and if I open this image you are going to see  
22:40
something like this you're going to see in this  case only one row but let me show you another  
22:46
one which contains more than one annotation I  remember there were many for example this one  
22:52
which contains two different rows and each one of  these rows is a different object in my case as I  
22:59
only have alpacas in this data set each one of  these rows is a different alpaca and this is how  
23:05
you can make sense of this information the first  character is the class, the class you are detecting  
23:13
I wanted to enlarge the entire file and  I don't know what I'm doing there okay
23:21
okay the first number is the class you are  detecting in in my case I only have one so  
23:26
it's only a zero because it's my only class and  then these four numbers which Define the bounding  
23:33
box right this is encoded in the YOLO format which  means that the first two numbers are the position  
23:39
of the center of the bounding box then you have  the width of your bounding box and then the  
23:46
height of your bounding box, you will notice  these are all float numbers and this basically  
23:52
means that it's relative to the entire size of  the image so these are the annotations we have  
23:57
downloaded and this is in the exact same format  we need in order to train this object detector  
24:03
so remember when I was downloading these  annotations we noticed there were many many many  
24:11
different options all of these different options  are different formats in which we could save the  
24:16
annotations and this is very important because you  definitely need to download YOLO because we are  
24:22
going to work with yolo and everything it's pretty  much ready as we need it in order to input into  
24:27
yolo V8 right if you select YOLO that's exactly  the same format you need in order to continue with  
24:34
the next steps and if you have your data into  a different format maybe if you have already  
24:39
collected and annotate your data and you have your  data in whatever other format please remember you  
24:45
will need to convert these images or actually to  convert these annotations into the YOLO format  
24:52
now this is one of the things we need for  the data this is one of the things we need in  
24:57
order to we need to format in order to structure  the data in a way we can use this object detector  
25:04
with yolo V8 but another thing we should do is  to create very specific directories containing this  
25:12
data right we are going to need two directories  one of them should be called images and the other  
25:18
one should be called labels you definitely need  to input these names you cannot choose whatever  
25:25
name you want you need to choose these two names  right the images should be located in an directory  
25:31
called images and the labels should be located in  a directory called labels that's the way yolo V8  
25:37
works so you need to create these two directories  within your image directory is where you are going  
25:42
to have your images if I click here you can  see that these are all my images they are all   within the images directory they are all within  the train directory which is within the images  
25:53
directory this directry is not absolutely needed  right you could perfectly take all your images all  
26:01
these images and you could just paste all your  images here right in the images directory and  
26:07
everything will be just fine but if you want you  could do something exactly as I did over here and  
26:14
you could have an additional directory which is  in between images and your images  
26:20
and you can call this whatever way you want this  is a very good strategy in case you want to have  
26:26
for example a train directory containing all the  training images and then another directory which  
26:31
could be called validation for example and this  is where you are going to have many images in   order to validate your process your training  process your algorithm and you could do the  
26:40
same with an additional directory which could be  called test for example or you can just use these  
26:46
directories in order to label the data right  to create different versions of your data which is  
26:53
another thing which is very commonly done so you  could create many directories for many different  
27:00
purposes and that will be perfectly fine but you  could also just paste all the images here and  
27:07
that's also perfectly fine and you can see that  for the labels directory I did exactly the same we  
27:13
have a directory which is called train and within  this directory is that we have all these different   files and for each one of these files let me  show you like this it's going to be much better
27:29
for each one of these files for each one of  these txt files we will have an image in the  
27:36
images directory which is called exactly the  same exactly the same file name but a different  
27:41
extension right so in this case this one is called  .txt and this one is called .jpg but you can  
27:48
see that it's exactly exactly the same file name  for example the first image is called oa2ea8f  
27:55
and so on and that's exactly the same name as  for the first image in the images directory   which is called oa2ea8f and so on so basically for  absolutely every image in your images directory  
28:09
you need to have an annotations file and a file in  the labels directory which is called exactly the  
28:17
same exactly the same but with a different extension  if your images are .jpg your annotations files  
28:25
are .txt so that's another thing which also  defines the structure you'll need for your data  
28:31
and that's pretty much all so remember you need  to have two directories one of them is called  
28:37
images, the other one is called labels within the images  directories is where you're going to have all your  
28:42
images and within your labels directories is where  you will have all your annotations, all your labels  
28:47
and for absolutely every single image in your  images directory you will need to have a file  
28:53
in the labels directory which is called exactly  the same but with a different extension if  
28:59
your images are .jpg your annotation files should  be .txt and the labels should be expressed in  
29:07
the yolo format which is as many rows as  objects in that image and every single one  
29:15
of these rows should have the same structure you  are going to have five terms the first one of them  
29:20
is the class ID in my case I only have one class  ID I'm only detecting alpacas so in my case this  
29:27
number will always be zero but if you're detecting  more than one class then you will have different  
29:32
numbers then you have the position the X and Y  position of the center of the bounding box and  
29:39
then you will have the width and then you will  have the height and everything will be expressed   in relative coordinates so basically this is  the structure you need for your data  
29:51
and this is what this step is about so that's  pretty much all about converting the data or about  
29:58
formatting the data and now let's move on to the  training now it's where we are going to take all  
30:03
this data and we are going to train our object  detector using yolo V8 so now that we have  
Train yolov8
30:09
taken the data into the format we need in order to  train yolo v8 now comes the time for the training  
30:16
now it comes the time where we are going to take  this custom data set and we are going to train an  
30:21
object detector using yolo V8 so this is yolo  V8 official repository one of the things  
30:28
I like the most about YOLO V8 is that in order  to train an object detector we can do it either  
30:34
with python with only a few python  instructions or we can also use a command line  
30:42
utility let me see if I find it over here we can  also execute a command like this in our terminal  
30:51
something that looks like this and that's pretty  much all we need to do in order to train this  
30:56
object detector that's something I really really  liked that's something I'm definitely going to   use in our projects from now on because I think  it's a very very convenient and a very easy way  
31:06
to train an object detector or a machine learning  model so this is the first thing we should notice  
31:11
about yolo V8 there are two different ways  in which we can train an object detector we   can either do it in python as we usually do or  we can run a command in our terminal I'm going  
31:23
to show you both ways so you're familiar with both  ways and also I mentioned that I am going to show  
31:30
you the entire process on a local environment in a  python project and I'm also going to show you this  
31:36
process in a google colab so I I know there are  people who prefer to work in a local environment I  
31:42
am one of those people and I know that there are  other people who prefer to work on a Google colab  
31:47
so depending on in which group are you I  am going to show you both ways to do it so you  
31:54
can just choose the one you like the most so let's  start with it and now let's go to pycharm this is  
32:00
a pycharm project I created for this training and  this is the file we are going to edit in order to  
32:05
train the object detector so the first thing I'm  going to do is to just copy a few lines I'm just  
32:12
going to copy everything and I'm going to remove  everything we don't need copy and paste so we want  
32:18
to build a new model from scratch so we are going  to keep this sentence and then we are going  
32:24
to train a model so we are just going to remove  everything but the first sentence and that's all  
32:31
right these are the two lines we need in order to  train an object detector using yolo V8 now we are  
32:38
going to do some adjustments, obviously the  first thing we need to do is to import  
32:45
ultralytics which is a library we need to use in  order to import yolo, in order to train a yolo  
32:51
V8 model and this is a python Library we need to  install as we usually do we go to our terminal and  
32:56
we do something like pip install and the library  name in my case nothing is going to happen because  
33:02
I have already installed this library but please  remember to install it and also please mind that  
33:08
when you are installing this Library this library  has many many dependencies so you are going to  
33:14
install many many many many different python  packages so it's going to take a lot of space  
33:21
so definitely please be ready for that because you  need a lot of available space in order to install  
33:26
this library and it's also going to take  some time because you are installing many many  
33:31
many different packages but anyway let's continue  please remember to install this library and these  
33:37
are the two sentences we need in order to run  this training from a python script  
33:43
so this sentence we're just going to leave it as  it is this is where we are loading the specific  
33:48
yolo V8 architecture the specific yolo V8  model we are going to use you can see that  
33:54
we can choose from any of all of these different  models these are different versions or these are  
34:01
different sizes for yolo V8 you can see we have  Nano small medium large or extra large we are  
34:06
using the Nano version which is the smallest one  or is the lightest one, so this is the one we are going  
34:13
to use, the yolo V8 Nano, the yolo V8 n then about  the training about this other sentence we need to  
34:20
edit this file right we need a yaml file which  is going to contain all the configuration for our  
34:27
training so I have created this file and I have  named this file config.yaml I'm not sure if this  
34:33
is the most appropriate name but anyway this is  the name I have chosen for this file so what I'm  
34:39
going to do is just edit this parameter and I'm  going to input config.yaml this is where the  
34:46
config.yaml is located this is where the main.pi  is located, they are in the same directory so if I do   this it's going to work just fine and then let  me show you the structure for this config.yaml  
34:56
you can see that this is a very very very simple  configuration file we only have a few Keys which  
35:04
are PATH train val and then names right let's  start with the names let's start with this this  
35:12
is where you are going to set all your different  classes right you are training an object detector  
35:18
you are detecting many different categories many  different classes and this is where you are going   to input is where you're going to type all of  those different classes in my case I'm  
35:28
just detecting alpacas that's the only class  I am detecting so I only have one class, is the  
35:34
number zero and it's called alpaca but if you are  detecting additional objects please remember to   include all the list of all the objects you are  detecting, then about these three parameters these  
35:46
three arguments the path is the absolute path to  your directory containing images and annotations  
35:54
and please remember to include the absolute path.  I ran some issues when I was trying to specify  
36:02
a relative path relative from this directory from  my current directory where this project is created  
36:08
to the directory where my data is located when  I was using a relative path I had some issues  
36:15
and then I noticed that there were other people  having issues as well I noticed that in the GitHub  
36:22
repository from YOLO V8 I noticed this is in the  the issues section there were other people having  
36:28
issues when they were specifying a relative path  so the way I fixed it and it's a very good way to  
36:34
fix it it's a very easy way to fix it it's just  specifying an absolute path remember this should  
36:40
be an absolute path so this is the path to this  directory to the directory contain the images  
36:47
and the labels directories so this is this is the  path you need to specify here and then you have to  
36:55
specify the relative path from this location to  where your images are located like the specific  
37:03
images are located right in my case they are in  images/train relative to this path if I show you  
37:12
this location which is my root directory then if  I go to images/train this is where my images are  
37:20
located right so that's exactly what I need to  specify and then you can see that this is the  
37:28
train data this is the data the algorithm is going  to use as training data and then we have another   keyword which is val right the validation dataset  in this case we are going to specify the  
37:40
same data as we used for training and the reason  I'm doing this is because we want to keep things  
37:46
simple in this tutorial I'm just going to show  you the entire process of how to train an object  
37:52
detector using yolo V8 on a custom data set  I want to keep things simple so I'm just going   to use the same data so that's pretty much all  for this configuration file now going back to  
38:02
main that's pretty much all we need in order to  train an object detector using yolo V8  
38:10
from python that's how simple it is so now I'm  going to execute this file I'm going to change the  
38:16
number of epochs I'm going to do this for only  one Epoch because the only thing I'm going to  
38:21
show you for now is how it is executed, I'm going to  show you the entire process and once we notice  
38:28
how everything is working once we know everything is up and running everything is   working fine we can just continue but let's just  do this process let's just do this training for  
38:38
only one Epoch so we can continue you can see that  now it's loading the data it has already loaded 
38:47
the data you can make use of all the different  information of this debugging  
38:52
information we can see here you can see now  we were loading 452 images and we were able  
39:00
to load all the images right 452 from 452 and if  I scroll down you can see that we have additional  
39:07
information additional values which are related  to the training process this is how the training   process is going right we are training this object  detector and this additional information which we  
39:19
are given through this process so for now the  only thing we have to do is only waiting we  
39:25
have to wait until this process is completed so  I am going to stop this video now and I'm going   to fast forward this video until the end of this  training and let's see what happens okay so the  
39:37
training is now completed and you can see that  we have an output which says results saved to  
39:42
runs/detect/train39 so if I go to that directory  runs/detect and train39 you can see that we have  
39:52
many many different files and these files are related to how the training process  
39:57
was done right for example if I show you these  images these are a few batches of images which  
40:03
were used in order to train this algorithm  you can see the name is train batch0   and train batch1 I think we have a train batch2 so we have a lot of different images of a lot  
40:13
of different alpacas of different images we used  for training and they were all put together they  
40:19
were all concatenated into these huge images so  we can see exactly the images which were used for  
40:25
training and The annotation on top of them right  the bonding boxes on top of them and we also have  
40:31
similar images but for the validation dataset right remember in this case we are using the same  
40:38
data as validation as we use for training so it's  exactly the same data it's not different data but  
40:46
these were the labels in the validation data set  which is the training data set and these were the  
40:52
predictions on the same images right you can see  that we are not detecting anything we don't have  
40:58
absolutely any prediction we don't have absolutely  any bounding box this is because we are doing a   very shallow training we are doing a very dummy  training we are training this algorithm only for one epoch 
41:10
this was only an example to show you the output  how it looks like to show you the entire process   but it is not a real training but nevertheless  these are some files I'm going to  
41:19
show you better when we are in the next step  for now let me show you how the training is done  
41:26
from the command line from the terminal using the  command I showed you over here using a command like  
41:34
this and also let me show you how this training  is done on a Google colab so going to the terminal  
41:41
if we type something like this yolo detect train  data I have to specify the configuration file  
41:51
which is config.yaml and then model yolov8n.yaml   and then the number of epochs this  
42:01
it's exactly the same as we did here exactly the  same is going to produce exactly the same output  
42:07
I'm just going to change the number of epochs for  one so we make it exactly the same and let's see  
42:14
what happens you can see that it we have exactly  the same output we have loaded all the images and  
42:20
now we are starting a new training process and  after this training process we are going to have  
42:25
a new run which we have already created the new  directory which is train40 and this is where  
42:31
we are going to save all the information related  to this training process so I'm not going to do  
42:37
it because it's going to be exactly the same as  as the one we did before but this is exactly how   you should use the command line or how you  can use this utility in order to do this training  
42:50
from the terminal you can see how simple it is  it's amazing how simple it is it's just amazing  
42:58
and now let me show you how everything is done  from a Google colab so now let's go back to the  
43:03
browser so I can show you this notebook I created  in order to train yolo V8 from a Google colab  
43:09
if you're not familiar with Google collab the way  you can create a new notebook is going to Google  
43:15
Drive you can click new more and you select  the option Google collaboratory this is going  
43:21
to create a new google colab notebook and you  can just use that notebook to train this object  
43:27
detector now let me show you this notebook and  you can see that it contains only one two three   four five cells this is how simple this will  be the first thing you need to do is to upload  
43:39
the data you are going to use in order to train  this detector it's going to be exactly the same   data as we used before so these are exactly  the same directories the images directory and  
43:49
the label directory we used before and then  the first thing we need to do is to  
43:54
execute this cell which mounts Google Drive into  this instance of google collab so the only  
44:03
thing I'm doing is just I just pressed  enter into this cell and this may take some time  
44:10
but it's basically the only thing it does is  to connect to Google Drive so we can just access  
44:17
the data we have in Google Drive so I'm going to  select my account and then allow and that's pretty  
44:25
much all then it all comes to where you have the  data in your Google drive right in the specific  
44:32
directory where you have uploaded the data in  my case my data is located in this path right  
44:40
this is my home in Google Drive and then this  is the relative path to the location of where  
44:49
I have the data and where I have all the files  related to this project so remember to specify this   root directory as the directory where you have  uploaded your data and that's pretty much all  
44:58
and then I'm just going to execute this cell  so I save this variable I'm going to execute  
45:05
this other cell which is pip install ultralytics the  same command I ran from the terminal in my local  
45:11
environment now I'm going to run it in Google  collab remember you have to start this command by  
45:17
the exclamation mark which means you are running  a command in the terminal where this process is  
45:24
being executed or where this notebook is being  launched so remember to include the exclamation  
45:30
mark everything seems to be okay everything  seems to be ready and now we can continue to   the next cell which is this one you can see that  we have done exactly the same structure we have  
45:42
input exactly the same lines as in our  local environment if I show you this again you  
45:47
can see we have imported ultralytics then we have  defined this yolo object and then we have called  
45:52
model.train and this is exactly the same as we are  doing here obviously we are going to need another  
45:59
yaml file we are going to need a yaml file in our  Google Drive and this is the file I have specified  
46:07
which it's like exactly the same  configuration as in the um as in the in the  
46:17
yaml file I showed you in my local environment is  exactly the same idea so this is exactly what you  
46:23
should do now you should specify an absolute  path to your Google Drive directory that's the  
46:30
only difference so that's the only difference  and I see I have a very small mistake because   I see I have data here and here I have just  uploaded images and labels in the directory  
46:40
but they are not within another rectory which  is called Data so let me do something I'm going   to create a new directory which is called Data  images labels I'm just going to put everything  
46:50
here right so everything is consistent so now  everything is okay images then train and then the  
46:57
images are within this directory so everything  is okay now let's go back to the Google collab  
47:04
every time you make an edit or every time you do  something on Google Drive it's always a good idea to  
47:10
restart your runtime so that's what I'm going  to do I'm going to execute the commands again  
47:16
I don't really need to pip install this Library  again because it's already installed into this  
47:21
environment and then I'm going to execute this  file I think I have to do an additional edit which  
47:28
is uh this file now it's called google_colab_config.yaml and that's pretty much all I'm just going  
47:37
to run it for one Epoch so everything is exactly  the same as we did in our local environment and  
47:43
now let's see what happens so you can see that  we are doing exactly the same process everything  
47:48
looks pretty much the same as it did before we  are loading the data we are just loading the  
47:56
models everything it's going fine and  this is going to be pretty much the same process  
48:01
as before you can see that now it takes  some additional time to load the data because now  
48:08
you have... you are running this environment you're  running this notebook in a given environment and  
48:14
you're taking the data from your Google Drive so  it takes some time it's it's a slower process but  
48:21
it's definitely the same idea so the only thing we  need to do now is just to wait until all this uh  
48:27
process to be completed and that's pretty much all  I think it doesn't really make any sense to wait  
48:32
because it's like it's going to be exactly the  same process we did from our local environment   at the end of this execution we are going to have  all the results in a given directory which is the  
48:43
directory of the notebook which is running this  process so at the end of this process please  
48:48
remember to execute this command which is going  to take all the files you have defined in this  
48:54
runs directory which contains all the runs you  have made all the results you have produced and  
49:01
it's going to take all this directory  into the directory you have chosen for your files  
49:08
and your data and your google collab and so on  please remember to do this because otherwise   you would not be able to access this data and  this data which contains all the results and  
49:19
everything you have just trained so this is how  you can train an object detector  
49:26
using yolo V8 in a Google collab and you can  see that the process is very straightforward and   it's pretty much exactly the same process exactly  the same idea as we did you in our local environment  
49:37
and that's it so that's how easy it is to train  an object detector using yolo Y8 once you have  
49:44
done everything we did with the data right once  you have collected the data you have annotated  
49:49
data you have taken everything into the format  yolo V8 needs in order to train an object  
49:55
detector once everything is completed then  running this process running this training  
50:00
is super straightforward so that's going to be  all about this training process and now let's  
50:06
continue with the testing now let's see how these  models we have trained how they performed right  
50:13
let's move to the next step and this is the last  step in this process this is where we are going  
Test yolov8
50:19
to take the model we produced in the training  step and we're going to test how it performs  
50:24
this is the last step in this process this is how  we are going to complete this training of an object  
50:30
detector using yolo v8, so once we have trained  a model we go to the uh to this directory remember  
50:39
to the directory I showed you before regarding... the  directory where all the information was saved  
50:45
where all the information regarding this training  process was saved and obviously I I'm not going to  
50:50
show you the training we just did because it was  like a very shallow training like a very dummy   training but instead I'm going to show you the  results from another training I did when I Was  
51:01
preparing this video where I conducted exactly the  same process but the training process was done for  
51:08
100 epochs so it was like a more deeper training  right so let me show you all the files we have  
51:15
produced so you know what are all the different  tools you have in order to test the performance of   the model you have trained so basically you have  a confusion Matrix which is going to give you a  
51:25
lot of information regarding how the different  classes are predicted or how all the different  
51:31
classes are confused right if you are familiar  with how a confusion Matrix looks like or it  
51:38
should look like then you will know how to read  this information basically this is going to give  
51:44
you information regarding how all the different  classes were confused in my case I only have   one class which is alpaca but you can see that  this generates another category which is like  
51:55
uh the default category which is background and we  have some information here it doesn't really say  
52:00
much it says how these classes are confused but  given that this is an object detector I think the  
52:06
most valuable information it's in other metrics in  other outputs so we are not really going to mind  
52:12
this confusion Matrix then you have some plots  some curves for example this is the F1 confidence  
52:18
curve we are not going to mind this plot either  remember we are just starting to train an  
52:25
object detector using yolo V8 the idea for this  tutorial is to make it like a very introductory training  
52:30
a very introductory process so we are not going to  mind in all these different uh plots we have over  
52:37
here because it involves a lot of knowledge and  a lot of expertise to extract all the information  
52:44
from these plots and it's not really the idea for  this tutorial let's do things differently let's  
52:51
focus on this plot which is also available in  the results which were saved into this directory  
52:58
and you can see that we have many many many  different plots you can definitely go crazy  
53:03
analyzing all the information you have here  because you have one two three four five   ten different plots you could knock yourself out  analyzing and just extracting all the information  
53:15
from all these different plots but again the idea  is to make it a very introductory video and a very  
53:21
introductory tutorial so long story short I'm  just going to give you one tip of something the  
53:28
one thing you should focus on these plots for now  if you're going to take something from this video  
53:34
from how to test the performance of a model  you have just trained using yolo v8 to train an object  
53:40
detector is this make sure your loss is going  down right you have many plots some of them are  
53:48
related to the loss function which are this one this  one and this one this is for the training set and  
53:55
these are related to the validation set make  sure all of your losses are going down right this  
54:03
is like a very I would say a very simple way to  analyze these functions or to analyze these plots  
54:09
but that's... I will say that that's more powerful  that it would appear make sure all your losses are  
54:18
going down because given the loss function we  could have many different situations we could   have a loss function which is going down which  I would say it's a very good situation we could  
54:28
have a loss function which started to go down and  then just it looks something like a flat line and  
54:34
if we are in something that looks like a flat line  it means that our training process has stuck so it  
54:41
could be a good thing because maybe the the  algorithm the machine learning model really   learned everything he had to learn about this  data so maybe a flat line is not really a bad  
54:52
thing maybe I don't know you you would have to  analyze other stuff or if you look at your loss  
54:58
function you could also have a situation  where your loss function is going up right  
55:04
that's the other situation and if you my friend  have a loss function which is going up then you  
55:11
have a huge problem then something is obviously  not right with your training and that's why I'm  
55:16
saying that analyzing your loss function what  happens with your loss is going to give you a   lot of information ideally it should go down if  it's going down then everything is going well  
55:28
most likely, if its something like a flatline  well it could be a good thing or a bad thing I  
55:34
don't know we could be in different situations  but if it's going up you have done something  
55:40
super super wrong I don't know what's going on  in your code I don't know what's going on in   your training process but something is obviously  wrong right so that's like a very simple and a  
55:50
very naive way to analyze all this information  but trust me that's going to give you a lot a  
55:56
lot of information you know or to start working  on this testing the performance of this model  
56:03
but I would say that looking at the plots and analyzing  all this information and so on I would say that's  
56:09
more about research, that's what people  who do research like to do and I'm more like  
56:15
a freelancer I don't really do research so  I'm going to show you another way to analyze this  
56:22
performance, the model we have just  trained which from my perspective it's a more...  
56:30
it makes more sense to analyze it like this and it  involves to see how it performs with real 
56:38
data right how it performs with data you have  used in order to make your inferences and to  
56:44
see what happens so the first step in this more  practical more visual evaluation of this model of  
56:49
how this model performs is looking at these images  and remember that before when we looked at these  
56:56
images we had this one which was regarding the  labels in the validation set and then this other  
57:05
one which were the predictions were completely  empty now you can see that the the predictions  
57:10
we have produced they are not completely empty  and we are detecting the position of our alpacas  
57:17
super super accurately we have some mistakes  actually for example here we are detecting   a person as an alpaca here we are detecting also  a person as an alpaca and we have some missdetections  
57:30
for example this should be in alpaca and it's not  being detected so we have some missdetections but you  
57:36
can see that the the results are pretty much okay  right everything looks pretty much okay the same  
57:42
about here if we go here we are detecting pretty  much everything we have a Missdetection here we  
57:48
have an error over here because we are detecting  an alpaca where there is actually nothing so things are  
57:55
not perfect but everything seems to be pretty much  okay that's the first way in which we are going to  
58:01
analyze the performance of this model which is  a lot because this is like a very visual way to  
58:07
see how it performs we are not looking at plots we  are not looking at metrics right we are looking at  
58:13
real examples and to see how this model performs  on real data maybe I am biased to analyze things  
58:22
like this because I'm a freelancer and the way it  usually works when you are a freelancer is that  
58:27
if you are building this model to deliver this  project for a client and you tell your client oh  
58:34
yeah the model was perfect take a look at all  these plots take a look at all these metrics   everything was just amazing and then your client  tests the model and it doesn't work the client  
58:44
will not care about all the pretty plots and so  on right so that's why I don't really mind a lot  
58:51
about these plots maybe I am biased because I am a  freelancer and that's how freelancing works but I  
58:56
prefer to do like a more visual evaluation  so that's the first step we will do and we   can notice already we are having a better  performance we are having an okay performance  
59:08
but this data we are currently looking at right  now remember the validation data it was pretty  
59:14
much the same data we use as training so this  doesn't really say much I'm going to show you  
59:20
how it performs on data which the algorithm have  never seen with completely and absolutely unseen  
59:27
data and this is a very good practice if you  want to test the performance of a model, so I have  
59:32
prepared a few videos so let me show you these  videos they are basically... remember this is  
59:38
completely unseen data and this is the first video  you can see that this is an alpaca which is just  
59:44
being an alpaca which is just walking around  it's doing its alpaca stuff it's having an  
59:51
alpaca everyday life it's just being an alpaca  right it's walking around from one place to   the other doing uh doing nothing no it's doing  its alpaca stuff which is a lot this is one of  
1:00:04
the videos I have prepared this is another video  which is also an alpaca doing alpaca related stuff  
1:00:11
um so this is another video we are going to  see remember this is completely unseen data and   I also have another video over here so I'm  going to show you how the model performs on these  
1:00:22
three videos I have made a script in Python  which loads these videos and just calls the  
1:00:29
predict method from yolo v8, we  are loading the model we have trained and we are  
1:00:36
applying all the predictions to this model and  we are seeing how it performs on these videos  
1:00:43
so this is the first video I showed you and these  are the detections we are getting you can see  
1:00:49
we are getting an absolutely perfect detection  remember this is completely unseen data and we are  
1:00:54
getting I'm not going to say 100 perfect detection  because we're not but I would say it's pretty good  
1:01:00
I will say it's pretty pretty good in order to  start working on this training process uh yeah  
1:01:07
I would say it's pretty good so this is one of  the examples then let me show you another example  
1:01:14
which is this one and this is the other video  I showed you and you can see that we are also  
1:01:19
detecting exactly the position of the alpaca  in some cases the text is going outside of the  
1:01:25
frame because we don't really have space but  everything seems to be okay in this video too  
1:01:30
so we are taking exactly the position of this uh  alpaca the bonding box in some cases is not really  
1:01:36
fit to the alpaca face but yeah but everything  seems to be working fine and then the other video  
1:01:43
I showed you you can see in this case the detection  is a little broken we have many missdetections  
1:01:49
but now everything is much better and yeah in  this case it's working better too it's working  
1:01:55
well I would say in these three examples this one  it's the one that's performing better and then the  
1:02:03
other one I really like how it performed too in  this case where the alpaca was like starting its  
1:02:09
alpaca Journey... we have like a very  good detection and a very stable detection then it  
1:02:15
like breaks a little but nevertheless I would say  it's okay it's also detecting this alpaca over   here so uh I will say it's working pretty much  okay so this is pretty much how we are going to  
1:02:28
do the testing in this phase remember that if you  want to test the performance of the model you have  
1:02:34
just trained using yellow V8 you will have a lot  of information in this directory which is created  
1:02:39
when you are yolo the model at the end of your  training process you will have all of these files   and you will have a lot of information to knock  yourself out to go crazy analyzing all these  
1:02:51
different plots and so on or you can just keep it  simple and just take a look at what happened with  
1:02:56
the training loss and the validation  loss and so on all the loss functions make sure  
1:03:01
they are going down that's the very least thing  you need to make sure of and then you can just  
1:03:07
see how it performs with a few images or with  a few videos, take a look how it performs  
1:03:12
with unseen data and you can make decisions from  there maybe you can just use the model as it is  
1:03:18
or you can just decide to train it again in this  case if I analyze all this information I see that  
1:03:26
the loss functions are going down and not  only they are going down but I notice that there  
1:03:31
is a lot of space to to improve this training, to  improve the performance because we haven't reached  
1:03:38
that moment where everything just appears to be  stuck right like that a flat line we are very far  
1:03:44
away from there so that's something I would do  I would do a new deeper training so we can just  
1:03:50
continue learning about this process also I w  change the validation data for something that's  
1:03:57
completely different from the training  data so we have even more information and that's  
1:04:02
pretty much what I would do in order to iterate in  order to make a better model and a more powerful  
Outro
1:04:09
model so it is going to be all for today my name  is Felipe I'm a computer region engineer if you   enjoyed this video please remember to click the  like button it's going to help me a lot it's going  
1:04:18
to help the channel and you know how it is so this  is going to be all for today in this channel I   make tutorials, coding tutorials which are related  to computer vision and machine learning I also  
1:04:28
share my experience as a computer vision engineer  I talk about different things related to computer   vision or machine learning so if you're curious  to know more about these topics please consider to  
1:04:37
subscribe to my channel this is going to be all  for today and see you on the next video
Transcript


Search in video
0:00
Welcome back to the channel. In this
0:02
video, I'll walk you through the
0:03
complete process of creating a custom
0:05
object detection data set using Rooflow
0:08
step by step. This is perfect for AI,
0:11
ML, computer vision, and YOLO users.
0:15
Let's get started.
0:20
Before we move to Rooflow, we need a
0:22
collection of images. For that we use a
0:25
video file may be downloaded from the
0:27
internet or recorded using ACC TV webcam
0:30
or IP camera. This Python script uses
0:34
Open CV to extract clear frames from the
0:36
video and saves them as images
0:38
automatically. You just run the code and
0:41
your data set folder is created for you.
0:43
Just follow step by step and you'll have
0:46
your custom data set ready in minutes.
1:19
Now our data set is ready. The script
1:22
has created a folder full of images from
1:24
your video. You can find this folder at
1:27
the location you set in the code. That's
1:28
the my path on your computer. Inside
1:31
that folder, you'll see all the
1:33
extracted frames that are sharp and
1:35
clear, ready for annotation. This folder
1:38
will be used in the next step when we
1:39
upload it to Rooflow. Just go to that
1:42
path and open the folder. You'll see
1:44
your custom data set ready to go.
1:53
Now, let's move to Rooflow. Go to
1:55
roofflow.com and sign up for a free
1:57
account if you haven't already. Once
2:00
you're logged in, click on create new
2:02
project. You can rename the project
2:04
folder if you want, but I'll keep it as
2:06
it is. Make sure to enter the object
2:09
name, for example, helmet, person, or
2:12
whatever object you are detecting. Then
2:15
choose the annotation type as object
2:17
detection and leave the licenses default
2:19
unless you have special requirements.
2:22
Now it's time to upload the image data
2:24
set we created earlier using the Python
2:26
code. Go to the folder where the images
2:29
was saved. That's the same path from
2:31
step one. And upload all the images to
2:33
Rooflow. Just follow the video step by
2:36
step. Everything is shown clearly. So
2:38
you can do it with Easy.
3:09
Now that all your images are uploaded,
3:11
it's time to annotate them. Annotation
3:14
means drawing bounding boxes around the
3:16
object you want to detect. In this case,
3:18
for example, a helmet. After uploading
3:21
is complete, click on save and continue.
3:45
Now Roboflow will ask how you want to
3:48
label the images. Click on label images
3:51
myself because we are doing manual
3:53
annotation. Then click on start
3:56
annotating. Now you'll see your images
3:58
one by one. Use your mouse to draw a box
4:01
around the object and type the label.
4:03
For example, helmet. Just follow the
4:06
steps shown in the video. It's very
4:08
easy. Note, it may take some time
4:10
depending on how many images you have,
4:12
but don't worry. It's a one-time process
4:14
that will make your model work
4:16
accurately later.
5:22
A few moments Later.
6:33
Now that annotation is complete, let's
6:35
add the images to your data set. Click
6:37
on add images to data set. Next, RobFlow
6:41
will ask you to choose a split method.
6:43
Select the option to split images
6:45
between train validation test sets. Then
6:48
click on add images. That's it. Now your
6:51
images are ready as a properly
6:53
structured data set for training.
6:58
Congratulations, you have successfully
7:01
created your custom data set on Rooflow.
7:03
Now click on train a model. Then select
7:06
train with Rooflow train or go for
7:08
custom training. If you are using your
7:10
own model like YOLO, I'm using custom
7:13
training here. Just follow the steps
7:15
shown on the screen. After that, your
7:18
data set is fully ready.
7:46
Now click download data set to download
7:48
it in your preferred format. In my case,
7:51
I'm choosing YOLO 11. You can also
7:53
select YOLO 8, TensorFlow or any other
7:56
supported format. Then click show
7:59
download code and you'll see the Python
8:01
code snippet for using this data set.
8:03
Click continue and copy that code.
8:16
Now it's time to run our data set in
8:18
Google Collab and start training our
8:20
model. We'll be using the code provided
8:22
by Rooflow and running it step by step
8:25
in Google Collab. I've already set up a
8:28
Google Collab notebook for you and the
8:30
link is available in the video
8:31
description or on GitHub. Just open the
8:34
notebook and follow the video step by
8:36
step.
9:15
Now go back to Rooflow to copy your data
9:18
set link. Click on download data set and
9:21
choose YOLO 11. Then click on show
9:24
download code. Click continue and copy
9:26
that code. Now go back to your Google
9:29
Collab notebook and paste the code into
9:31
a new code cell. Then click run. Note,
9:35
if it shows an error like runtime
9:37
disconnected or says restart runtime
9:40
required, just click on restart runtime,
9:42
wait a few seconds and run the same cell
9:45
again. It will work properly the second
9:47
time.
11:13
After your data set is downloaded into
11:15
Collab, it's time to train your model.
11:18
Go to the left side file explorer in
11:20
Collab and open your data set folder.
11:22
Inside that folder, you'll find a file
11:25
called data yaml. Right click on
11:27
data.yam. Click copy path. Now scroll
11:30
down to the training cell and paste the
11:32
path into the following command. Make
11:35
sure to replace the data path with your
11:37
actual data. Yum path. Set imgsz equals
11:42
120. This should match the size of your
11:44
training images. Set epoch equals 40.
11:47
You can change it as per your training
11:49
needs. Then just run the cell and your
11:51
training will begin. Training may take
11:54
some time depending on your internet
11:56
speed and GPU availability in Google
11:58
Collab. So be patient and let the model
12:00
finish learning properly.
13:22
Now that training is complete, it's time
13:24
to download your trained model. On the
13:26
left hand side of Google Collab, go to
13:28
the file explorer panel. Follow this
13:31
path. Runs then detect then train then
13:33
weights. Inside the weights folder,
13:36
you'll find two files. Best.pt. This is
13:38
the best version of your trained model
13:40
based on validation performance last.pt.
13:43
This is the last checkpoint. Right click
13:45
on best.pt. Click download to save it to
13:48
your computer.
13:51
Now let's bring your trained YOLO model
13:53
into action using PyCharm. First open
13:56
PyCharm and go to your project folder.
13:59
Then locate your downloaded best.pt file
14:01
and copy it into your PyCharm
14:03
environment or project folder. Install
14:06
required libraries inside your project.
14:08
You'll see a file called lab.txt.
14:11
It contains all the required Python
14:13
libraries in PyCharm. Open the terminal
14:16
and simply run bash. Copy edit pip
14:20
install rlab.txt.
14:22
This will automatically install
14:23
everything your detection code needs
14:25
like ultritics, open cv, python and
14:28
more. Final step, run detection script.
14:31
Now you are all set. Just enter your
14:33
video file name in the code where
14:35
needed. e.mpp4. Make sure best.pt is
14:38
loaded correctly in the model path and
14:41
hit run. Don't forget to set the correct
14:43
Python file as current file if you are
14:46
using multiple scripts. And that's it.
14:49
Your customtrained YOLO model is now
14:51
running inside PyCharm detecting objects
14:53
from your video in real time. Just
14:56
follow the steps and your AI project is
14:58
complete.
15:00
Wow, it's actually detecting objects
15:02
with great accuracy. That means
15:04
everything we did from collecting the
15:06
data set, annotating in Rooflow,
15:08
training the model, and running
15:10
detection in PyCharm worked perfectly.
15:13
All project related files, code, and
15:15
setup instructions are available in the
15:17
description below. Thank you so much for
15:20
watching. If you found this helpful,
15:22
like the video, share it with your
15:24
friends. Subscribe to the channel for
15:27
more AI and project tutorials. Follow us
15:29
on Instagram Nexus TechFusion and if you
15:32
are facing any project related issues or
15:34
want help building a custom project just
15:37
DM us. We provide complete support
15:40
components 3D printing and engineering
15:42
solutions. See you in the next one and
15:45
all the best for your AI journey.
Transcript


Search in video
Intro
0:00
so in this video here I'm going to show
0:01
you how we can train a custom Yol V8
0:03
model this is a new one Yol v8.1 where
0:06
we can actually go in and do oriented
0:08
bounding boxes so in the previous videos
0:09
I showed you guys how we can set it up I
0:11
basically went through the documentation
0:12
and so on we used the code snippet from
0:15
the Alo litic documentation but in this
0:16
video here we're going to grab a custom
0:18
data set we're going to have some cups
0:19
you guys are probably familiar with that
0:21
data set that I've created myself but
0:22
we're going to use that data set I'm
0:23
going to show you how we can use Robo
0:25
flow for that how we can annotate our
0:27
images and also how we can export it so
0:29
right now we can just export directly so
0:30
I created a custom scripts that can go
0:32
in and format the annotations to the
0:33
correct format with the new uliv 8.1
0:37
model for our in Bounty box so let's
0:39
just jump straight into it we're going
0:40
to cover how we can annotate images
0:42
export them convert them to the correct
0:43
format how you can train your own custom
0:45
model and then we're going to run
0:46
inference on a waiter stream to see the
0:48
results of oured bounding boxes so we're
Labeling with Roboflow
0:50
now jump straight into roope flow this
0:52
is the platform that we're going to use
0:53
to annotate our images I already have an
0:55
annotated data set you can go and use
0:57
that directly or you can go and use your
0:59
own custom data sets I'm basically just
1:00
going to show you the whole throw here
1:02
first of all we can go and see the
1:03
results so these are like some of the
1:04
test set I'm just going to zoom out a
1:06
bit and run another example so you can
1:08
basically just see that I've tried to
1:09
train a model here on roof flow but
1:11
basically you can go in create a new
1:12
project on a roboff flow upload your
1:14
images and then you can go in and do
1:15
annotation there's one trick here with
1:18
robot flow to be able to do oriented
1:20
bounding boxes if you go inside projects
1:22
so when we want to create oriented
1:23
bounding boxes with Yol V8 for now this
1:25
is how we can do it so we actually need
1:27
to go in and choose instant segmentation
1:29
instead of optic detection because for
1:31
optic detection with Robo flow right now
1:33
they don't support R in bounding boxes
1:35
if you're using annotation tool that
1:36
already supports RN bounding boxes you
1:38
can just use that but here we actually
1:40
need to do instant segmentation and then
1:42
export it in another format to be able
1:44
to use Robo flow so that's just one of
1:46
the things right now I'm definitely sure
1:48
that Ro flow is going to change this
1:49
over time so right now we can just
1:51
choose instant segmentation then you set
1:52
up the project name what you're going to
1:54
detect and so on but I have already done
1:56
that so I have this cup segmentation
1:58
data set we can go and take a look at
1:59
some of the images so I basically just
2:01
went inside the images once you have
2:03
uploaded your data set to robot flow you
2:04
will get inside this annotation tool you
2:06
can choose new classes you can go in and
2:08
annotate the images but to be able to do
2:10
this with our in bounding boxes we need
2:12
this polygon tool and then you can just
2:15
draw these mask here around your optic
2:17
that you want to do optic detection off
2:19
with the new Yoli 8.1 for oriented
2:22
bounding boxes we know the traditional
2:24
bounding boxes here is just like a
2:25
rectangle or like a square which is
2:27
fixed so it's not like rotating with the
2:29
object so let's say that we want to have
2:31
our cup here then let's go up and take
2:33
an example so right now we draw a
2:34
bounding box around the cup like this
2:36
but in reality if we actually like want
2:38
the best detections possible then we
2:40
should probably like have our bounding
2:42
box in this example here rotate a bit to
2:44
the left so we're fitting the object
2:45
better and that is really useful in a
2:47
lot of different applications and
2:49
projects so just go back here again and
2:51
delete this annotation but this is the
2:52
tool that you can use and is really easy
2:54
to work with and we can also see in just
2:56
a second how we can export the data set
2:58
so yeah then you'll just going and label
3:00
the images you're going to use the
3:01
polygon tool so you'll just draw like a
3:03
polygon around your object so right now
3:06
I've just done that for all the images
3:07
so you just go around it like this
3:09
around the edges once you have label
3:11
your whole data set you're ready to go
3:13
let's go back here again we have
3:15
different versions we can also go in and
3:16
generate a new data set so with Robo
3:18
flow we can go in and do pre-processing
3:20
we can specify the size of our image
3:22
that we want to use so right now we're
3:23
just going to use 640x 640 Al
3:26
orientation we can also go and do other
3:28
pre-processing options but don't play
3:29
around too much with that just using
3:31
robw images is often the best one to do
3:35
we can also apply augmentation steps
3:36
here so again if you only have like a
3:37
few examples you have like 50 images or
3:40
so you can go and apply data
3:41
augmentation on top of your data set
3:43
basically just to generate more data
3:44
that varies uh bit from sample to sample
3:48
so we can do flip horizontal vertical
3:50
flip zoom in like we can crop out an
3:52
image if you want to have like some
3:53
zooming effect and this just helps our
3:55
model generalize better we can also
3:57
apply augmentation or like rotation and
3:59
a lot of other options in here so I've
4:01
just chosen these three here that is
4:03
normally what I use we can hit continue
4:05
and then we can create a new project
4:06
specifying if you want to like 3x 4X 5x
4:09
2x our data set so right now we have
4:11
just like 5x I think but I've already
4:13
generated a couple of versions here if
4:15
you just go inside the versions this is
4:17
the model that I showed you in the start
4:18
we can try it out directly with roof
4:20
flow but once we have label our data set
4:22
we can go inside our versions again this
4:24
is still instant segmentation as you
4:25
guys are familiar with but we're trying
4:27
to do optic detction with oriented
4:29
Bounty boxes but this is just how we
4:30
will have to do it with Robo flow for
4:32
now you can use whatever other like
4:34
annotation labeling tool out there if
4:36
the support bounding boxes like or in
4:37
the bounding boxes you can probably
4:39
export it directly in that format maybe
4:41
you'll need to do some conversions
4:42
because Yol V8 is using a specific
4:44
format um and their own format for
4:46
actually like training these models so
4:49
right now let's go and Export our data
4:50
set we can select the format that we
4:52
want to choose so previously if you're
4:54
doing standard updation you'll be five
4:56
you be seven you'll be eight but now we
4:58
actually need to go up and choose this
5:00
yolo5 oriented bounding boxes and this
5:03
is not really the format that we can use
5:04
directly I'm going to create a custom
5:06
formatting script that takes this output
5:08
here from text files and convert it into
5:10
the specific one that they're using from
5:12
alter litics you can either download a
5:15
SI folder to your computer or show the
5:17
downloadable code that we can copy paste
5:19
directly into our Google collab notebook
5:21
so this is what you will get you will
5:22
also need to do this to act like get the
5:24
API key for your Robo flow account you
5:27
can also use the terminal or raw URL but
5:29
we just copy this and throw it directly
5:31
into our Google collab notebook and we
5:34
can export our data set run our training
5:36
script export our model and using in our
5:37
own custom Python scripts for our own
5:39
applications and projects so we'll just
Colab Setup
5:41
jump straight into the Google cab
5:43
notebook and walk you guys through that
5:45
so first all make sure that you actually
5:46
connect to your dpu you can go up to the
5:48
runtime change runtime type and then you
5:50
can specify if you want to use a dpu
5:52
here we want to do that when we're
5:54
training these uh deep learning machine
5:56
learning
5:57
models so let's just run this in Med SMI
6:00
here to start with just to get some
6:01
information about our graphics card the
6:03
GPU that we're using and also just to
6:05
make sure that we're act like utilizing
6:07
the GPU for
6:09
training so here we can see that we have
6:11
the Tesla T4 we can see the number of
6:12
gigabytes that we have in our dpu Ram um
6:15
and all this information here so that is
6:16
always pretty useful to do then we're
6:18
going to import our OS here just to set
6:20
up a home directory that we can use
6:22
later on because we're going to do a lot
6:23
of with our um directories and so on
6:25
because we need to do the mapping and
6:27
the conversion formatting of our data
6:29
said then we're going to install yate so
6:31
we're just going to P install to ltic
6:33
it's going to download the newest
6:34
version and that is pretty much
6:36
everything that we need to install it
6:37
will take care of all of it then we just
6:40
need some additional ones here for
6:41
displaying our images after it's done
6:43
training and so on and we also need to
6:45
be able to import YOLO as the
6:46
architecture so let's now just start the
6:48
installation then we're going to import
6:50
these different modules and dependencies
6:52
that we need later on Al lytics they
6:54
have some different like CLI Basics that
6:56
we can go and use as well so we can just
6:58
call this YOLO command and then there's
7:00
different tasks we have detect classify
7:02
segment but now we also have oriented
7:04
bounding boxes train predict validation
7:06
export we're going to use these
7:07
different modes throughout the notebook
7:09
and you can also specify the model
7:11
instead of specifying for example like
7:12
SE now we can just go in and specify OPB
7:15
and that is also how you can go in and
7:17
use pre-trained models already for that
7:19
and do inference on videos YouTube
7:21
videos webcam streams images and so on I
7:25
already have a video about that so
7:26
definitely check that out but we're also
7:27
going to do it later in this video here
7:29
so definitely stay tuned for that we're
7:31
going to run it on a live video stream
7:33
so now we have the setup complete here
Dataset and Formatting
7:34
as we can see now we can go down and set
7:36
up our custom data set so first of all
7:38
here we need to make a directory for
7:40
data set we CD into it so this is how we
7:42
can call command lines from the buun
7:45
terminal from roof flow we're going to
7:47
import rooff flow and this is just the
7:48
code s that I've copy pasted from roof
7:51
flow so right now I'm not showing my API
7:53
key but I'm just going to copy paste
7:55
that in I'm going to run this program
7:56
here or like this block of code and then
7:58
we're going to see the directory over
8:00
here to the left and see the file
8:01
structure so here we can see that is
8:03
downloading Robo flow setting that up
8:05
and also exporting our data set and that
8:07
should be done in just a second so right
8:09
now it's just setting up some um open CV
8:11
stuff let's you're going to see loading
8:13
rlow workspace loading the project and
8:15
then it's siing uh this data set here to
8:17
our um environment here in Google cab
8:20
notebook so let's go inside our
8:22
directory we have the data set that we
8:23
created cup segmentation to and then we
8:26
can see we have our test train and
8:27
validation split we have the data yl
8:30
file down here as we also need if we go
8:32
inside each of these individual folders
8:34
We have all the images so all the images
8:36
raw images and then we have our
8:37
annotations in this Yol V8 or ined
8:40
bounding box format so start with here
8:43
I'm just going to rename these folders
8:44
so we're going call it images and labels
8:47
I like to just do
8:49
that labels and we'll need to do it for
8:51
our
8:54
validation okay so now we have our
8:56
images and we also have our labels if we
8:58
just go inside and see see one of the
8:59
examples we can see it over here to the
9:01
left so right now this is the format
9:03
that they're using when we're exporting
9:05
it for roof flow and now we need to
9:06
convert it to the correct format from
9:08
ultral litics that they're using to
9:10
train the Yol V8 model on so we have our
9:12
X1 y1 X2 Y2 and so on all the way up to
9:17
um X4 and y4 so this is basically the
9:19
corners of our R bounding box and then
9:22
they also have the label this is not the
9:23
correct format for training this alter
9:26
litics model so if you just go inside
9:28
the documentation now for R bounding
9:30
boxes this is all the documentation that
9:31
you're going to go through they have
9:33
some examples they lift the different
9:34
models they have Nano small medium large
9:37
and extra large model how you can train
9:39
it so this is what we're going to use in
9:40
just a second but here we have the data
9:42
set format which is really important so
9:45
obb data set format can be found
9:47
detailed in the data set guide so let's
9:48
try to go inside that one our in the
9:51
bounding boxes data sets overview so
9:53
this is the supported obb data set
9:54
format the YOLO obb format so here we
9:58
have class index where before we had
9:59
class label and this is not the correct
10:01
order and then we basically just have
10:02
all the corners of our bounding boxes
10:05
but Yol V8 here also uses normalized
10:07
coordinates to the image Dimensions so
10:09
we also need to take care of that and
10:11
I've just created like a a script here
10:13
like a block of code where we can do all
10:14
of that conversion so here we can see
10:17
the different kind of like
10:18
representations that we can use as well
10:20
so this is how ideally it should look
10:22
afterwards to be able to train these
10:24
models so this DOTA V2 here is the data
10:26
set that they have pre-trained their
10:27
models on and they also have a script
10:29
for converting it from from the DOTA
10:31
data set to the YOLO r& bounding box
10:33
format so this is a custom format from
10:35
altic so we need to create our own
10:37
script that can convert and format our
10:39
files even though you're using another
10:41
annotation or labeling tool you most
10:43
likely need to go in and Export it and
10:45
actually like write your own custom
10:46
scripts to go in and do this formatting
10:49
of your data again if you're using the D
10:52
DOTA format you can just go in and call
10:54
this function directly but yeah we're
10:56
going to write a custom script that can
10:57
actually go in and create this form
10:59
format instead of the one that I just
11:00
showed you in a second before so here we
11:03
see we have this format let's just close
11:05
that and let's go down and take a look
11:06
at the script that we have done so first
11:09
of all here we can't really have any
11:10
spaces so let's make sure that we don't
11:12
have any spaces in our data so right now
11:15
I'm just going to go inside our labels
11:16
so again we saw in here with our labels
11:19
we like have this Dash here so we need
11:20
to make sure that we have that
11:22
throughout the whole data and also in
11:23
our yl
11:24
file just close this we have our data.
11:27
gaml so here we actually need to add
11:29
this
11:30
Dash so let's do that to start
11:33
with and again you guys can follow this
11:35
through exactly step by step and this is
11:37
how you can train the models then I also
11:39
like to go in and just take all of these
11:41
path individually so let's copy this
11:43
path here train validation and test
11:46
because sometimes it is not it doesn't
11:48
know that it's inside like content in a
11:50
Google cab notebook so here we have our
11:53
train we have our validation so that
11:55
will be valid and then we have our test
11:57
images here as well so this basically
11:59
just the data set or like the data file
12:01
um that that specifies like the config
12:03
for our training so now we have that we
12:06
can close that we have all the labels we
12:08
have the images so right now I'm going
12:10
to create this blogger code that is
12:12
going to do the conversion it doesn't
12:13
really do too much it's just taking the
12:15
class names you can take that directly
12:16
from the data yl file that you got from
12:19
roof flow just make sure that you act
12:20
like add these dashes so that's all you
12:23
have to do there then we specify the
12:24
image width and also the image height of
12:26
our images because we're going to
12:28
normalize all the values to be able to
12:30
train our models or else it won't work
12:32
then we can specify the folder path so
12:34
again you can also just create a
12:35
followup running through all these
12:36
folders but right now I'm just going to
12:37
do it individually for all the free
12:39
splits we also need our class indenes
12:42
because again we have individual like
12:44
labels so we don't have the
12:46
labels we actually need class indexes so
12:49
that's how we can go and create a
12:50
dictionary with our class name so we can
12:52
map that to specific values which is the
12:54
format that we need to use for allytics
12:57
and youate for or any bounding boxes so
13:00
we specify the follow path here content
13:01
data sets cop segmentation to valid and
13:04
dead labels so we only have to to
13:06
actually like convert the labels the
13:08
images or just raw images we don't have
13:09
to do anything with that we have a
13:12
function for normalizing the values we
13:13
basically just take the coordinate and
13:15
the image value or like the coordinate
13:17
the bounding box values then you divide
13:18
it by the maximum values which will be
13:20
the image height then we can go and
13:22
create a followup here basically just
13:23
running through all the different files
13:25
in our directory and in the folder that
13:27
we have specified up here then we're
13:29
going to take all the text files we're
13:30
going to take the specific file path
13:33
then we're just going to open up that
13:34
file read in all the lines then for all
13:36
of the lines we're just going to have a
13:37
follow Loop running through all the
13:39
lines so if you just go inside our
13:40
labels again we just want to extract
13:42
each of the these individual lines and
13:44
have a follow Lo running through each of
13:45
them doing the conversion and also just
13:48
saving them again into a new text file
13:50
in the correct format so that's pretty
13:52
much everything that is doing so if
13:54
length part is equal to 10 which means
13:56
that we have a correct um and a valid
13:58
annotation in our labels then we're
14:01
going to extract the labels which will
14:02
be the second last element we have the
14:04
coordinates which will be the first
14:05
eight elements and then we're going to
14:07
do this mapping from our dictionary from
14:09
the class labels to class IND index we
14:12
then check the class index if that is
14:14
valid we go down and normalize our
14:16
coordinates then we can create a new
14:17
line here which is just taking the class
14:19
index so this is the first index or like
14:21
the first value that we want in our ul8
14:24
or in a bounding box format and then
14:26
we're basically just going to join all
14:28
the different nor Iz coordinates
14:29
together so we have our X1 y1 all the
14:32
way up to X4 y4 we're just going to join
14:34
all of that together with spaces in
14:37
between then we can append this to the
14:39
new lines list that we have up here then
14:42
we can open up a new file and just write
14:43
out each of these individual lines that
14:45
we have just extracted and converted and
14:47
then we have everything so let's not
14:49
just try to run this let's start with
14:50
our validation so that is our test set
14:53
up here let's just run this block of
14:54
code and we should be able to do it it
14:56
did run let's go down and check our
14:58
validation if we go inside our labels
15:00
let take the first one here so now we
15:02
have our class index and then we have
15:03
all the values here for our bounding or
15:05
in the bounding box right after that so
15:08
this is the correct format and now we
15:09
can actually go in and train our model
15:10
but first of all we need to make sure
15:12
that we actually like do it on each of
15:13
these
15:16
um
15:17
folders so we have a test and then we
15:20
also need a train again you guys can
15:21
just like create a follow running
15:23
through all of this but again this took
15:25
like around 2 seconds to do that it will
15:27
take me 5 Seconds to create a fold
15:30
I'm trying to optimize my time um so
15:32
yeah let's go and check the other ones
15:33
here let's just take train we did that
15:35
at the last one the label in the correct
15:37
format so now we have everything we have
15:39
our whole data set we have annotated our
15:40
data set exported it done the formatting
15:42
and now we can just go in and train it
15:44
directly with the commands from alra
Train Custom Model
15:46
litics so let's now close this one now
15:48
we can go inside custom Training we have
15:51
our task obb for are in the bounding
15:53
boxes we set it into train mode we can
15:55
specify what type of model we want to
15:57
use again they have these five different
15:59
models depending on the size that you
16:00
want to use set the data data set.
16:03
location and then we need to specify our
16:04
data. JL file which is just mapping to
16:07
the directories with our images and also
16:09
our labels and that's also why I called
16:11
um the other folders for labels because
16:14
it's going to extract that we can
16:15
specify the number of epoch the image
16:17
size that we have export our data set
16:18
into and then we can also specify the
16:20
bat size so just change the bat size
16:22
here to eight because we have 640 images
16:25
so we should be good with that now we
16:27
can just run this blogger code and it
16:28
will start the training it's going to
16:29
set up the whole model going to download
16:31
the model start the training Loop and so
16:33
on and then we can follow the metrics
16:35
and follow the progress throughout all
16:37
of the epoch here so here we can see
16:39
that it has loaded in 330 images for
16:41
training validation 19 images um zero
16:45
corrupt labels so it it is actually able
16:48
to go in and extract all the information
16:50
correctly so that just verifies that all
16:53
of our annotations all of our formatting
16:55
and so on to the correct format is act
16:57
like done correct or you'll get like a
16:58
ton of warnings here so definitely make
17:00
sure that you check that if it look like
17:02
this you're good to go and your model
17:04
will start train we can even see here
17:06
like after the first iteration or like
17:07
the first Epoch we have a mean average
17:10
position 50 of
17:12
0.583 so that is already like pretty
17:14
good but again we're training on Cops
17:16
and that is already in in most of the
17:18
pre-train models and a relatively easy
17:20
task to do but now we already have like
17:22
a mean average position of close to one
17:24
so that is the ideal value so we can see
17:26
that already here after like the second
17:28
Epoch we already have like a model of 98
17:32
in mean average position .5 so that is
17:34
close to one which is the ideal value
17:36
and we can also see the meaners position
17:37
0.5 to 95 in intervals of 05 so it looks
17:42
pretty good our model is already like
17:43
pretty much converged so it doesn't
17:45
really matter like too much to train for
17:47
for longer maybe our class loss is
17:49
increasing as we can see here so yeah
17:51
kind of like makes sense to do that
17:52
until our class laws act like decreases
17:54
so let's just let it run here and then
17:56
we can take a look at the results
17:57
afterwards so our model is now done
Results in Collab
17:59
training we can see the validation
18:00
results down here at the bottom we can
18:02
see the number of images that we have in
18:03
our validation set we can see the
18:04
inference time and so on and we can also
18:06
go in and see all the different metrics
18:08
so again this looks very good our model
18:10
has prely converged and is very accurate
18:13
at predicting these cup that we have in
18:15
our data set so let's now go over here
18:17
to the left then we have our runs
18:18
directory Now obb train we have the
18:20
weights we can go and extract the best
18:22
weights here let's go in and download
18:23
that so we can use it in our own custom
18:25
script we can also take our last Model
18:27
um but in this example here they're
18:29
probably like both pretty similar we can
18:31
then go and extract both our Precision
18:33
recall curve if one curve confusion
18:36
Matrix and so on we can also see the
18:37
number of labels results but we can also
18:40
just go in and use these lines ex
18:42
directly so then we can go and take the
18:44
bit different images here both our
18:45
results validation batch and so on so
18:47
this is how we can see our model is
18:49
decreasing over time so we're taking a
18:51
look at the class Lots that's decreasing
18:53
pretty much converged at the end we can
18:54
see the metrics for precision it's going
18:56
up our recall is going up so our
18:58
Precision recoil curve is very nice
19:01
class loss is going down for our
19:02
validation set and also the mean
19:04
Precision is going up so we have a
19:06
really good model has Precision close to
19:08
one and also recoil close to one so we
19:10
probably can get a better model for this
19:13
we can take a look at a validation patch
19:14
here for our predictions and this is
19:16
basically the Rend bounding boxes that
19:17
we get for our cups so now we can
19:20
actually see it fits these bounding
19:21
boxes it rotates and Orient these
19:23
bounding boxes around our um Optics now
19:26
compared to just our traditional
19:28
bounding boxes that we know so this is
19:30
very awesome very useful in different
19:32
applications let's say that you have
19:33
like cars from aerial images driving
19:35
around now we can actually like follow
19:36
the cars better around we can get these
19:38
fitted bounding boxes so this is kind of
19:40
like segmentation but fitting our
19:42
bounding box around it raer to label and
19:45
so on if we have like an annotation tool
19:47
that can do our bounding boxes directly
19:49
compared to setting up instant
19:50
segmentation and so on but this is the
19:53
validation results look very good again
19:55
like 100% accuracy here 70% confidence
19:57
score 90 90% confidence
20:00
score so here we can see all the
20:02
different labels white cup Halloween cup
20:04
hand painted cup and so on and this is
20:06
the cup that we're going to use now so
20:08
now go down and validate costom model we
20:10
can just directly run that we're just
20:11
going to change our validation so I have
20:13
to stop this because our task is act
20:15
like
20:16
obb so task is obb do validation and
20:19
then we just need to
20:21
specify the directory for our best
20:23
trained weights so let going to run that
20:25
and use this model for validation
20:29
and we can do the exact same thing with
20:31
our predict we just specified the mode
20:33
set that equal to predict and we need to
20:35
change this to obb as
20:37
well train one because we only have one
20:39
train run confidence score where we want
20:42
to to have the images from the source
20:44
and if you want to save them as well
20:45
into directory so we can take a look at
20:47
it but again this is just the exact same
20:48
thing that we get after training so it's
20:50
running validation after the training
20:52
then we can do inference with our custom
20:53
model right now we're just going to run
20:55
through all of our test images so we can
20:57
go and visualize all of that then I'm
20:59
just going to have this follow running
21:00
through all the predictions so we can re
21:03
realize the results I usually like to do
21:04
that before we export the model and
21:06
using our own custom scripts because
21:08
just to make sure that our model act
21:09
like generalizes we get good enough
21:11
predictions or we want to iterate back
21:13
find too data set retrain our models
21:15
test and tune some different parameters
21:17
before actually like going in and using
21:18
in our own applications and projects so
21:21
now we have that we can go and run this
21:23
directly so this will be all the images
21:25
that we're just running through let me
21:26
just zoom out so now we can see that we
21:29
have these orang bounding boxes look
21:30
very good we're basically detecting
21:33
every single cup out there so here we
21:35
can see that because we actually like
21:36
annotating the the handle as well is
21:37
just trying to do the segmentation mask
21:39
and fit the orang the bounding box
21:40
around it that's why we get some some
21:42
weird bounding box in this example but
21:44
again you can use your own data sets
21:46
whatever data set that you want to use
21:47
out there if with your labels so you
21:50
actually get a double label here both
21:51
hand pained cup and also like a white
21:53
cup so that's a bit odd but we can see
21:55
that is fairly like pretty good in
21:57
general across all our chest images so
22:00
we downloaded the model we took the best
Inference
22:02
weight I've then imported into my new
22:05
directory here for a custom project that
22:07
we're going to set up so this is all the
22:08
code that we need to do from allytics
22:10
we're going to import YOLO we can create
22:12
an instance of our YOLO model then I've
22:14
just renamed my best model here to cup-
22:17
op for R Bounty boxes and this is how
22:20
you can use a custom model you can also
22:21
go in and specify the the YOLO model for
22:23
the pre models use that directly but I
22:25
have videos about that here on the
22:27
channel so you only need these free Lan
22:29
code import YOLO set up an instance of a
22:32
model and then run predictions we can
22:34
either like throw in a YouTube url here
22:36
URL to like whatever image on the
22:37
internet video webcam stream nonpr PL
22:43
image and so on we have a number of
22:44
different parameters that and also
22:46
arguments that you can find on the
22:47
alytic documentation but right now I'm
22:49
just going to show the results and also
22:51
save the results for later use so
22:54
everything that we need is this file
22:56
here we need our custom some train model
22:58
and then we also need a video that we're
23:00
just going to throw it through so this
23:01
is the video that I've just created this
23:03
is not the exact same background and so
23:04
on that I've trained the data on the
23:06
camera is also probably like a bit
23:07
closer compared to the data set but the
23:09
model still generalizes pretty well so
23:11
this is the image and this is the video
23:13
that we're basically just going to throw
23:14
through the model let's just do it let's
23:16
open up a terminal run the program and
23:17
see the
23:18
results so let's open a terminal new
23:21
terminal going to open a command prompt
23:24
cond
23:26
activate python ob. piy let's run it now
23:31
and see the results it's just going to
23:32
create an instance of our custom train
23:33
model and now it's going to run the
23:35
video I'll just drag it over here so we
23:37
have the hand pained cup it also detects
23:39
the Halloween cup here as well again
23:40
this is not the exact same data set and
23:43
background and distance camera and so on
23:44
that has been trained on so that is
23:46
probably why we get these false
23:47
predictions for um some of them but we
23:50
can see the hand ped cup it does a
23:51
really good extraction of that it does
23:52
the bounding box it rotates it fair nice
23:54
again we're not doing any tracking at at
23:56
all so right now I'm just going to put
23:58
it here on the table and now we can see
23:59
the results are um a bit
24:03
better so now we can see the results it
24:06
does detect something here on the left
24:07
with my airpods so it thinks that that
24:09
is a hand pain ined cup but again this
24:11
is not like this is not similar this is
24:13
like very far away from the data set
24:14
that we annotated but it is a bit old
24:16
data set so here we can see like when
24:17
I'm actually taking the camera further
24:19
away it detects it as a cup but if it's
24:21
too close it doesn't really recognize it
24:23
as the handp painted cup any longer so
24:25
here we can see that our R in bounding
24:27
boxes it is fall following the object
24:28
around so that is very nice now we can
24:31
also go and see inside our runs predict
24:33
free then we also get the results
24:35
extracted so you can use that video
24:37
later on because we have this argument
24:38
set equal to true for saving the video
24:41
and the results so this is very awesome
24:43
we have the Ron Bing boxes that rotates
Outro
24:45
around we get all these extractions you
24:47
can extract all the results again this
24:49
is just like an arbitrary video we can
24:51
go in and F tune our data set make it
24:53
more General have different backgrounds
24:55
have different distances so again
24:56
sometimes we can see that the video or
24:58
the camera was just too close to the cup
25:00
to be able to detect it because it was
25:01
like trained further away and also with
25:03
a different background orientations and
25:05
so on but just sort of validation
25:06
results when we trained the model so
25:09
that's pretty much everything we've been
25:10
through the whole pipeline for training
25:11
these R the bounding box models for Yul
25:15
8 so I hope you guys have learned a ton
25:17
definitely try it out I'll throw the
25:18
Google cab notebook under the video here
25:19
in the description check it out try it
25:21
out your own custom data set this is
25:23
really useful for opdate detection
25:25
applications where you want to have like
25:26
these bounding boxes oriented rotating
25:29
together with your Optics instead of
25:30
just having these Statics and
25:32
non-dynamic bounding boxes as we have
25:34
been used to with 8 and also a ton of
25:37
other different optic T and models out
25:39
there so I hope you have learned a ton
25:40
definitely try it out I hope to see you
25:42
guys in one of the upcoming videos until
25:43
then Happy learning

Search in video
Introduction
0:00
hello everyone and welcome back to the
0:02
channel in today's video we are going to
0:03
do custom object detection using YOLO
0:06
v11 this is going to be a two glass
0:08
object detection and here are the steps
0:11
that we are going to take first we will
0:13
set up the environment for YOLO
0:15
v11 then we will annotate images in Yolo
0:19
format and after that we will train YOLO
0:22
v11 object detection model and finally
0:25
we will run custom object detection on
0:27
images videos and webcam
0:30
the time stamps would be in the
0:32
description below let's get
0:34
started all right let's create an empty
Setting up Virtual Environment
0:37
directory and call it yolo v11 _ custom
0:43
now open Anaconda
0:46
prompt and go to that directory which is
0:49
in D drive and then the coding bug YOLO
0:53
v11 uncore custom now let's create a new
0:57
environment cond create minus n yolow V1
1:01
_ custom Python =
1:04
3.11 and minus y hit enter once that is
1:09
done let's activate that environment by
1:12
command cond activate YOLO V1 _ custom
1:16
so now you can see the environment is
1:18
activated let's clear the
1:20
screen now we will install ultral litics
1:23
by command pip install ultral
1:28
litics after it it completion let's run
1:33
Python and import
1:35
torch and run command torch. ca. isore
1:40
available now this will tell us if our
1:43
GPU is being used so here it's false so
1:45
we need to fix that let's exit
1:49
it go to official by torch website and
1:54
on get started page
1:56
here just select latest build of pytorch
2:00
for your operating system using pip and
2:02
for Python and you have multiple options
2:05
for Cuda I am going with Cuda 12.1 and
2:09
just copy this command paste here and do
2:12
not forget to add minus minus upgrade
2:14
flag after install and hit enter it will
2:18
uninstall the older version and install
2:20
the newer version now let's verify if
2:23
our GP is being detected by pytorch so
2:27
type Python and then import tor T and
2:30
again run command torch. ca. isore
2:33
available and it prints through that
2:36
means it is going to use our GPU when
2:38
we'll train our custom model now head
Annotating custom dataset in YOLO format
2:41
back to our YOLO v11 custom directory
2:43
and create a new folder called images
2:46
and here I will paste all the images
2:48
that I'm going to use for our custom
2:51
object detection these are images for
2:54
traffic signals and here is the video
2:57
that we will use later for testing
2:59
purposes
3:00
now let's install another library with
3:03
command pip install label Das Studio we
3:07
will be using this library to annotate
3:09
our images after it is installed let's
3:12
run it by command label Das
3:16
Studio it is going to run on the local
3:19
machine and it's going to ask you to log
3:22
in so my account is already saved here
3:26
but if you do not have any account you
3:28
can always click on sign up
3:30
and it's very easy to set up your
3:32
account with your email address and
3:34
password so my account already exists so
3:38
I'm just going to use that to log in and
3:39
here you can see all my previous
3:41
projects that I
3:43
did so for this video we are going to
3:46
create a new project let's rename it to
3:49
YOLO V1 custom and then head to data
3:53
import and click on upload files here we
3:57
are going to select all the images that
4:00
we just copied and open them and all of
4:03
these images are imported here so now
4:05
head to labeling setup and from computer
4:08
vision we will select object detection
4:11
with bounding boxes and here are two
4:14
default classes that we are going to
4:15
remove and we'll add our own labels here
4:19
so as this is traffic signals data set
4:21
we are just going to Define two classes
4:22
red light and green light and we'll add
4:25
them we can also change colors of each
4:28
class just to distinguish them when we
4:31
annotate the
4:33
data now hit
4:37
save here we can see all of our images
4:40
and we can change its view to grid and
4:44
now click on label all tasks so here we
4:47
can see our two classes that we defined
4:50
earlier these have hot Keys press one to
4:54
select red light and draw a bounding box
4:56
press two to select green light and draw
4:58
a bounding box
5:00
be as precise as possible I'm just going
5:02
to do it quickly for demonstration
5:04
purposes once you done on one image
5:07
click on submit and it will present you
5:08
the next image repeat the same process
5:11
on the next image once you are done with
5:13
all the images you will see this message
5:16
no more tasks left in the queue so at
5:18
this point you can head back to YOLO V1
5:21
custom and then click on export and here
5:24
you have multiple formats that you can
5:26
use to export your annotations we are
5:28
going to select your yo and then click
5:31
on export it's going to give us a zip
5:34
file so we're going to save it in the
5:37
same directory that we're working in
5:39
Yolo v11 custom and here we have that
5:42
file we do not need this older images
5:45
folder we just going to delete that now
5:47
extract this zip folder and you will see
5:49
multiple files here so this classes. txt
5:53
contains the information about the
5:55
classes that we defined earlier in label
5:57
studio and these two folders have have
6:00
images and their corresponding labels we
6:02
are just going to copy these two
6:04
folders head back to root folder create
6:08
another folder called train and one more
6:11
folder called well so I'm just going to
6:15
paste the images and labels directory
6:17
that we copied earlier here in the train
6:19
folder and here you can see all the
6:21
images and here you have all the
6:23
corresponding labels for those images
6:27
now we also need some images for for
6:30
validation purposes so I'm just going to
6:32
cut last five images from images folder
6:37
head back to root folder and then Val
6:39
folder and here create another folder
6:42
images and paste those images here now
6:46
we also need their corresponding labels
6:49
in validation folder so I'm just going
6:51
to cut last five annotation
6:55
files from labels directory in the train
6:58
head back to well folder create another
7:01
directory called
7:02
labels and paste them here so now we
7:07
have two folders train which has images
7:10
and labels and another folder Val which
7:12
has its own images and labels we do not
7:15
need this ZIP file so let's delete that
7:18
now let's create a script file called
7:20
train.py I'm going to open it in Sublime
7:24
Text but you can use ID of your
7:27
choice create another text next file
7:30
called Data setor custom.
7:34
yml this file is going to contain the
7:36
information about the custom data set
7:38
that we are going to use to train YOLO
7:41
v11 open this file in text editor as
7:44
well so I'm going to Define path for
7:47
train it's going to be this train
7:49
directory that we defined earlier so I'm
7:51
just going to copy this path from here
7:53
and paste here then for Val Define the
7:58
path to our Val folder so I'm again
8:01
going to repeat the same process and
8:03
copy the path in this yml
8:06
file now Define NC which is total number
8:09
of classes as we have just two classes
8:11
so write two and now we need to Define
8:15
what those classes
8:16
are so names this is going to be a list
8:19
so now on the Zero index just copy the
8:21
class names from the text file that we
8:23
saw earlier from zero index green light
8:26
goes here from index one red light right
8:30
goes here so now we have defined these
8:33
two class names and that's it our custom
8:37
data set file is ready now head back to
Train YOLO v11 custom
8:39
train.py
8:40
and from ultral litics import
8:44
Yello now Define model equals YOLO and
8:48
here we are going to define the model
8:50
that we are going to finetune and you
8:52
can find the list of all the available
8:54
models on ultral litics GitHub
8:56
repository scroll down and in the
8:59
detection section you will see all the
9:02
available models so every model has its
9:05
own pros and cons smaller models are
9:08
faster but are less accurate larger
9:11
models are slower but more accurate so
9:14
I'm just going with the middle ground
9:16
and I'm going to select YOLO v11 M which
9:19
is medium model so click on this and it
9:22
will start downloading a PT
9:25
file here you can see the file is
9:28
downloaded so head back to our train.py
9:31
file and write the file name of this
9:33
downloaded model here YOLO V1 m.pd so
9:37
now call model. train and inside train
9:40
we have to Define certain parameters so
9:43
data is equal to this data setor custom.
9:47
yml
9:51
file imgsc which is image size it's
9:55
going to be 640 as we saw earlier on
9:57
tics getup Repository batch equals
10:01
8 EPO the total number of epo would be
10:05
100 and workers equal one and device
10:10
equal 0 for using GPU 0 or if you're
10:13
using CPU just write CPU here but as we
10:16
have GPU so I'm just going to write zero
10:19
so let's move it to the second file so
10:22
that we can see all of the parameters so
10:25
head back to Anaconda prompt label
10:27
studio is still running so press contrl
10:28
C to exit it and then clear screen now
10:32
run command python train.py
10:37
so there is an error okay so I
10:41
misspelled epox so let's head back to
10:43
our script and correct
10:47
this now run the command again python
10:50
train.py now the training would start
10:54
and we can see the 11 images are there
10:56
in train and five in the F folder and
11:00
those are loaded
11:01
successfully but if you encounter this
11:06
error that data loader worker exited
11:10
unexpectedly so what you have to do is
11:12
go back here and set workers to
11:15
zero now if you'll run training script
11:18
again it's going to start the
11:25
training there we have it so while the
11:28
training is still running let me explain
11:31
a few things if you add more images into
11:33
your train folder you need to delete
11:35
this cachier file labels. cache and
11:40
similarly if you add more images into
11:42
Val folder you need to delete this cache
11:44
file as well otherwise it will not use
11:46
the newer
11:47
images inside runs folder we can see we
11:51
have another folder detect which has all
11:53
our training attempts so our latest
11:56
attempt is train five so inside that you
11:59
will will find vits folder and here we
12:01
have best. PT that we are interested in
12:04
so once the training will complete we
12:05
are going to come
12:08
back so once the training is complete
12:11
you will be able to see all the
12:12
statistics of the training for each
12:14
class and the file that we are
12:16
interested in is this best. PT so let's
12:19
just copy this and head back to our YOLO
12:22
V1 custom folder and paste here and I'm
12:24
going to rename it to YOLO V1 custom
Run custom object detection on images
12:29
that's it training is complete and now
12:31
we will move to inferencing on images
12:34
videos and webcam let's create another
12:37
script called predict dopy open that
12:40
script and again from ultral litics
12:43
import YOLO and model equals YOLO and
12:47
here we are going to give file name of
12:49
this custom model that we just renamed
12:51
YOLO V1 custom. PD and now we are going
12:55
to call model. predict so to run
12:58
prediction image let's take one image
13:01
from Val folder paste here rename it to
13:05
1.jpg and now in this model. predict
13:08
Source equals 1. jpg now show equals
13:14
true and save this file now in Anaconda
13:18
prompt run python predict.
13:21
piy now here are the results but the
13:25
results are not saved anywhere so if you
13:27
want to save this prediction
13:30
head back to your prct dop script and
13:34
set parameter save equals true save the
13:38
file and run the script
13:42
again this time it's going to save the
13:45
output in runs detect and here we have
13:49
predict and inside predict we have this
13:52
predicted
13:56
image I'm going to delete all these
13:59
remaining folders so we just have
14:01
results of predictions okay back to
14:04
break. Pi we can set confidence so conf
14:08
equals 0.6 so all the predictions that
14:10
have confidence lower than 0.6 are going
14:13
to be
14:14
discarded here we have the result but
14:16
all predictions had a confidence of more
14:19
than 0.6 already so nothing
14:21
changed if you do not like the size of
14:24
the text or the thickness of the lines
14:26
you can set another parameter line _
14:29
thickness you can set it to three or
14:32
four whatever you like so by setting it
14:35
to one let's run the script again or it
14:39
says line uncore thickness is going to
14:41
be obsolete you should use LINE
14:42
underscore width anyways here you can
14:45
see the size of the text and the width
14:47
of the line is changed so head back to
14:50
our script and change this parameter to
14:53
line _ width and this time I'm going to
14:55
set it to two so let's run the script
14:58
again no warning this time that's good
15:02
so here are the results and I like this
15:05
text size so let's keep it to two so if
15:08
you want to save the cropped objects
15:11
that are detected so you can set another
15:13
parameter savecore crop equals true and
15:16
now if you'll run the script it's going
15:19
to save all the detected lights in this
15:23
crops folder so green light have all
15:26
these three green lights that were
15:28
detected if you need the annotations in
15:31
Yolo format you can set another
15:32
parameter save _ txt equals true and
15:36
this time if you will run this script
15:38
inside brick folder you will have this
15:39
labels folder as well which will have
15:42
these annotations in the YOLO format now
15:45
if you do not want to see the labels on
15:47
the predictions you can set hide _
15:49
labels equals true and to hide
15:53
confidence you can set hide _ conf
15:55
equals
15:56
true oh these will also be Obsolete and
16:00
we should use show labels and show conf
16:02
okay here is the result and you can see
16:04
there are no confidence or labels shown
16:07
on these predictions so let's change
16:10
these two parameters to showcore con and
16:15
this to show underscore labels finally
16:17
we can set classes equals which would be
16:19
a list so which classes you want the
16:23
model to predict so we have two classes
16:25
0 and one so we are going to use those 0
16:28
for for green light and one for red
16:31
light so if you just want to predict
16:34
green light just set it to zero and if
16:36
you want to predict red light just set
16:39
it to one so but I'm going to use both
16:41
so 0o and one so now if you want to run
Run custom object detection on videos
16:44
on the video so how would we do that so
16:47
just change this source to traffic
16:49
lights. MP4 and let me set save crop to
16:54
false and save tx2 to false and maybe
16:56
set confidence to 0.7 and now if we will
17:00
run this script it's going to run
17:02
prediction on the video and now if we go
17:05
to detect and then predict we can see
17:07
the output of that video with all the
17:10
predictions I'll set this confidence
17:13
back to
17:13
0.6 so now if you'll like to run the
Run custom object detection on webcam
17:16
custom object detection on some IP
17:18
camera you can set the address of that
17:21
IP camera here as source and it will
17:23
work on that and if you like to run on
17:26
local webcam just set it to zero
17:29
and run the predict script again and you
17:33
will see the output from your local
17:36
webcam now head back to predict. piy I'm
17:39
going to comment this code and we'll see
Export YOLOv11 to ONNX or TFLITE
17:42
how to export this model to another
17:44
format so model. export format equals
17:49
and I'm going to export is as onx so on
17:53
andx now run this script and you will
17:56
see this YOLO v11 custom do onx file now
18:01
which other formats are supported you
18:03
can check that on the official ultral
18:05
litics documentation so I'll put that
18:07
link in the description so here if you
18:09
scroll down you can see all of these
18:11
formats are supported so we use this onx
18:14
so if you want to save as tensorflow you
18:17
can use savore model and if you want as
18:19
tensorflow light you can use TF light so
18:23
just change this format here as DF light
18:25
and it's going to export that or you can
18:28
use saved score model now one more thing
YOLOv11 Custom Object Detection Command Line
18:31
you can run all of these from the
18:34
command line directly without using
18:36
Python scripts so how to do that let me
18:38
show you quickly so in the command
18:40
prompt write YOLO detect predict model
18:44
equals yolow v11 custom. PT and Source
18:48
equals 1.jpg this is going to produce
18:51
identical results to the python script
18:54
that we did earlier and here you can see
18:56
the output is saved in predict 10 and
18:58
its same output so if you would like to
19:01
train from the command line you can use
19:03
YOLO detect train model equals this time
19:07
use YOLO v11 n. PT and then data equals
19:12
data setor custom. yml eox equal 100
19:16
image size equals 640 workers equals 0
19:21
device equals z so basically all the
19:25
parameters that we used in the Python
19:27
scripts patch equals 8 this is going to
19:30
start the training and here you can see
19:32
the output of this training just like
19:35
before with that I think I'm done if you
19:39
have learned something of value today
19:41
hit like And subscribe to the channel
19:43
consider a support on the patreon to
19:44
help the channel out I will see you next
19:46
time
19:48
[Music]
19:55
[Music]
Transcript


Search in video
0:00
hi guys my name is B Ahmed and welcome
0:03
to my YouTube channel so guys in this
0:05
video we'll be learning about how we can
0:07
perform image classification with the
0:08
help of Yello 11 so I think you already
0:11
know in my YouTube channel I started
0:13
Yello 11 Series so here I think you
0:15
remember I was discussing about what is
0:17
YOLO 11 that means with the help of YOLO
0:19
11 what kinds of task you can perform
0:21
like I showed you we can perform object
0:23
detection segmentation po estimation
0:24
image classification and oriented
0:26
bounding box then I also showed you how
0:28
we can perform the custom object
0:30
detection training with the help of
0:31
yellow 11 then I already covered the
0:34
instance segmentation on top of our
0:35
custom data as well then I think I told
0:37
you I will also show you the remaining
0:39
task like how we can perform the image
0:41
classification POS estimation these are
0:43
the task as well so in this video I'll
0:45
show you how we can perform the custom
0:47
image classification with the help of
0:49
yellow 11 so for this guys what I have
0:51
done I have created one amazing notebook
0:53
so in this notebook actually I have
0:55
prepared all the code you need uh to
0:57
perform this custom training so if you
0:59
have already check my previous video so
1:01
I think you will be able to understand
1:02
this video because I'm going to use the
1:04
previous knowledge only so that's why
1:06
I'm not going to again discuss
1:07
everything from scratch so please try to
1:09
go ahead and watch these are the video
1:11
then you'll be able to understand okay
1:12
what I'm trying to say here so first of
1:14
all let me open the yellow uh 11 GitHub
1:17
so this is the yellow 11 GitHub guys and
1:20
this is from allytics so they published
1:21
this amazing model and you can see uh
1:24
they have also given the model zo so it
1:26
is having different different model zo
1:27
so we already covered the detection
1:29
segmentation now today we'll be covering
1:31
the classification and these are the
1:32
classification model okay and this is
1:35
the notebook and to use the alter litics
1:37
first of all you have to install the
1:38
alter etics package so let me first of
1:40
all install this package so make sure
1:41
you connect this notebook uh with your
1:43
GPU so you will get free GPU if you're
1:45
using free collab so T4 GPU you can get
1:48
and try to save and try to install this
1:50
allytics package and all the resources
1:52
would be given in the description from
1:54
there you can download and you can uh
1:55
execute with me so guys as you can see I
1:59
have successfully installed this alter
2:00
litics now the next thing uh you need to
2:02
get the data classification data so for
2:05
me I already prepared the data let me
2:06
show you how your data will look like so
2:08
here I collected uh actually data
2:10
related chicken fle okay so you can see
2:12
so this is the data related chicken
2:14
disease okay so chicken is having
2:15
different different kinds of disease so
2:17
one of the disease actually will see
2:19
chicken will be affected by coxy diois
2:21
okay so this is the disease guys so let
2:23
me show you this disease if you search
2:26
this coxy dioses in
2:28
Google so this is one of the disease
2:31
okay affected by chicken see okay now
2:34
we'll just try to
2:36
classify this particular disease with
2:38
the help of chicken Fel so that's why
2:40
I've collected some chicken Fel image so
2:42
let me show you so this is the coxis
2:44
affected chicken fele so here is the Fel
2:47
image guys okay so it uh you can see it
2:50
is kinds of yellow okay yellow color so
2:53
this is the coxy diis affected chicken
2:55
fle now there is another folder I'm
2:57
having called healthy okay so and this
2:59
is the healthy fle okay you can see it's
3:01
a black and white kinds of fle so this
3:03
is the healthy fle okay so with the help
3:05
of this particular image we can classify
3:07
whether this chicken has been affected
3:09
by coidis disease or this chicken is
3:11
healthy okay now this is the data format
3:14
guys you can see so this is my main
3:16
folder called Chicken F image inside
3:17
that I'm having two folder called
3:18
training and validation if I open the
3:20
training inside that you'll see I'm
3:22
having two level one is the coxy diois
3:24
okay other is the healthy now if you
3:26
have multiple level what you can do you
3:28
can create another folder okay with help
3:30
of that name and inside that you can
3:31
keep the image okay so you can see
3:33
inside this folder I'm having some of
3:34
the image around 200 image actually I
3:36
kept for the healthy also I kept around
3:38
200 image make sure your data should be
3:40
labeled okay let's say you are keeping
3:41
100 image in the coxy diis and 50 image
3:44
in the healthy so your data would be
3:45
imbalance so make sure always keep your
3:47
data Balan okay so if you're taking 200
3:49
image also try to keep in the other lbel
3:51
200 image okay that's how we can balance
3:53
your data now inside validation folder
3:56
also I created this two folder called
3:58
coidis and healthy inside that I kept
4:00
the covid image and inside healthy I
4:03
kept the healthy image okay that's how
4:05
if you have multiple folder multiple
4:06
level you can create the multiple label
4:08
and you can keep your data and now you
4:10
can ask me where I will get these kinds
4:11
of data no need to worry there is one
4:13
amazing website um you can refer called
4:17
roof flow okay so roof flow is a
4:19
complete computer vision actually
4:20
platform so here you can uh get any
4:22
kinds of tools and technology related
4:24
computer vision so in my YouTube channel
4:26
I already covered lots of tutorial
4:27
related roof flow like how we can use
4:29
roof flow platform to anate our data how
4:31
we can get the different different kinds
4:33
of data set how we can use the
4:34
supervision okay see it is also having
4:36
different different products like it is
4:37
also having supervision okay then Auto
4:39
annotation everything I already covered
4:41
in my YouTube channel you can also check
4:42
them so I will use one product called
4:44
Universe okay let me open the universe
4:47
so inside this universe all kinds of
4:48
data are available only you just need to
4:50
come here and you need to class uh you
4:51
need to select what kinds of task you
4:53
want to perform let's say I want to
4:54
perform classification you can filter
4:56
out with the help of classification you
4:57
can even search here okay let's say I
4:59
need this particular image you can also
5:00
search here let's say I need chicken
5:02
disease okay so what I will do I'll
5:04
search by chicken disease so see guys if
5:06
you search chicken disease you will get
5:07
different different kinds of chicken
5:08
disase kinds of image okay now you can
5:09
directly download these are the image so
5:12
if you want to download let's say I want
5:13
to download this image I'll just click
5:16
here now there is a option you will get
5:19
called Download Project just click on
5:20
the Download Project then you will see
5:22
this option called download data set
5:23
just try to click here and you need to
5:25
select the folder structure form it okay
5:27
because it's a classification so here
5:28
you don't need to select any kinds of
5:31
other format okay try to select the
5:32
folder structure because if you see the
5:34
uh classification level the level would
5:36
be folder structure so you have to uh
5:39
write the label name inside the folder
5:41
you can see this is the folder coxy diis
5:43
and healthy your model will
5:44
automatically get this name from here
5:46
okay so that's why we always need to
5:47
follow the folder structure whenever we
5:49
are doing the classification task and
5:50
for object detection segmentation I
5:52
think I already covered okay how to
5:53
prepare the data and how to format the
5:55
data now simply just click on um
5:57
continue after selecting the ZIP down
5:59
downloader you can also directly
6:00
download from the code but I will try to
6:02
uh ZIP and
6:04
download so guys after downloading it
6:06
you just need to unzip the folder and
6:07
after unzipping you can see you will get
6:09
similar kinds of folder but here you
6:11
don't need the test one you can delete
6:12
the test one so I deleted the test one
6:14
and I get only the train and validation
6:16
okay so this is my folder structure okay
6:18
now I think you got it how to get the
6:20
data okay now how to perform the
6:22
training for this uh what you can do you
6:24
can upload this data in your Google cor
6:26
for this just try to make a z file of
6:28
this folder uh either you can directly
6:30
download from the r also it's completely
6:32
fine now this Z file you just need to
6:34
upload uh inside the Google collab so
6:36
let me upload
6:38
here so this is my image I'll just try
6:42
to upload here so guys you can see it
6:44
has uploaded successfully now first of
6:46
all I have to unzip this folder so for
6:48
this you can execute this line of code
6:50
just copy the path and try to mention
6:52
the path of your zip file okay now if I
6:54
unzip so it will unzip your data and it
6:57
will give you uh this one so let me show
7:00
you so this is the folder inside that
7:02
you are having the trending image as
7:03
well as the validation image okay so now
7:06
finally I'm going to prepare my training
7:08
code and if you're using uh yellow 11
7:11
with the help of Alterna it's like very
7:13
easy to do so only just need to import
7:15
Yolo from alter litics after that you
7:17
just need to give the model which model
7:19
actually you want to use as a pre-end
7:20
model and you can see here we are having
7:22
different different kinds of Model N
7:23
Model S model M model and I already told
7:25
you what is n Model S model and Model N
7:27
means Nano the smallest medium large X
7:29
lar okay now I'm going to use this model
7:31
and Nano model so here I'm given the
7:33
model ID okay now simply you just need
7:36
to give the data location so this is my
7:37
data location guys you just need to
7:38
provide the folder path it will
7:40
automatically load the data number of
7:42
ebok you want to train I want to train
7:43
only 20 eok and you'll see that after
7:46
training 20 this model will perform
7:47
amazing okay it will as you actually
7:49
100% accuracy okay so uh it's amazing
7:53
model guys they have created now this is
7:55
the image size that means the during the
7:56
training they used 640 dimensional image
7:59
so that's why I've given the size now
8:01
see if I execute so first of all it will
8:03
download the model from the internet
8:05
then it will load the data okay after
8:07
that it will start the training so guys
8:09
as you can see my training has started
8:11
and it has created one runs folder
8:13
inside runs actually it will save all
8:14
the artifact like your weits your all
8:16
the matrixes everything it will say so
8:18
this training will take some time so I
8:20
will pause the video Once training is
8:21
completed then I will come
8:25
back so guys as you can see my training
8:27
has completed and uh this is is the
8:29
accuracy score you got top accuracy one
8:32
that means 100% accuracy you got so in
8:35
this case actually what you can do you
8:36
can decrease the EPO size I trained uh
8:38
20o you can also train 10 EPO in the 10o
8:41
also you can get the best accuracy here
8:43
so now if you see in the runs folder it
8:45
has created uh some of the folder Let Me
8:47
Show You Just Let me
8:48
refresh so it has created one classify
8:51
folder inside that train and weights
8:54
okay this is your weights the model
8:55
actually you have trained and these are
8:57
the uh evaluation metrics so let's plot
9:00
some matrixes and let me show you the
9:03
model performance for this I'm going to
9:05
use this image function from the display
9:08
and see this is the result.png inside
9:11
that I'm having my result.png so this is
9:13
what I want to show you so result.png
9:16
this one okay just copy the path and
9:19
mention it here and now if I execute
9:21
you'll see that this is the performance
9:24
of your model so this is the time uh
9:27
this is the Matrix you see uh you got
9:29
actually 100% accuracy this is the loss
9:31
and this is the loss as per your TR uh
9:33
epox is increasing loss is also
9:34
decreasing and this is the accuracy that
9:36
means your uh Epoch is increasing
9:38
accuracy is also increasing okay that me
9:40
model uh is learing better now let's do
9:43
the inference so I'll load my train
9:45
model so the model actually I have train
9:47
which is available inside here inside wa
9:49
folder copy the path and try to mention
9:52
it here now here I'll give some testing
9:54
image to test okay whether it's working
9:55
fine or not so previously I already
9:57
executed let me again show you
9:59
so I will upload some coxy diis affected
10:02
Fel then I will also upload some healthy
10:09
Fel you can also directly upload the
10:12
folder and you can give the folder
10:13
location it will load all the image and
10:15
it will do the inference but here I'll
10:17
show you the individual image okay so
10:19
first of all let me show you the coxy D
10:20
is affected I'll copy the path and give
10:23
the image path here now see if I execute
10:25
the code it will load the model and it
10:27
will load the image now see coxy diis
10:30
100% that is that means this image has
10:32
been classified with the coxy diis
10:34
affected you can see healthy is 0.0 okay
10:36
so that's how it is classifying and it's
10:38
amazing you can see 100% confidence your
10:40
model it's a coxy affected image now see
10:43
if I refresh you can't see the image has
10:45
been saved okay and if you want to get
10:46
the image what you can do you can give
10:48
this parameter save is equal to True
10:49
here I also commented and let me show
10:52
you here you can also give The
10:53
Confidence Code like if your model is
10:55
50% confidence this image is a coxy is
10:58
affected that time it will show you okay
11:00
so you can also play with The Confidence
11:02
Code so here let me give you another
11:03
image so let's say I'll give this
11:05
healthy image and now if I execute
11:08
you'll see that it will show you even it
11:10
will also save inside the folder if you
11:11
want see this is the healthy image one
11:14
uh one that means 100% confidence the
11:16
Heth the image Co is 0.0 now if I
11:18
refresh inside classify see it has
11:20
created one predict folder inside
11:22
predict you will see this image has been
11:24
saved okay now if you want to also plot
11:26
the results like what is inside results
11:29
you can also see that okay you'll see
11:31
the complete uh like label as well as
11:34
the um array okay n array of the
11:37
predicted image so yes guys that's how
11:39
we can perform the custom uh image
11:41
classification with the help of yellow
11:42
11 and now we can pick up any kinds of
11:45
data set and you can do the
11:46
classification on top of it so yes guys
11:48
this is all from this video I hope you
11:50
liked it and if you liked it guys so
11:52
please try to share this video with your
11:53
friends and family with that guys thank
11:54
you so much for watching this video and
11:56
I will see you next time
Transcript


Search in video
Introduction to YOLOv11
0:00
YOLO 11 is out and in this tutorial we'll find unit for object detection on
0:06
custom data set this guide provide a comprehensive step-by-step work through
0:12
of the entire process whether you already have your images or you need to find the data no data set no problem
0:20
we'll start by finding free already annotated data sets for object detection
0:26
already have images I'll show you how to label them for training and create your data set next we'll train YOLO 11 model
0:34
we cover training in two distinct environments your local machine and
0:39
Google callab once we do it we'll evaluate the trained model's performance and save it finally we'll deploy the
0:48
model run it in browser and use it with python SDK we have a lot to cover so
0:54
let's get started any time that I need to train the model but I don't have the
Finding Free Annotated Datasets for YOLOv11
0:59
data set the first place I look is roof flow Universe there are already half
1:04
million data sets on the website and most of them are for object detection so
1:10
there is a high chance that somebody already annotated the images that you or I need for training and just scrolling
1:17
through the website we can see that those data sets come from different use cases there is some foodball data set
1:25
there are some medical images some aerial images there is data set to the detect fire and smoke there are some
1:33
data sets related to video games and data sets to detect safety equipment and
1:39
as a matter of fact the data set that I will use today in this tutorial also comes from roof flow universe so some
1:46
time ago I got interested in document understanding and I was curious if I
1:51
will find a data set that will help me out to process arive uh papers and
1:57
that's how I found this TFT ID data set and the cool thing is that you can go
2:04
into those images take a look at the annotations evaluate the quality and
2:09
yeah that data set is exactly what I needed to make this tutorial like I said many of you
Image Labeling for YOLOv11
2:16
probably already have the images or plan to collect them you may scrape the internet or take images with your phone
2:22
or scan documents and if that's the case let me quickly show you how to annotate
2:27
them there are of course multiple options on the market some of them open source like cat or my open source
2:36
project called Mak sense but if you're looking for something that is free to use and with minimum hustle roof flow is
2:43
probably your best option and of course I don't say that because it's roof flow
2:49
YouTube channel you can totally trust me on that no but honestly I think it's the
2:54
best so let me show you how to use it so for the next few minutes I will will
3:00
pretend that I don't have the data set that is fully annotated already and I will use the sap set of images from TFT
3:07
ID data set to guide you through the whole labeling process the first thing
3:12
that you need to do is to log into your roof flow account or create one if you don't have one already and once you do
3:20
it you will probably end up in this main workspace view here I click the new
3:26
project button that is in top right corner and that takes me to project creation view now I can provide the
3:33
project name I'm just replicating the TFT ID data set so that's the name that I'm going to use and I need to provide
3:41
annotation group this is pretty much the answer to the question what objects am I going to annotate and in my case those
3:47
are going to be research papers we select project type here object
3:53
detection and create a public project the first thing that you should do once
3:58
you create a fresh project is Define the list of classes that you would like to annotate like I said I replicate TFT ID
4:05
data set so in my case I will use figure table and text once you're happy with
4:11
the list of classes you can click add classes button success yay great
4:17
successful and now the next step is to upload your data to the project to do
4:24
that we go to upload data View and here we just drag and drop our images and and
4:30
once we are happy with the list of images that you would like to add to your project we click save and continue
4:36
depending on the size of those images and the amount of images that you uploaded that process can take between
4:42
few seconds and few minutes the next view allow us to pick the way that we
4:47
are going to annotate the data set you can use the auto annotation uh
4:52
functionalities that are in roof flow but for this particular data set I don't really recommend that we can hire
4:58
somebody to annotate the data for us or we can manually annotate the data set
5:03
and this is the option that I'm going to use here that view will probably look a
5:09
little bit different for you uh I have a lot of people in my workspace and I can
5:14
just invite them to help me with annotation but uh for this particular
5:20
job I will just assign uh those images to myself and here it is our first
5:28
annotation job is is created and I can just start annotating uh so I click Start
5:35
annotating button this takes me to the editor where I can move between images and let's say that I would like to
5:41
annotate this one I just zoom in I draw the bounding box around the object that
5:46
uh is interesting to me I pick the class I submit and I just do it for every
5:52
object visible on the image so I had uh two more text Fields one more table uh
5:59
over here here and one more text uh
6:04
below the table I try to make all the bounding
6:09
boxes uh consistent so the padding around the object is the same here is
6:15
another a page from uh a research paper uh a large uh text
6:21
field um here's another text and below it uh there is a small table so just
6:27
zoom into it uh to make sure that we annotate everything nicely there it is table and here is
6:36
another text let's pick the class great I hope that you get the idea already and
6:43
once you annotate all of your images you will end up in this approve reject view
6:49
where you can for the last time take a look at all of your annotations make sure that they are right before adding
6:55
the images and the annotations to the data set I annotate all of those Imes myself so I will just click add all but
7:03
you can be more granular and add or reject specific images once you do it
7:09
you can add your approved images to the data set this will take you to this uh
7:14
view where you can select how many images would you like to put in your train validation and test subsets I will
7:21
just keep the default ratio of 7021 and add those
7:26
images and the last step is to create a version uh when you create a version you
7:32
can apply processing steps and augmentations and for this specific data
7:38
set I will skip the processing steps I will even remove the resize only keep
7:43
the auto Orient and I will not apply an augmentations as I work with documents
7:48
so I don't want to flip them rotate them or I don't know uh Shear them so I skip
7:55
all of those but for your specific use case uh those might be useful click continue and create and depending
8:03
on the amount of images and their size that creation step may take a little bit
8:08
of time but once it's completed you finally have the data set that you can use for training and to do that you will
8:15
Cate uh you will click the download data set button pick the export format that
8:21
you would like to use in our case that will be yolow V1 and download but I will cover that step during the training
8:28
process we have the data now it's finally time to train the model I will be using Linux
8:35
machine with Nvidia GPU and I will perform most of the configuration and
8:40
training using terminal interface if you don't know Linux commands don't be
8:45
scared I'll try to explain everything along the way and spell out what's happening if you're using Windows make
8:53
sure to use WSL uh windows subsystem for Linux it just provides a more consistent
9:00
and compatible environments for running all the tools and
Setting Up Your Local YOLOv11 Training Environment
9:05
scripts so here we are in Linux terminal and the first thing that we are going to
9:11
do is to create a separate directory uh for our tutorial so I navigate to
9:17
documents using CD command change directory and once I'm inside I create
9:25
new directory called uh YOLO 11 tutorial and once again I go into that directory
9:32
inside I create new VM and for those of you who don't know VMS allow you to uh
9:40
work on different python projects um in parallel you can have different uh
9:46
python version in each VM you can have different versions of your
9:51
dependencies in each VM and those versions uh don't clash with each other
9:58
and uh once I'm inside uh I activate uh that VM now every python
10:07
related command that I am going to run will be executed in context of that
10:12
specific vmf our python enironment is ready now it's time to fill it with all
10:18
the necessary dependencies to train YOLO 11 model you will need ultral lithics package and one of its core dependencies
10:25
is pych if you don't know pych is probably the biggest machine learning framework right now used to research and
10:32
deploy neural networks like YOLO 11 for example and to run efficiently it's
10:38
actually very close to your Hardware that means that you have different installations for CPU and for NVIDIA GPU
10:45
and for amdgpu and to make sure that everything is configured properly ultral ltic
10:51
documentation actually recommends you install pytorch separately on your own
10:57
and let me show you how to do it the easiest thing you can do is to go to
11:04
p.org website and scroll down a little bit they have this really cool uh
11:10
installation command generator where you can choose uh your operating system and
11:15
the hardware that you are using and the command changes dynamically depending on
11:21
your configuration I'm just picking uh Cuda
11:27
11.8 and I will tell you why in just a second I copy the command and we go back
11:33
to the terminal so as you hopefully noticed the structure of the
11:40
installation command uh was depending on the Cuda version that I have installed
11:46
uh so um let me show you how to check the Cuda version you have there are multiple ways to do that I will just uh
11:52
use Nvidia SMI or alternatively uh nvcc version and my
11:58
version of Cuda is 11.6 this is pretty old uh Cuda version
12:05
it's actually uh so old that in that uh command generator that was on pytor org
12:12
uh website there was no command for um Cuda
12:18
11.6 but the latest version that was there was 11.8 fairly close so what I
12:24
did is I copied the version for 11.8 and now we will will do small edits
12:31
to make make sure that it will work for me so I paste it in the terminal and I
12:37
change the end number uh in the URL
12:45
from8 to6 but probably already guessed it's means 11.8 and
12:53
11.6 uh and I will do few more edits so I will remove torch audio because we are
12:58
not working with audio today and I will remove the free at the end of Peep free
13:05
install uh because I can just use peep within uh my python uh virtual
13:11
environment now I click run and uh yeah depending on your
13:18
internet connection depending on the on the individ individual GPU that you have
13:24
execution of that particular command might take a little bit of time
13:30
so uh let's use the magic of Cinema and and skip the
13:36
installation once the installation is completed uh we can use a peep list to
13:42
list all the dependencies that are installed in our python environment uh and we can see that torch
13:51
and torch Vision are both installed with support for Cuda 11.6 exactly what we
13:58
need and now we can uh just install uh ultral
14:04
litics peep package so we type peep install ultral litics uh press
14:13
enter and here just before depending on
14:18
your Hardware depending on your internet connection the installation might take a little bit of time but significantly
14:25
less than than pytorch probably the largest dependenc see over here is open
14:32
CV uh but yeah all around the installation of ultral ltic once you
14:38
have a torch already shouldn't take like more than 30 seconds we see that it's already wrapping up and yeah once it's
14:47
done um it's good idea to run uh YOLO help just to confirm that the ultral
14:55
litic CLI works as expected looks like our python environment is
15:01
ready now it's time to download the data set as I said before I'll be using this
15:06
TFT data set that hopefully will allow me to train model to detect text figures
15:12
and tables in research papers and to download it I navigate to a data set
15:20
section click download data set button pick the export format that I want to
15:26
use in our case uh yellow 11 and for the local training uh I'll be using uh the
15:32
zip file so I uh check this uh radio button and click download and depending
15:39
on the size of your data set that might take a few minutes to
Understanding YOLO Annotation Formats
15:45
complete once the download is completed I'm back into the terminal and the first thing that I will do is confirm that our
15:52
ZIP file with our data set is in downloads directory and to do it I will
15:59
use LS command with two options l and a so I will use a option to print all the
16:06
files and I will use l option to do it in the long description format and then
16:13
I will look for files that have TFT string in their name remember our data
16:18
set is called TFT ID and that's uh the grab command and the pipe in between
16:25
those commands is used to stream the output from the First Command into the
16:30
second command so that I can chain them and indeed there is a TFT ID zip file in
16:38
our downloads directory so the next thing that I will do is I will create a
16:44
data sets directory inside my current directory so inside YOLO 11 tutorials
16:50
and I will go into that directory and then I will copy our uh TFT ID data set
16:58
zip into my current directory I can confirm that the copy
17:04
operation was successful by just listing files in my current directory uh indeed
17:10
there is our ZIP file over there now I will use unzip command uh to extract
17:16
files uh from that zip directory uh and I will use once again
17:21
two additional options the first one is Dash Q to make it quiet because by default unzip list all the files that
17:28
are extracted and we have thousands of files and another one is dasd to specify
17:33
the target directory uh and now uh I will try to
17:41
take a look inside the target directory to uh understand the structure of uh my
17:47
data set and to do it I will use the tree command um tree command also has
17:53
plenty of options but first I will use dashd stands for directory so uh This
17:59
Way tree will only display directories inside directories um and on once we execute
18:07
that command we can see that we have three subdirectories train test and valid those of course stand for the data
18:16
set splits and each of subd directories have two more directories in them so we have
18:23
images and labels inside each subdirectory and uh this is the this is
18:29
the structure of the data that you need to maintain for ultral litics to understand how to load your data set
18:35
during the training and to be honest it's very similar to the structure of data that we know from the previous
18:42
versions of Yola model now the next step is to go beyond
18:48
directories and to understand the file structure so another option uh that we
18:53
can use in uh tree command is uh Dash
18:59
capital L that stands for layer of the tree so we can decide how deep we want
19:05
to go and and this time I'm am using uh-h
19:11
L3 so that I won't go inside images and labels
19:17
subdirectories because like I said before we have thousands of of files in those subd directories so those would
19:24
just flat our uh terminal and we wouldn't be able to see anything
19:29
so uh we see free files inside TFT ID
19:35
data set there are two RMS that I don't really care about and there is one data
19:40
yam file and that file is certainly interesting to us and I want to show it to you but before I'll do it let me just
19:48
switch that uh dl3 into- L4 to show you
19:55
the amount of data that we have in those directories so we see that we have uh
20:01
over 17K files uh inside data set which is
20:06
interesting yeah because we only had less than four less than 9,000 images
20:12
but somehow we have uh 177,000 um files inside the data set and
20:18
I will I will explain why is that in in just a second but first let's take a look at the at the data yaml file uh
20:27
that I told you about uh a second ago so to take a look inside files we can use
20:33
cat command cat pretty much prints the content of the file into the terminal so
20:40
I'm I'm using Cad and specifying the path of the data yam uh file and uh we
20:46
can see that's pretty uh typical uh yaml file inside um not very long um so let's
20:55
take a look what do we have here so first of all we have a train Val and test keys in that uh yaml file and those
21:03
um specify relative paths to the train
21:08
test and valid subd directories or actually to directories storing images
21:15
inside those sub directories to be precise um that I showed you just a
21:21
second ago next we have NC that stands for number of classes I believe and in
21:27
our data set we have only fre classes and the next one is uh names and this is
21:34
pretty much the list of the classes uh that we have in the data sets kind of redundant information you don't need the
21:40
N see if you have the names but whatever um okay so now let's take a look at
21:47
those files why do we have 17K files uh if we only have less than
21:53
9,000 images so let me first least all
21:58
all the images inside the train sub directory or
22:04
uh even better let's just uh print the first 20 images for that we can use the
22:10
head command and we can just specify the number of U entries that we would like to show um and because uh things are
22:20
ordered alphabetically when you use LS um this order is constant so we can just
22:26
print the first 20 line and this is always be the same 20 lines um and once
22:33
again I'm using the pipe to stream the output of the First Command into the second
22:39
command um you know what let's uh drop the LA uh to make uh the representation
22:47
a bit more concise concise I'm only interested in the in the names awesome
22:53
and now we can open new terminal window I will just move move it uh over
22:59
here and here we will display the content of the labels directory from the
23:06
train split uh so just before I will only show you the first 20 files uh from
23:13
that directory and uh if we will take a deeper look we'll notice that the names
23:20
of the files in the images directory and the labels directory are the same the
23:26
only difference is the file extension so in the images we have jpex and labels we
23:32
have txt files and you can see that the first file have the same name if we will
23:37
take a deeper look the second and the third um have the same name as well and the last file also have also have the
23:45
same name so now let's copy one of those names let's go back to our original
23:52
terminal and use cut once again this time not to display the data but to
23:57
display the content of that labels files and once we do it we see a bunch
24:04
of numbers what do they mean let me show you okay so I went ahead and I copied
24:12
the first three lines from our annotation file if you would take a look at the original file it had like 10
24:19
lines in it but three lines is just enough for me to explain how yellow annotation format works and I also
24:26
rounded all floating Point numbers to second digit after the comma for those
24:31
of you who are unfamiliar with u software engineering jargon floating
24:36
Point numbers are just the numbers where we have some digits after the comma and two digits is once again just enough for
24:43
me to explain how the whole annotation format works so in yellow annotation
24:49
every line describe a single bounding box along with a class associated with
24:55
the bounding box so we have four floating points numbers that describe the position and the size of the
25:03
bounding box and we have one integer that describe the class ID of that
25:09
particular bounding box now those numbers in the txt file are separated by
25:15
a single space and they form kind of like this table so the First Column from the left
25:25
is actually the class ID now what is class ID that's a convention that we use
25:30
in computer vision so for example in our data set we have three classes figure table and text those are the class names
25:39
but in practice turns out it's often a lot more convenient to use number
25:44
representation not that text representation that's why we map those class names into class IDs those are
25:53
integers starting with zero up to n minus one when n is the amount of
26:00
classes that we have in our data set so in our case Zero maps to figure one maps
26:06
to table two maps to text so anytime that we will see a line in our
26:13
annotation txt file that starts with digit two that means that the whole line
26:21
describe a bounding box associated with text class now we have four more columns uh
26:30
starting from left to right those are X Center y center with and height of the
26:38
bounding box so it's a little bit weird that we use X Center and Y Center to
26:43
define the position of the bounding box but this convention is uh is coming back
26:49
probably to the first YOLO uh model ever uh and it just uh stayed the same uh
26:57
through the the years now important thing is that those numbers are normalized so X Center y center with and
27:06
height of those bounding boxes that were initially expressed in pixels got
27:12
divided by respective dimensions of the image uh so that in the end uh the
27:19
values are between zero and one now that we have our python
Training YOLOv11 Locally
27:24
environment ready and we understand the structure of our data said it's time to start the training you can do it in two
27:32
ways using python SDK or the CLI but for the purpose of this tutorial I'll be
27:37
using the CLI option but before I trigger the actual training let's run
27:43
YOLO help command to understand the structure of the command we need to start the training so we see that every
27:51
command and ultral litic CLI have this YOLO task mode and then additional
27:57
argument structure and if we go to ultral litics documentation we can see that values for
28:04
the task may be uh detect segment classify pose and obb this stands for
28:13
oriented bounding boxes in our case we are going to use detection and the mode
28:19
can be train validate predict export track and Benchmark in our case this is
28:26
going to be train if we scroll a little bit lower we can see an example of uh
28:32
detection training command with some of those additional arguments specify the
28:38
most important of those are data this is the path leading to data yam file the
28:44
file that we saw just a few minutes ago and model this is the name of the
28:50
checkpoint that we are going to use to start the training there are also some additional hyperparameters that I will
28:56
discuss in just a second but before we do that let's talk a little bit more about the checkpoint
29:03
options that we have so every task have five checkpoints that you can use
29:11
ranging from Nano to X and those checkpoints uh differ in sizes so the
29:19
amount of parameters the model have but also the expected map value in that
29:25
table we can see values on Co data set so YOLO
29:31
11n have significantly lower expected map value than Yol 11x and that's
29:38
something that you should keep in mind when you will uh choose the checkpoint for your training I'm using YOLO
29:46
11n but I'm only using that because I want to have small model so that it
29:52
trains fast during the recording of the tutorial in your case you might actually
29:57
aim aim for something bigger than the Nano
30:03
version so now we are back in terminal and we can finally start the training so
30:08
what I do is I type in Yolo detect train command and I provide additional
30:15
arguments like we saw the first one should be data and here I provide the
30:21
path leading to data yam that is inside the root directory of my data set the
30:29
next one is model and here like I said I will use YOLO 11 Nano version I also
30:36
specify two more hyper parameters EPO and set it to five an image size and set
30:42
it to 640 I will tell you more about those hyper parameters in just a second I
30:49
trigger the training and it fails and when we examine the error we can see
30:54
that ultral litic c complains about in correct path leading to validation
31:00
images and this is the error that a lot of people experience so let me show you
31:05
how to solve it the first thing I do is I navigate to the root directory of my
31:11
data set so go to data sets TFT ID and then I run PWD command this is the Linux
31:19
command that shows you the global path of the directory that you're in and I
31:26
just copy the result of that command and we will edit the data I will do it using
31:33
Nano this is the command line editor but you can use vs code pretty much any text editor that you have and what I do is I
31:42
update the paths to train test and validation subset and I remove those two
31:49
dots and I paste the result of PWD command essentially what I do is I
31:54
change the relative paths to Global Puffs once I finish I close I save the
32:02
file I can confirm that the file was updated by just using cut data yam
32:07
command that we already know we can see that paths to train test and valid subsets are now global and now what we
32:15
can do is we can go back and restart the training so I just run the same command
32:22
hit enter and the training starts uh we see a lot of high hyper
32:28
parameters um that either we defined or are set to default values you can see uh
32:35
the summary of the uh model architecture and we see that the training runs now
32:42
what can we do to monitor our training so let me go back to the second terminal window that we had open and the most OB
32:49
obvious thing is to run Nvidia SMI here we see the GPU usage but this is only
32:56
the snapshot of of the uh GPU usage that was registered at this specific moment
33:02
in time and if you would like that uh Nvidia semi command to be updated
33:08
constantly what we can do is we can you use watch uh this is the command that
33:13
reruns the command all the time and we set the update period to 10th of a
33:19
second this way we have this kind of like real time um training monitoring
33:24
showing us the uh usage of our GPU by the way this Nvidia SMI plus watch
33:32
combo gives you very basic information about your training job so if you would
33:38
like to know more there are other tools that you can use two names that come to
33:43
my mind are Envy top and GPU stat so you can visit their GitHub Pages learn how
33:51
to install them and this way you will know a lot more about your training job
33:56
but also about the behavior of your model and the GPU memory allocation during the
YOLOv11Training Hyperparameters
34:03
deployment the model is training so let me use this time to tell you a little bit more about those additional
34:10
arguments that you can pass to training command we only set four of them model
34:16
data EPO and image size you already know what model and data is so let me tell
34:23
you a little bit more about the rest of them starting with EPO a single Epoch of
34:29
training is model going through every image in your training set so when we
34:35
set our EPO to five that means that our model will see every image from our
34:41
training set five times the more EPO you set the longer the training but
34:48
potentially the higher accuracy of the model you train image resolution is the
34:54
resolution of the image used during the training but also during the deployment of the model so before the image go
35:02
through the model it gets resized to some predefined
35:07
resolution uh this is because in your training set there is a high chance that your uh images have different resolution
35:15
so you resize all of them to a common resolution that you will use during the
35:20
training now if you set the image size to a higher value you will have a higher
35:26
chance to detect small objects but the higher the image resolution the higher
35:31
the memory allocation your model will need to train uh another argument uh
35:37
that have similar outcome is the batch size so when I said that the epoch is a
35:44
model going through every image in your training set the model does not look at
35:49
every image separately it looks at multiple images all at once and the
35:55
batch size regulate uh the amount of memor the images that model will see at
36:01
once so when we have thousand images in our training set and our Epoch is set to
36:07
10 for example the model will need 100 iterations to go through each image in
36:13
the training set but when the batch size is set for example to 50 it will only
36:19
need 20 iterations to get through all of those images in the end increasing the
36:25
speed of the training but also like I in said increasing the memory allocation
36:30
another interesting uh parameter that we see on that list is device uh we can for
36:36
example use this parameter to use multiple uh gpus for training of a
36:41
single model I only have single GPU so I didn't use it but if you have multiple
36:47
gpus you can speed up the training uh this way uh and people who have Macs for
36:52
example can uh use a device argument to specify that they would like to use MPS
36:58
so the Apple silicon uh rather than the regular CPU for the training or
37:04
deployment and probably the last one on that list that I think it's uh really interesting is the
37:10
patience so patience the parameter that you can use to uh stop the training um
37:17
so the model is actually constantly evaluated during the training and we can
37:22
use the patients to stop the training if we see no progress during the training so if for for example for 10 Epoch the
37:30
accuracy of the model did not improve there is a high chance that it won't improve in the next 50 Epoch so we might
37:36
just as well decide to stop the training right now and save some time and uh save
37:42
the money that we would use for the training uh as you can see there are a
37:47
lot of other parameters that you can uh choose during the during the training so
37:54
I highly encourage you to go through that list look for something sounds interesting uh try different values um
38:02
and experiment okay we are back in terminal and we can see that our training is
Evaluating Your YOLOv11 Model's Performance
38:09
almost done uh we are just performing the final evaluation of the model and
38:14
it's completed and we can see that results of the training have been saved to runs detect train to directory so
38:24
let's run ls- la on our current directory and we see two things happened
38:32
first of all uh YOLO 11 n. PT file was saved this is the
38:39
checkpoint that we used to initiate the training and it was just cached locally
38:44
so that if we would rerun our training job we wouldn't need to download it once again but also there is a new directory
38:53
created called runs and this is the directory that is created by ultral litics CLI during the
39:00
training and if we will run 3-D on this directory we'll see that
39:07
inside we have detect subdirectory and inside the detect directory see another
39:14
train and train two directories so why do we have two train directories it's
39:21
because our first training job actually failed remember we did run YOLO uh
39:27
detect train command but it failed because it couldn't locate all images
39:33
from validation set but because it did run uh the train directory was created
39:39
and then when we rerun uh the command to train the model once again another train
39:45
tool directory was created so if we would I don't know um update the
39:51
hyperparameter and run the train job once again another train free d Dory
39:57
would be created and we have this detect directory because we did run detection task if we would run another training
40:05
job but for segmentation we would have detect and segment subd directories in the runs
40:13
directory now let me open runs detect train to directory so we can examine the
40:21
artifacts that were saved there during and after the training so the first
40:27
thing that we see is the weights directory the first one from the left
40:32
and inside uh there are last PT and best
40:38
PT files those are the weights that were saved uh during an after the training
40:45
but uh we'll play with them in just a second for now let's focus on uh other
40:51
assets that are in this directory and the first thing that I want to take a look is the confusion metrix so this is
40:59
the chart that helps us to understand how many false positives and false
41:05
negatives happened during the model evaluation after the training but also
41:12
whether the model confuses classes so whether the model detects the object but
41:19
fails to assign the correct class to it and from this chart we see that the
41:25
model has pretty much only one weakness and this is uh false predictions of text
41:33
class that means that the object detect text where there shouldn't be any text
41:40
and that's uh probably related to short training time so if we would have maybe
41:46
10 Epoch instead of five we would be able to optimize uh the model in this
41:53
specific Edge case another interesting artifact is
42:01
results.png so uh like I told you during the training the model is constantly
42:06
being evaluated and uh we use multiple different metrics to do that and uh on
42:14
this chart we see values of all of those metrics after every Epoch and we can see
42:22
that at Epoch number five uh box loss for rain was still higher than box loss
42:32
for validation that means that our model is most likely undertrained and we could
42:39
easily train it for 10 20 EPO longer and
42:44
it would still be uh not overfitting uh and the last uh thing
42:50
that I would like to take a look is uh Val batch
42:55
results so uh those are uh bounding boxes representing detections that
43:01
happen during the evaluation of the model at the end of the training on the validation set and here I mostly look
43:09
for things that are clearly wrong something that uh I can visibly see and
43:16
maybe do some adjustments in my training set to improve on those specific edge
43:22
cases but here on this chart I don't see anything clearly out of
43:29
ordinary we finished the training we evaluated the model now it's time to put it through the test since we trained a
Running Inference with Your Trained YOLOv11 Model
43:36
model capable of analyzing research articles let's use it to process some ml
43:42
papers it would be awesome to use it to process a paper talking about YOLO 11
43:48
but traditionally ultral lithics did not released a paper along with the model so
43:53
I got creative and picked another iconic paper for us to use let's check how our
44:00
model handles detecting text tables and images in attention is all you need
44:07
paper okay so I did found attention is all you need paper on the
44:12
internet uh it's in PDF format so let's download
44:17
it uh to our downloads directory and once we do it let's go back to the
44:24
terminal and first thing first let's confirm that it is uh really in the
44:30
downloads directory so let's do ls- la on downloads and this time grab for PDF
44:36
files and yep there is only one PDF over there that must be our attention is all
44:42
you need paper so now let's copy uh the
44:48
paper from downloads into the data sets directory that we created at the beginning of the tutorial and inside the
44:57
data sets let's create test directory what I will do now is I will convert
45:02
that PDF into range of PNG images and I will save them into this test directory
45:10
and to do it I will use a little bit of image magic so image magic is a uh
45:18
command line tool is a package that helps you to do crazy things with images and it's not really related to the topic
45:26
of this tutorial but the command that you see right now on the screen will help us to convert that PDF file into uh
45:36
a list of PNG images and we'll save that to our test directory we'll do a lot of
45:44
things along the way like removing the alpha Channel and setting the background color uh to
45:51
White and uh once that will happen uh we will uh run the inference
45:58
on those images if you are interested in image magic I highly encourage you to
46:03
visit their GitHub page um the docks are crazy uh so it's not easy uh to learn
46:11
that tool but it's very powerful now let's uh take a look at the images that
46:18
we generated so let's open that directory in the UI and we see that we have separate PNG file for every page
46:26
and can see this iconic U Transformers diagram um on the page three okay so now
46:35
we can go back to terminal and run uh
46:40
detection uh through the CLI so once again we do YOLO detect but this time we
46:46
set the mode to predict we specify few arguments the
46:52
first one is source and here we pass the path to our tests test directory this
46:59
way we uh say that we want to run the inference on every asset that is in that
47:04
directory so on every image and we also specify the model that we would like to
47:09
use in our case we will use the best model we got from our training job so we
47:15
do runs detect train to weights best PT and then when we run it we see that the
47:21
CLI shows us uh what classes and how many instances of every class
47:27
uh were detected on every image and we also get an information that the results
47:34
were save in runs detect predict directory now we can just copy that path
47:42
and open it in the UI to take a look we can open the first image from
47:49
the left this is the title page it it's a little bit unfortunate that ultral litics uh decided to use why white or
47:57
almost white color to represent this class but we can still see that sections
48:03
of the text were correctly detected take a look on another page and yeah we we
48:10
managed to to detect this iconic Transformers diagram on page
48:16
three looks like it works now let's switch the environment
YOLOv11 Training in Google Colab
48:22
and let me show you how to run the training and evaluate the model in Google
48:27
collab as usual The Notebook for this tutorial can be found in roof flow
48:33
notebooks repository where we store demo notebooks for all our YouTube tutorials
48:40
the link to the Google collab for this notebook will be also included in the description below the video here we just
48:48
need to scroll a little bit lower to uh model tutorials table and look for YOLO
48:55
11 object detection then click open and collab button once
49:01
we do it we'll get redirected to Google collab and before we start the training
49:07
we need to take care of few setup steps the first one is to add roof flow API
49:16
key to the secret step so uh to get roof
49:21
flow API key you need to sign in into your roof flow profile or of course
49:27
create one if you don't have it yet and then go to
49:32
settings API keys and here we copy the
49:38
private API key once we are back to Google colab we open the secret stab
49:46
that is in the left side panel of Google collab and we add new secret call it
49:55
roof flow APA key uh without spaces with
50:00
underscores um and all capital letters and enable the access to that um
50:07
specific secret um so that we will be able to use it later on now of course I
50:14
have my API key set already so I'm just closing uh this tab and the next step is
50:20
to confirm that we have access uh to the GPU and similarly to the Lo training to
50:27
do it I run Nvidia SMI uh and here what we can see is U the
50:34
version of the Cuda that is accessible in uh this
50:40
environment if you don't see similar output maybe with different Cuda version
50:46
or different GPU uh what you need to do is you need to open runtime then click
50:52
change runtime type and most likely uh you you are on the CPU version of uh
50:59
Google collab so you need to choose one of the accessible uh gpus I'm using
51:05
Nvidia L4 but I'm pretty sure it's not accessible on the free tier so if you
51:11
are on the free tier just uh choose uh T4 GPU click save and restart the
51:16
notebook if you set your secrets uh they should uh be still there so it's only to
51:23
get the access to the uh to the GPU the next step is to install uh the required
51:31
dependencies so on a local before we installed ultral litics we need to
51:36
install py and we needed to make sure that the py version is right for our
51:41
Cuda over here in collab uh the pie
51:46
torch is already installed so we don't need to install it what we can do is run PE least and grab uh to look for pytorch
51:55
and we see that that both torch and torch Vision are installed and in both
52:02
cases they're installed with the Cuda support so that's perfect all we need to
52:08
do is run uh install ultral litics and install roboplow uh because here we will
52:15
use ultral litics uh to run the training but we will use roof flow to pull the
52:21
data set from Robo roof flow universe so far we only downloaded the
52:27
data set as a zip file let me show you how to do it using roof flow SDK so just
52:34
like before we select data set in the left side panel but this time when we
52:41
click download data set button and select the uh appropriate export format
52:48
we will select show download code instead of download zip to computer and
52:54
once we do it and click continue the code snippet will get
52:59
generated uh I'll just copy the last three lines of that code snippet but if
53:07
you want the code snippet that will install roof flow SDK give you your API key and download the data set all at
53:14
once can just copy that paste it in the collab and execute I will just go back
53:20
to coll up and update those last three lines of that code snippet execute the
53:26
cell and downloading of the whole data set will take a little bit of time as a
53:32
matter of fact according to uh what we see on the screen will take around 3
53:38
minutes so let's use the magic of Cinema to speed up the whole
53:44
process okay our data set is downloaded so now we can open uh the file browser
53:51
in the left side panel and take a look at its structure uh we did that already
53:57
for local training so this time I will go very quickly and only talk about the
54:03
most important things so the data set have three subdirectories uh test train
54:11
and valid and a data yam file uh that um
54:16
provides the most important metadata about the data set at the very top of the uh yam file we have uh three keys
54:25
test train and valid those um store uh
54:30
relative paths to images directories that are located within each
54:36
subdirectory so now if we open those subdirectories we see that each of them
54:42
have images and labels uh directories inside and if we open one of those
54:50
labels directories and click on one of those txt files we see a typical uh YOLO
54:59
annotation format we spoke a lot about that format uh in the section covering
55:06
local training so if you're interested feel free to use the chapters you will
55:11
learn a lot more our environment is configured our data set is downloaded so
55:18
we can start the training but before we do that let's open the resources tab
55:24
that is on the right right side of Google collab we'll use it to keep track
55:31
of the GPU vram usage uh during the training and by the way over here we see
55:38
that the total amount of vam that we have at our disposal is
55:45
22.5 uh gabt so it's actually a fairly large uh GPU and like I said if you are
55:52
on a free tier you wouldn't have access to it but I use it uh to uh experiment a
55:59
little bit so contrary to what I did on a local here I will uh train the same
56:06
model but uh twice as long when it comes to EPO but probably faster when it comes
56:12
to time as I will set uh EPO to 10 but batch size to
56:18
128 and I can do it because it's a fairly large GPU now I just hit enter and start
56:26
training just like on local we see the list of all parameters uh along with values the
56:35
definition of the neural network and when we scroll lower we see that our
56:40
train and validation sets are being uh loaded uh and the training should start
56:48
in just a matter of few seconds and once the training starts we'll see a drastic
56:55
Spike on the GPU vram uh usage chart so
57:00
the first Epoch have just started and the spike should happened right now
57:07
exactly so we see that uh immediately we allocated
57:13
17.4 uh gabes of that uh vram and the single Epoch takes around 1
57:22
minute to complete and we uh will run 10 of them so let's use the magic of Cinema once
57:30
again uh to speed up the process uh the training have uh
57:36
completed and we see that the artifacts were saved and runs detect train
57:42
directory so now let's use Linux LS command to list all the files that we
57:48
can find there and uh display few charts that will tell us a little bit more
57:55
about the quality of the model that we trained so the first chart that I'm
58:00
interested in is the confusion metrix so if you uh saw our local
58:06
training um we had a small problem with the model that we trained locally it had
58:13
pretty much one weakness and those were false uh positive for text class and
58:21
back then after five epochs of training we had over five 00 maybe even 600 of
58:28
those false positive on the validation set and here we see that we've just uh
58:35
changing the EPO count from 5 to 10 we managed to lower that value below 400 so
58:43
the model is significantly better at not producing those false detections the
58:49
next chart um that I'm interested in is the results PNG uh one where we have um the values
58:58
of all the important metrics that were collected during the training here the
59:04
most important metrics is box loss both for train and validation set and we can
59:11
see that the Box loss for train is still higher than the Box loss for validation
59:17
so we can see that the model is still not overfeeding you could still probably
59:23
uh train it for a little bit longer and it will probably produce even even better results and the final thing that
59:30
I always like to take a look at is a uh subset of predictions from the
59:37
validation uh batch uh just as a sanity check to look for anything that is
59:43
clearly out of ordinary but uh similarly to the local training I don't see any
59:49
problems with this model and as a final step we'll run our
59:55
c in uh validation mode so what will happen is we'll load uh our freshly
1:00:02
trained model into the memory and we'll run it against images from the validation set to compute the map
1:00:10
95 metric for each class and for all classes collectively this is the most
1:00:17
popular metric uh to Benchmark object detection models and we can see that in
1:00:23
all cases we crossed 90% this means that at least on images from the validation
1:00:28
set the model is uh handling that very well and it's capable of detecting um
1:00:36
images uh text and tables with high
Saving Your Fine-Tuned YOLOv11 Model Weights
1:00:41
accuracy we know that our model works well so now it's time to save it this is
1:00:47
especially important if you run your training in Google collab because Google collab environment is only active for
1:00:54
several hours and once that time pass all files that were accessible in that
1:01:00
environment will be deleted including your freshly trained model so of course
1:01:06
you can download that file to your local that works especially if you have only
1:01:12
one project that you plan to work on but if you have multiple projects I
1:01:18
guarantee in three months time you will not remember what have you done and in what project so I actually recommend you
1:01:25
upload load the weights to roof flow and let me show you how to do
1:01:31
it so I'm not sure if you remember but we actually trained the model on the data set I found on roof flow universe
1:01:39
so before we'll be able to upload our weights we need to Fork that data set so
1:01:45
that it will be assigned to my account and then I will be able to tie my new model to that data set to do it I go
1:01:53
into images and click for data set beta button and once uh that happens the
1:02:01
forking process is triggered depending on the size of a data set it may take
1:02:06
more or less time but my data set is fairly large um so it will take a little
1:02:12
bit of time but once that process incompleted we'll end up in this view so
1:02:18
this is now my data set I can for example change the name of the classes here I can upload more data I can
1:02:26
annotate more data so that gives me a lot of options especially if I would like to improve uh the model that I just
1:02:34
trained but importantly it allows me to upload my newly trained model into this
1:02:41
project so let's do it at the bottom of uh the Google collab you will find a
1:02:47
code snippet that shows you how to upload the weights to roof flow universe
1:02:53
but that code snippet assumes that you are the owner of the project that you use for training and in our case that is
1:03:00
not the case I use the project from roof flow universe and I just created the
1:03:05
fork and I would like to tie the model to that fork so to do it I will just paste this code here uh this way we will
1:03:14
use rlow SDK to load the metadata of our fork and now we'll just uh make a small
1:03:22
updates in the code snippet uh switch the version to our project and uh run
1:03:29
the cell now the weights are being uploaded into my Fork I can click uh
1:03:36
this uh link that is displayed and this will take me to the page where the model
1:03:41
will be loaded depending on the size of the model uh you uploaded that might
1:03:46
take a few minutes to complete but once it's completed this view change into
1:03:52
something like this and now we have a lot more options first of all we have all the metadata of our training but we
1:03:58
can deploy the model in all uh different ways let me show you how it works I will
1:04:04
just uh take one page from attention is all un need paper and we uh drop it here
1:04:11
and immediately the model will get loaded into the memory uh of the of the
1:04:16
browser because it will run locally I'll run inference on that file and we can
1:04:23
see that just as in the case of local the model is capable of detecting both
1:04:28
the figure and the text on that page awesome don't get me wrong running your
Deploying Your YOLOv11 Model
1:04:36
model locally in browser is cool but uploading your rats to rlow allow you to
1:04:42
do so much more depending on the use case that you're trying to solve whether or not you train the model to detect
1:04:48
animals or parts that are manufactured in the factory you can for example
1:04:53
deploy that model on small Ed device like this Jetson Nano and run your
1:04:59
inference directly in the forest or in the factory to lower the inference cost
1:05:05
and the latency you can for example deploy it on phones or in the cloud or
1:05:10
you can deploy your model in collab and that's exactly what I will show you right
1:05:17
now and we are back in Google collab I know long time no see uh but this is
1:05:24
actually a new Google collab separate environment I created this so you can be sure that the weights are no longer here
1:05:31
the weights are actually in roof flow universe and we will pull them to do that we start by installing two python
1:05:40
packages that we are going to use the first one is inference that we are going to use to pull the model and run it
1:05:46
locally the second one is supervision that we are going to use to visualize
1:05:53
the result of our inference one once once the dependencies are installed we
1:05:58
need to make sure that we will have access to roof flow API key this is the same key we used before to pull the data
1:06:06
set from roof flow universe or to upload our weights and now we will use it to pull our model back into fresh Google
1:06:14
collab environment uh I will copy that code uh and close the tab paste it here
1:06:23
and now we'll use Google collab SDK to extract the value of roboff flow API key
1:06:29
from secrets into the constant so to do it we'll call user data get and pass the
1:06:37
name of secret we would like to extract in our case it's roof flow API key all
1:06:43
capital with underscores instead of spaces now it's time to pull the actual
1:06:49
model to do it we will use a get model function that we need to import from
1:06:55
inference package and uh to pull the model we need the API key but we also need model ID
1:07:03
and we will get that model ID from roof flow UI from the page that was created
1:07:09
when we uploaded the model okay so we just copy this string
1:07:18
and go back to Google collab paste it as model ID and now we
1:07:24
are ready to load the model so I just create new variable call it the model and call get model function pass model
1:07:32
ID and roof flow API key hit enter and
1:07:39
after a few seconds the model uh should be loaded and now what we can do is we can
1:07:45
test the model uh on some example image and I'm just lazy so at this point I
1:07:51
will just upload the F page from attention is all you need paper so that
1:07:57
it will be accessible in uh Google collab environment and we'll just use
1:08:02
open CV to load that page as uh the number array so let's do it so first
1:08:10
thing first I import open CV so we type import CV2 and now I will Define uh a
1:08:19
new constant um image path copy the path to our uploaded image uh past past it
1:08:26
here and uh create a new variable called image uh run IM read function from open
1:08:33
CV pass our image path hit shift enter and now the image is loaded into the
1:08:42
memory the next step is to run the inference so I create the results a
1:08:47
variable uh run model infer function pass the image along with the confidence
1:08:55
level let set it to 0.4 and that uh would uh return a list
1:09:01
uh by default so I just pick the first value from that list here is the results
1:09:08
object um that we got so now we will use
1:09:13
uh the supervision to parse that result and display uh the bounding boxes on the
1:09:19
image so we import supervision as SV and
1:09:26
we SV and now uh we will create a
1:09:31
detection object from. uh inference result so we create variable called
1:09:37
detections uh call sv. detections from
1:09:43
inference from inference and now we will pass our
1:09:51
results object uh into uh that function and as a result we'll get the detections
1:09:58
object so uh the results object and the detections object store the same
1:10:03
information it's just detections is the object that we can use to um pass to all
1:10:09
sorts of different utilities that we have in supervision uh next I always like to
1:10:14
create a copy of the original image before I annotate anything on that image case I will mess something up and first
1:10:22
thing first I will run a box annotator to draw bounding boxes around the
1:10:27
detected uh objects you just pass the annotated image and the detections and
1:10:33
we can copy all this paste it below and now change the box annotator into label
1:10:40
annotator run that cell and uh right now the annotated
1:10:46
image should have both the boxes and the labels so we can display that image in
1:10:52
the Google collab and yeah there you have it we have all
1:11:00
the detected objects just like in the UI just like on local model
Conclusion
1:11:08
works and that's it I really hope that you enjoyed the video it turned out to
1:11:13
be quite a long one but we covered a lot of useful things like how do you
1:11:18
annotate the data or what is the data format for Yola models and how to train the model in two different different
1:11:25
environments and of course how to deploy the model in the end I think that's a very solid foundation to anybody who
1:11:32
would like to use YOLO 11 in their project so if you enjoyed the video make
1:11:38
sure to like And subscribe if you have any questions leave them in the comments
1:11:43
my name is Peter and I see you next time bye
Transcript


Search in video
Intro
0:00
so in my previous videos I showed you how  to train an image classifier and an object   detector using yolo V8 now is the time for  semantic segmentation I'm going to show you  
0:10
the entire process of how to train a semantic  segmentation algorithm using yolo V8 from  
0:15
how to annotate the data how to train  the model in your local environment and   also from a google colab and finally a super super  comprehensive guide on how to validate the model  
0:25
you trained my name is Felipe welcome  to my channel and now let's get started  
Dataset
0:30
so let's start with tpday's tutorial and the first  thing I'm going to do is to show you the data   we are going to be using today so this is a  dataset I have prepared for today's tutorial and  
0:40
you can see that these are images of ducks we are  going to be using a duck dataset today and this is  
0:46
exactly how the images look like now for each one  of our images for absolutely every single one of  
0:52
our images we are going to have a binary mask we  are going to have an image a binary image where  
0:58
absolutely every single Pixel is either white or  black and absolutely every single white pixel it's  
1:04
the location of our objects all the white pixels  are the location of the objects we are interested  
1:10
in in this case the objects are our Ducks so let  me show you an example so it's a little more  
1:17
clear what I mean regarding the white pixels are  the location of our objects so this is a random  
1:22
image in my data set this a random image of a duck  and this exactly its binary mask so take a look  
1:29
what happens when I align the these two images  and when I apply something like a transparency  
1:35
you can see that the binary mask is giving us the  exact location of the duck in this image so this  
1:43
is exactly what it means that the white pixels are  the location of our objects so this is exactly the  
1:49
data I am going to using in this tutorial and now  let me show you from where I have downloaded this  
1:55
data set this a dataset I have found in the  open images dataset version 7. let me show you  
2:01
this dataset super super quickly this is an  amazing dataset that you can use for many different   computer vision related tasks for example if I  go to segmentation you can see that we have many  
2:12
many many different categories now we are looking  at a random category of phones this is for example  
2:20
a semantic segmentation data set of phones and  let me show you if I go here and I scroll down  
2:27
you can see that one of the categories is here  duck so this is for example the exact same data  
2:35
I am going to be using in this tutorial this is  the exact same duck dataset I am going to be  
2:40
using in order to train a semantic segmentation  algorithm using yolo V8 and obviously you could  
2:47
download the exact same data I am going to use in  this tutorial if you go to open images dataset  
2:52
version 7 you can just download the exact same  duck dataset I am going to be using today or you  
2:58
can also download another dataset of other categories  so this is about the data I am going to use in  
3:04
this project and this is about where you can  download the exact same data if you want to, now  
Data annotation
3:10
let me show you a website you can use in order  to annotate your data because in my case I have  
3:16
downloaded a dataset which is already annotated  so I don't have to annotate absolutely any of my  
3:22
images absolutely all of my images already have  its binary masks right I already have the masks  
3:28
for absolutely all the images in my data set but  but if you're building your data set from scratch  
3:33
chances are you will need to annotate your images  so let me give you this tool which is going to  
3:39
give you which is going to be super super useful  in case you need to annotate your data it's called  
3:44
cvat and you can find it in cvat.ai and this is  a very very popular computer vision annotation  
3:51
tool I have used it I don't know how many times  in my projects and it's very very popular and  
3:56
it's very useful so I'm going to show you how to  use this tool in order to annotate your images  
4:03
so the first thing we need to do is to go to  start using cvat this is going to ask you to  
4:08
either register if you don't have an user already  or to login right I already have an user so  
4:14
this is logged into my account and now let me show  you how I'm going to do in order to annotate a few  
4:20
images actually I'm going to annotate only one  image because I am only going to show you how to   use it in order to create a binary mask for your  project but I'm just going to do it with only one  
4:32
image because it's yeah you only need to see the  process and that's going to be all so I'm going  
4:37
to projects I'm going to here to the plus button  create a new project the name of this project will  
4:43
be duck semantic sem seg this will be the name  of my project and it will contain only one label  
4:53
which is duck so I'm going to press continue  and that's pretty much all submit and open
5:03
now I'm going to create a task this is already  yeah create new task the task name will be duck
5:15
task zero one it doesn't really matter the name so  I just I just selected a random name then I'm  
5:26
going to add an image I'm just going to select  this image I'm just going to annotate one image   so this is going to be enough and submit and open  so this is going to take a couple of seconds this  
5:37
is where you need to select all of your images  all the images you want to annotate but in my  
5:43
case I'm only going to select one so I'm going to  press here in Job so this is going to open The  
5:49
annotation job right now I'm going to show you how  you can annotate this image how you can create a  
5:55
binary mask for this image you need to go here to  draw new polygon then shape so I'm going to start  
6:02
over here and this is pretty much all we need to  do in order to create this semantic segmentation  
6:10
data set for this image right in order to create  the binary mask for this image you can see that  
6:17
I'm just trying to follow the Contour of this  object and you may notice that the Contour  
6:25
I am following is not perfect obviously this is  not perfect and it doesn't have to be perfect   if you're creating a dataset if you are creating  the mask of an image if you are creating the  
6:37
mask of an object then it definitely doesn't need  to be pixel wise perfect right you need to make a  
6:45
good mask obviously but something like this as  I am doing right now will be more than enough  
6:51
so this is a very time consuming process you  can see and this is why I have selected only  
6:57
one because if I do many many images it's going  to take me a lot of time and it doesn't make any  
7:03
sense because the idea is only for you to  see how to annotate the images right so you can  
7:10
see that I'm following the Contour okay and this  is an interesting part because we have reached the  
7:17
duck's hand or its leg or something like that  this part of the duck's body and you can see  
7:24
that this is beneath the water this is below the  water and this is where you're going to ask  
7:30
yourself do you need to annotate this part or not  do you need to annotate this part as if it's part  
7:36
of the duck or not because you could say yeah  it's definitely part of this duck but you are not  
7:43
really seeing a lot of this object right it's like  part of the water as well so this is where you're  
7:49
going to ask yourself if you need to annotate this  part or not and in my case I'm going to annotate  
7:55
it but it's like you can do either way in all of  those sections in all of those parts where you are  
8:01
not 100% convinced then that's like a discussion  you could do it you could not do it it's up to you  
8:10
so annotating a few images is always a good  practice because you are going to see many many   different situations as I have just seen over  here right where I have just seen with this part  
8:22
of the duck which now I am super super curious  what's the name if you know what's the name  
8:28
of this part of the duck's body please let  me know in the comments below I think it's  
8:34
called hand right because it's something like  a hand they have over there but let me know if  
8:39
it has another name and you if you know  it please let me know in the comments below  
8:44
now let's continue you can see I'm almost there  I have almost completed the mask of this duck  
8:53
now I only have to complete this  peak or whatever it's called  
9:00
it seems I don't really know much about ducks  Anatomy I don't really know what is the name of this  
9:06
part either so anyway I have already completed  and once I am completed I have to press shift N and  
9:12
that's going to be all so this is the mask this  is a binary mask I have generated for this object  
9:18
for this duck and this is going to be pretty  much all what I have to do now is to click save  
9:24
you can see that this is this is definitely not  a perfect mask this is not a perfect like pixel  
9:31
wise perfect mask because there are some parts  of this duck which are not within the mask but it  
9:37
doesn't matter make it as perfect as possible but  if it's not 100% perfect it's not the end of the  
9:43
world nothing happens so I have already saved this  image and what I need to do now is to download  
9:49
this data so I can show you how to download the  data you have just annotated in order to create  
9:55
your data set so this is what I'm going to do  I'm going to select this part this option over  
10:01
here and I'm going to export task data set and  then I'm going to select this option which is  
10:11
segmentation mask 1.1 I'm just going to  select that option and I'm going to click  
10:16
ok so that's going to be all we only  need to wait a couple of minutes and   that's pretty much all the data has been  downloaded now I'm going to open this file  
10:26
and basically the images you are interested in  are going to be here right you can see in my  
10:34
case I only have one image but this is where  you're going to have many many many images   and please mind the color you will get all these  images in right in my case I have a download this  
10:46
image in red it doesn't really matter just mind  that you could have something different than  
10:51
white but once you have all your images what  you need to do is to create a directory I'm  
10:56
going to show you how I do it I am going maybe  here and I'm going to create a very temporal  
11:04
directory which I'm going to call tmp and this  is where I'm going to locate this image right  
11:11
and I am going to I'm going to create two  directories one of them is going to be masks
11:19
and then the other one is going to be called  labels and you're going to see why in only a   minute and this is where I'm going to locate  the mask here and then I am going to pycharm  
11:32
because I have created a script a python script  which is going to take care of a very very very  
11:39
important process we have created masks which are  images which are binary images and that's perfect  
11:46
because that's exactly the information we need in  order to train a semantic segmentation algorithm  
11:51
but the way yolo V8 works we need to convert  this image this binary image into a different  
11:59
type of file we are going to keep exactly the  same information but we are going to convert  
12:04
this image into another type of file so let  me show you how this is a phyton file I have  
12:09
created in order to take care of this process  and the only thing you need to do is to edit  
12:15
these fields this is where you're going to put  all the masks this is a directory which is going  
12:20
to contain all the masks you have generated  and this is going to be the output directory   you can see that these two variables are already  named properly in my case because this is the tmp  
12:30
directory I have just created this is where  I have located the mask I have just generated   with cvat and this is my output directory  so take a look what happens when I press play  
12:44
so the script has just been executed everything  is okay this is the mask I have input and  
12:51
this is the file which was generated from this  mask and this looks super super super absolutely  
12:58
crazy right it's a lot of numbers it's like  a very very crazy thing without going to the  
13:04
details let's just say that this is exactly the  same information we had here this is exactly  
13:10
exactly the same information we have here but in  a different format let's let's keep the idea right  
13:16
exactly the same information in a different format  and that's exactly the format yolo V8 needs  
13:22
in order to train the semantic segmentation  model so this is exactly what you need to do  
13:28
once you have created all of your masks you need  to download these files into your computer and  
13:34
then please execute this script so you can  convert your images into a different type of files  
13:42
and obviously this script will be available in  the GitHub repository of today's tutorial so  
13:48
that's pretty much all in order to create  your annotations in order to download these   annotations and in order to format everything the  way you should now let me show you the structure  
Data structure
13:58
you need to format the way you need to structure  all of your file system so it complies with  
14:04
yolov8 remember this is something we have already  done in our previous tutorials regarding yolov8  
14:10
once you have your data you need to structure  your data you need to format your data you need to  
14:18
structure your file system so yolo V8 finds  absolutely where everything is located right  
14:24
you're going to locate your images in a given  directory you're going to locate your annotations   your labels in another directory so everything  is just the way yolov8 expects it to be  
14:34
right so let me show you I have a directory  which is my root directory which is called Data  
14:40
within data I have three directories but this  directory the Masks directory is not really  
14:47
needed it's just there because that's the way I  got my masks my data but it's not really needed  
14:55
in order to show you this directory which is the  one containing all of my binary masks in order  
15:01
to be more clear that this is not needed for this  part of this process what I'm going to do is I'm  
15:07
going to delete this directory right now it's  gone okay now we only have two directories and  
15:15
these are exactly the directories we need in this  part of this process where we are creating all   the structure for our data so images you can see  that we have two directories one of them is called  
15:25
images the other one is called labels within  images we have two other directories one of them is   called train and the other one is called val and  train is the directory where we are going to  
15:34
have all of our training data this is where we are  going to have all of our training images these are  
15:40
all the images yolo V8 is going to use in order  to train the model in order to train the semantic  
15:47
segmentation model then val also contains images  and these are the images we are going to use in  
15:54
order to validate the model right so remember you  need to have two directories one of them should be  
16:00
called train is very important the name it should  be called train and the other one should be called val  
16:07
now going back you can see that we have two  directories one of them is images the other one  
16:12
is labels and if I go within labels you can see  that there are two directories also they are named  
16:17
train and val and if I open these directories  these are the type of files I have generated with  
16:24
the exact same script I showed you a few minutes  ago so within labels we have two directories  
16:31
train and val and train are all the annotations  we have generated from the training data from the  
16:40
training masks right and long story short we have  our root directory within the root directory have  
16:48
two directories one of them is called images  the other one is called labels within images   we have two directories train and val within  train and within val it's all of our data all  
16:57
of our images and within labels it's exactly the  same structure two directories train and val and  
17:02
within train and within val it's where we locate  all of our annotations right that's exactly the  
17:09
structure you need for your data please remember  to structure your file system like this otherwise  
17:16
you may have an issue when you are trying to train  a semantic segmentation model using yolo V8  
17:22
so that's pretty much all in order how to  structure the data and now let's move to  
Train
17:28
the interesting part let's move to the most fun  part which is training this semantic segmentation  
17:34
model now let's move to pycharm and I will show  you how to train it from your local environment  
17:40
so let's continue this is a pycharm project I  created for today's tutorial please remember to   install this project requirements otherwise you  will not be able to use yolo V8 now let's go  
17:50
to train.py this is a python script I created and  this is where we are going to do all the coding we  
17:56
need in order to train the semantic segmentation  model using yolo V8 and now let's go back to the  
18:04
yoloV8 official repository because let's see  how exactly we can use this YOLO this model in  
18:12
order to train this semantic segmentation model  I'm going to the segmentation section and I'm  
18:17
going to click on segmentation Docs now this  is going to be very very straightforward I'm  
18:23
going to train I'm going to copy this sentence  which is load a pre-trained model and then going  
18:28
back to pycharm I'm just going to copy paste and  then I am going to from ultralytics import YOLO
18:38
then I'm also going to copy this sentence  which is a model.train I'm going to change  
18:43
the number of epochs of 2 something like one  because remember it's always very very healthy  
18:49
it's always a very good idea to do like a very  dummy training to train the model for only one   Epoch to make sure everything is okay to make  sure everything runs smoothly and then you do  
18:58
like a more more deeper training so I'm going  to change the number of epochs and then I'm   also going to change the config file I'm going  to use this config file which is a config file  
19:10
I have over here and obviously you will find this  config file in the repository of today's video so  
19:16
long story short you can see that you  have many many different keywords but the only   one that you need to edit is this one right this  is the absolute path to your data in my case if  
19:29
I copy and paste this path over here you can see  that this is the directory which contains the  
19:36
images and the labels directories so long  story short just remember to edit this path to  
19:43
the path to the location of your data because  if you have already structured everything in  
19:49
the way I mentioned in the way I show you in  this video then everything else will be just   fine right the train and the val keywords are  very good as it is I mean you can just leave  
19:59
everything as it is but please remember to edit  this field which is the location of your data  
20:06
now going back to train.py this is pretty much  all we need to do in order to train the semantic  
20:12
segmentation model so I'm just going to press  play and let's see what happens and you can see  
20:18
everything is going well we are training our model  but everything it's taken forever everything is  
20:23
just going to take forever even though we are only  training this model for only one Epoch everything  
20:28
is going to take a lot of time so what I'm going  to do instead is just press stop I'm going to stop  
20:34
this training everything is going well I'm not  stopping this training because I had an error   or something no everything is going well but I am  going to repeat the exactly the same process from  
20:44
a Jupiter notebook in my Google collab because  if I use Google collab I'm going to have access  
20:50
to a free GPU and it's going to make the process  much much much much faster so I am going to use  
20:57
a google colab in order to train this model and  I recommend you to use a Google collab as well  
21:03
so I'm going to show you how to do it from  your Google collab environment please remember   to upload your data before doing anything in  Google colab please remember to upload your  
21:13
data otherwise it's not going to work for example  here you can see I have many directories one   of these directories is data and within data you  have labels and images and these are exactly the  
21:22
same directories I have over here so I have  already uploaded my data into my Google Drive   please remember to do it too otherwise you will  not be able to do everything we're going to do  
21:33
just now right so that's one of the things you  need to upload and then also remember to upload  
21:39
this config.yaml file the same file I showed you  in my local computer you also need this file here  
21:46
the only thing you will need to edit is this  path because now you need to specify the path  
21:52
the location of your data in Google Drive so I'm  going to show you exactly how to locate your data  
21:59
into your Google Drive and now let's move to the  Jupiter notebook obviously I'm going to give you  
22:05
this notebook this is going to be in the GitHub  repository of today's tutorial so you can just   use this notebook I'm just going to show you how  to execute absolutely every single cell and how  
22:14
everything works right and exactly how everything  exactly what everything means right exactly what  
22:21
are you doing absolutely every single cell so the  first thing I'm doing is just connecting  
22:26
my Google collab environment we google drive because  remember we need to access data from Google Drive  
22:32
so we definitely need to allow google collab to  access Google Drive so I'm just going to select  
22:38
my account and then I scroll all the way down and  press allow that's going to be pretty much all we  
22:46
need to wait a couple of seconds and now let's  continue what I'm going to do now is to Define  
22:51
this variable which is data dir and this is  the location of my data in my Google Drive now  
22:57
please mind this path this location because please  mind the way this is structured right please mind  
23:05
the first word is content then gdrive then my  drive and then is my relative path to my data so  
23:13
if you want to know exactly where you have upload  your data if you're not completely sure where you  
23:18
have uploaded your data what you can do is to do  an ls like I'm doing right now and it's going  
23:24
to give you all the files you have in the root  directory of your Google Drive then from there   just navigate until the directory where you have  uploaded your data in my case is my drive computer vision  
23:36
engineer image segmentation yolo V8 and then  data that's exactly where my data is located in  
23:42
Google Drive if I go to this directory you can see  that this is my drive then you can see that this  
23:49
is my drive then computer vision engineer image  segmentation yolo V8 and then data and this  
23:57
is exactly what I have over here so once you have  located your data the only thing you need to do is  
24:03
to edit this cell and to press enter so everything  is ready now I'm going to install ultralytics so  
24:10
I can use yolo V8 from The Notebook and this  is going to take a few seconds but this is going  
24:16
to be ready in no time, something you need to  do from your Google colab is to go to runtime  
24:23
and change runtime type just make sure it  says GPU just make sure you are using Google  
24:31
collab with GPU because if you are not using a  google collab with GPU everything is pretty much  
24:37
pointless right so just remember to check if  you are using a Google colab with GPU or not just  
24:45
do it before you start all this process because  otherwise you will need to run absolutely  
24:51
everything again so let's continue I have already  installed ultralytics and now I am going to  
24:56
run this cell and if you realize this is exactly  exactly the same type of information the same code  
25:04
I have over here in my local environment right I'm  just defining a model and then I am just training  
25:10
this model so what I need to do now is just  press enter and also mind that I have specified  
25:16
the config file right the location of my config  file and now I'm going to run a full training or  
25:23
actually I'm going to run a training for 10 epochs  so this is what I'm going to do and this is also  
25:28
going to take some time although we are going to  use a GPU this is going to take a few minutes as  
25:33
well so what I'm going to do now is just I'm going  to wait until this is completed and I'm going to  
25:39
pause my recording here and I'm just going to fast  forward this video until this process is completed  
25:45
okay so the training process is now completed we  have trained this model and everything is just  
25:50
fine and you can see the results have been saved  here under runs segment and train2. so the only  
25:57
thing we need to do now is to get the results we  got from this training we need to get the weights  
26:03
we need to get all the results all the different  metrics or the different plots because what we   need to do now is to analyze this training process  we need to validate that everything is just fine  
26:13
right so what we are going to do now is to get  all this information and the easiest way to do  
26:19
it is just running this command what we will do  when running this command we are going to copy  
26:25
all the content in the this directory  where the results have been saved under our  
26:31
Google Drive right remember to edit this URL, remember  to edit this path, this location, because  
26:38
you want to copy everything into a directory into  your Google Drive so just make sure everything is  
26:44
okay make sure this location makes sense and you  can just execute this cell and you're going to  
26:50
copy everything into your Google Drive now let me  show you my Google Drive I have already executed  
26:56
this cell so everything is under my Google Drive  this is the runs directory which was created when I ran  
27:01
that cell and under this other directory which is  called segment we have train2 so these are all  
27:09
of our results these are the results we are now  going to analyze so what I'm going to do now is  
27:15
just to download this directory and once we have  this directory into our local computer then we are  
27:23
going to take a look at all the plots at all the  metrics and I'm going to tell you exactly what   I usually do in order to validate this training  process so everything is now downloaded everything  
27:33
is now completed and let's take a look at these  files so what I'm going to do is I'm just going to  
27:41
copy everything into my desktop I need to do some  cleaning by the way so these are all the results  
Validate model
27:48
we got from this training process you can see that  this is a lot of information this is definitely  
27:54
a lot of information right we have many many  different files we have many different everything   we have the weights over here we have a lot of  information so let me tell you let me give you  
28:05
my recommendation about how to do this evaluation  how to do this validation from all these plots and  
28:13
from all of these results I would recommend you  to focus on two things one of them is this plot  
28:21
one of them is all of these metrics  and then I'm also going to show you how to take  
28:26
a look at these results at these predictions  from these images but for now let's start here  
28:33
you can see that this is a lot of information  these are a lot of metrics and you can  
28:39
definitely knock yourself out analyzing all the  information you have here you can definitely go  
28:45
crazy analyzing all of this information all  of these plots but I'm going to show you like   a very very simple and a very straightforward way  to do this analysis to do this validation this is  
28:56
something that I have already mentioned in my  previous videos on yolo V8 on how to train a  
29:01
model and how to validate this model which is take  a look what happens with the loss function take a  
29:07
look what happens with your loss plots with all  the plots which are related to the loss function  
29:13
and as this is a semantic segmentation type  of algorithm I would tell you take a look what  
29:18
happens with this loss with the segmentation loss  I would say take a look what happens with the training loss and the validation loss
29:27
and long story short just make sure the loss  function goes down right if your loss function is  
29:35
going down it's likely things are going well it's  not a guarantee maybe things are not really going  
29:42
that well and the model it doesn't really perform  that well it may happen but I would say that if the  
29:48
loss function is going down it's a very good sign  if at the contrary your loss function is going up  
29:55
I would say you have a very very serious problem  I would say there is something which is seriously  
30:00
wrong with your training process or with your  data or with your annotations or with something  
30:06
you have done something seriously wrong or there's  something seriously wrong with your data but I'm  
30:11
talking about something amazingly wrong seriously  wrong right if your loss solution is going up  
30:18
I don't know what's going on but something is  going on do you see what I mean so having a  
30:23
loss function which is going down yeah it's not  a guarantee of success I mean it's not like it's  
30:29
a good model for sure no you may have a  situation where you haven't trained a good model  
30:35
and your loss function is going down anyway but I  would say that it's a very very good sign at the  
30:42
very least your training loss and your validation  loss should go down and I'm talking about a trend  
30:48
of going down right for example here we have a  few epochs in which the loss function is going up  
30:53
that's okay that's not a problem we are looking  for a trend we should have a trend for the loss  
30:59
function to go down and that's exactly what  we have in this situation so long story short  
31:05
that's my recommendation on how to do this this  validation how to do this analysis on all the  
31:12
metrics we have over here for now focus on these  two and make sure they are going down and then  
31:17
in order to continue with this process with  this validation is that we are going to take   a look at what happens with our predictions how  is this model performing with some data with some  
31:28
predictions and for this we are going to take  a look what happens with all of these images   right you can see that these are some batches  and these are some some of our labels some of  
31:39
our annotations for all of these images and then  these are some of the predictions for these images  
31:44
right so we are going to take a look what happens  here and for example I'm going to show you these  
31:51
results, the first image, and you can see  that looking at this image which again these are  
31:57
not our predictions but this is our data these are  our annotations these are our labels you can see  
32:04
that there are many many many missing annotations  for example in this image we only have one  
32:09
mask we only have the mask for one or four ducks  we have one two three four five dogs but only one  
32:15
of them is annotated we have a similar behavior  here only one of the ducks is annotated here  
32:20
is something similar only one of them is annotated  and the same happens for absolutely every single   one of these images so there are a lot of missing  annotations in this data we are currently looking  at
32:31
and if I look at the predictions now these are  the same images but these are our predictions we  
32:37
can see that nevertheless we had a lot of missing  annotations the predictions don't really look  
32:43
that bad right for example in this case we are  detecting One Two Three of the five Ducks we  
32:49
so we have an even better prediction that we have  over here I would say it's not a perfect detection   but I would say it's very good right it's like  it's not 100% accurate but it's like very good  
33:00
and I would say it's definitely better than the  data we used to train this model so that's what  
33:06
happens with the first image and if I take a look  at the other images I can see a similar Behavior  
33:12
right this is the data  we used for training this algorithm and these are  
33:18
the predictions we got for these images and  so on right it seems It's like exactly the  
33:24
same behavior exactly the same situation for  this image as well so my conclusions by looking  
33:30
at these images by looking at these predictions  is that the model is not perfect but I would say  
33:37
performs very well especially considering that  the data we are using to train this model seems  
33:44
to be not perfect seems to have a lot a lot  of missing detections have a lot of missing  
33:50
elements right a lot of missing objects so  that's our conclusion that's my conclusion by  
33:56
looking at these results and that's  another reason for which I don't recommend  
34:02
you to go crazy analyzing these plots because when  you are analyzing these plots remember the only  
34:08
thing you're doing is that you are comparing your  data the data you are using in order to train this  
34:15
model with your predictions right the only thing  you're doing, you're comparing your data with  
34:22
your predictions with the predictions you had with  the model right so as the only thing you are doing  
34:27
is a comparison between these two things then  if you have many missing annotations or many missing  
34:35
objects or if you have many different errors  in your data in the data you're using to train   the algorithm then this comparison it's a little  meaningless right it doesn't really make a lot of  
34:47
sense because if you're just comparing one thing  against the other but the thing you are comparing  
34:53
with has a lot of Errors it has a lot of  missing objects and so on maybe the comparison  
34:59
doesn't make any a lot of sense whatsoever right  that's why I also recommend you to not go crazy  
35:05
when you are analyzing these plots because they  are going to give you a lot of information but  
35:12
you are going to have even more information  when you are analyzing all of these results  
35:18
and this is a very very very good example of what  happens in real life when you are training a model  
35:25
in a real project because remember that building  an entire dataset, a dataset which is 100%  
35:32
clean and absolutely 100% perfect is very very very  expensive so this is a very good example of what  
35:41
happens in real life usually the data you're using  to train the model, to train the algorithm has a  
35:47
few errors and sometimes there are many many many  errors so this is a very good example of how this  
35:54
validation process looks like with data which  is very similar to the data we have in real life  
36:01
which in most cases is not perfect my conclusion  from this evaluation for this validation could be  
36:09
improving the data taking a look what's going  on with the data and the next step would be to  
36:16
improve the data and by looking at these results  one of the ways in which I could improve this data  
36:21
is by using the predictions I'm getting instead  of the annotations I I used to train this model  
36:27
you see what I mean if the annotations if the  predictions we are getting are even better that   the annotations maybe our next step will be to use  these predictions in order to train a new model do  
36:38
you see what I mean so the by analyzing all of  these results you are going to make decisions  
36:44
on how to move forward on how to continue and  this is a very good example of how this process  
36:49
look like in a real project this is pretty much  how it works or how it looks like when you are  
36:55
working in a project when you are working either  in a company or if you're a freelancer and you're  
37:01
delivering a project for a client this is pretty  much what happens right there are errors things  
37:06
happen and you need to make a decision given all  the information you get with all this analysis  
37:11
so that's going to be all in order to show you  this very simple and very straightforward way in   order to validate this training process in order  to make some conclusions regarding what's going on  
37:20
right and regarding to make some decisions you  know how to how to move forward this project  
37:26
or this training process and now let me show you  something else which is within this directory the  
37:32
weights folder this is where your weights will be  located right because if you are training a model  
37:39
is because you want to have a model in order to  make predictions in order to make inferences and  
37:44
this is where your models will be located this  is where your model will be saved and this is  
37:50
something I have already mentioned in one of my  previous videos regarding yolo V8 remember you   will have two models one of them is called last.pt  another one is best.pt and the way it works is  
38:02
that remember that when you are training a model at  the end of absolutely every single Epoch you are  
38:09
updating your weights you are updating your model  so at the end of absolutely every single epoch  
38:15
you already have a model which is available  which you can use if you want to so last.pt  
38:21
means that you are getting the last model  the model you got at the end of your training  
38:27
process so in this case I am training a network  for 10 epochs if I remember correctly so this is  
38:34
the model we got at the end of the tenth Epoch and  then base.pt means that you are getting the best  
38:41
model the best model you train during the entire  training process if I show you the metrics again  
38:49
let's see the metrics over here you can see that  we have many metrics which are related to the  
38:55
loss function and then other metrics related to the  accuracy on how this model is performing and the   way yolov8 decides what's the best model  in this case which is a semantic segmentation type  
39:05
of problem may be related to the loss function  maybe it's taking the model at which you got the  
39:12
minimum loss or it may be related to some of these  plots some of these performances which are related  
39:18
to the accuracy to the performance maybe it's getting  the model for which you got the maximum Precision  
39:24
for example or the maximum recall or something  like that I'm not 100% sure I should look at the  
39:30
documentation but the way it usually goes is  that last.pt is the last Model you trained so  
39:36
it's at the end of your training process and then  best.pt is your best model and this best model is  
39:43
decided under some criteria so that's basically  how it works and what I usually do is taking  
39:50
last.pt because I consider that last.pt is  taking, is considering, way more information much  
39:57
more information because we are taking much more  data we are taking much more everything right in  
40:04
all the training process we are doing many many  different things so if you take the last model  
40:10
you are summarizing way more information that's  the way I see it so usually I take the last model  
40:18
usually I take last.pt and that's pretty  much all in order to show you this validation  
40:24
how validating this model looks like and now  let's move to the prediction let's see how we can  
Predict
40:30
use this model in order to make inferences in  order to make predictions so let's see how we  
40:35
can do that so let's go to pycharm let's go to  the pycharm project of today's tutorial and this  
40:41
is a python script I created in order to do these  predictions this python file is called predict.py  
40:47
and this is what we're going to do I'm going to  start importing from ultralytics import YOLO and  
40:56
then I am going to define the model path the model  we are going to use which in our case let's use  
41:04
last.pt from these results, from this directory,  so I am going to specify something like this...   
41:13
last.pt and then let's define an image path let's  define the image we are going to use in order to  
41:20
get our inferences so the image will be located...  this will be from the... from the validation set I'm  
41:29
just going to choose a random image something  like this one so I am going to copy paste
41:41
I am just going to paste it here so this  is the image we're going to use in order to test  
41:46
this model in order to make our predictions  and now I'm going to import CV2 as well  
41:53
because I'm going to open I'm going to read this  image and then I am going to get this image shape  
42:02
so this will be something like this this will be  image and then this is image.shape okay and now  
42:16
the only thing we need to do is to get our model  by doing something like YOLO and then model path  
42:28
okay and then we are going to get the results by  calling model of our image right and this is it  
42:37
this is all we need to do in order to get our  results in order to get our inferences but now  
42:42
let's do something else I am going to iterate for  result in results and now let's take a look at  
42:51
this mask let's take a look at this prediction  so I'm going to iterate like this for j, mask  
42:58
in result dot masks dot data and then I am  going to say something like mask Dot numpy
43:15
times 255 and this is our mask  and then I am going to resize it
43:27
to the size of the image so I'm going to input the  mask and then this will be if I am not mistaking  
43:34
the order is this one W and then H so this is just  the way it works this this is how we need to do it  
43:41
in order to get the prediction and then in order  to resize this prediction back to the size of the  
43:46
original image so this is how it goes and now the  only thing we need to do is to call CV2 imwrite  
43:54
and I'm going to save it I'm going to save it here  and the name will be something like that let's  
44:02
call it output this is only a test so we don't  really need to go crazy with the name let's call  
44:07
it output.png and this will be our mask and that's  pretty much all that's pretty much all let's see  
44:16
what happens I'm going to press play Let's see if  everything is okay or if we have some error okay  
44:22
so I did get an error and yeah this is because  we need to enumerate I forgot the enumerate we  
44:31
are not using J actually so I could just iterate  in mask but let's do it like this okay everything  
44:37
ran smoothly everything is okay we didn't get any  error and now if I go back to this folder to this  
44:44
directory I can see this is the output this is the  output we got and now in order to make absolutely  
44:51
and 100% sure everything is okay and this is a good  mask this is a good prediction I'm going to make  
44:57
an overlay I'm very excited I don't know if you  can tell but I'm very excited I'm just going to  
45:02
take this image over here and then I'm going back  here and I'm going to take the original image I'm  
45:10
going to do an overlay so this will be raise to top  I'm going to align these two images together and  
45:17
now let's make a transparency and let's see what  happens and you can see that we may not get like  
45:23
a 100% perfect mask but it's pretty well it's like  a very very good mask especially considering the  
45:30
errors we detected in our data so this is amazing  this is a very good detection  
45:36
this is a very good result so this is going to be  all for this tutorial and this is exactly how you  
45:42
can train a semantic segmentation model using  yolo V8 and this is the entire process from how to  
45:48
annotate the data how to train the model and how  to validate this model and then how to make some   predictions so this is going to be all for today  so thank you so much for watching this tutorial  
45:58
my name is Felipe I'm a computer vision engineer  and these are exactly the type of projects I make in   this channel so if you enjoyed this video I invite  you to click the like button and I also invite you  
46:08
to subscribe to my channel I also invite you to  take a look at what other videos I have published   so far for example this is the video YouTube  thinks is best for you so if I were you I would  
46:19
definitely check out that video so this is going  to be all for today and see you on my next video
Transcript


Search in video
Intro
0:00
hey guys and welcome to a new video in
0:01
this video here we're going to do custom
0:03
update detection with the YOLO V8 model
0:05
so this model was just released a couple
0:07
of days ago we're going to train it on
0:08
our own custom data set we're going to
0:10
use roboflow we're going to actually
0:11
like have a data set uh which we're
0:13
labeled with bounding boxes around our
0:15
Optics we're going to detect different
0:17
mocks in our images then we basically
0:19
jump into Google collab I'll show you
0:21
how you can actually train your own
0:22
custom URL V8 model it is actually like
0:25
fairly easy to do I'll show the whole
0:27
process I'll show you how to export it
0:28
and how we can actually like Import in
0:30
our own python script and then inside of
0:32
our python script we're going to run
0:33
real-time inference with our own custom
0:36
trained YOLO V8 model
Setup and Dataset
0:39
so we're going to jump straight into
0:40
Google app I'm going to show you the
0:42
block of code that you need to run to
0:43
act like run and train this YOLO V8
0:45
model we're going to do validation on
0:47
the model and then I'm going to show you
0:48
how we can actually export it we're just
0:50
going to download the model that we're
0:51
trained here in Google lab and then
0:54
we're basically just going to export the
0:55
model I'm going to show you how we can
0:56
load it into our own python script so
0:59
you guys can play around with it and you
1:00
can actually use it in your own projects
1:02
and applications so first of all here
1:04
I've just connected the runtime to the
1:06
GPU first of all we'll just verify that
1:08
we actually have access to the GPU for
1:10
the training because this will just
1:12
speed up our process a lot then we need
1:14
to PIV install Ultra Linux here which
1:16
will basically like be installation of
1:18
YOLO V8 so inside this autolytics we
1:20
have the yellow V8 model
1:22
if you're not familiar with the lv8
1:24
model I have another video where
1:25
actually just went over the GitHub
1:28
repository I showed you how you can do
1:29
live inference on a pre-trained Model
1:31
but here we can see that we have now
1:33
installed the YOLO V8 model and now we
1:35
are actually ready to use it
1:37
so here we can just import the different
1:39
kind of modules from other lytics we're
1:41
going to import YOLO we're going to
1:42
import OS and some display here so we
1:45
can actually display our predictions
1:46
with our model
1:48
here we're just going to run this block
1:49
of code and now we should actually have
1:51
all the modules imported that we need so
1:54
the new yellow V8 model here from
1:55
autolytic is act like setup uh really
1:58
really nicely
2:00
because we just have this kind of like
2:01
YOLO format we just have like this whole
2:03
Coast structure where we have the YOLO
2:06
files and basically you just run these
2:09
YOLO commands here into the Python
2:10
scripts you can pass in different kind
2:12
of like arguments to it methods and
2:14
modes and so on so now we can actually
2:16
adjust first of all before react like
2:18
we're in the trainer model we'll just
2:19
see how to do interference on a single
2:22
example with a pre-trained dual V8 model
2:25
so basically we're not just have this
2:27
YOLO command here now from the YOLO
2:30
you'll V8 repository from autolytics
2:33
that were just PIV installed then we can
2:35
just set the task here equal to detect
2:37
so then we would like like do update
2:39
detection we can also do instant
2:40
segmentation and classification with the
2:42
yellow weight model I'm going to create
2:43
another video where we're going to train
2:45
our own custom instance segmentation
2:47
model as well and then we're going to
2:48
use it in our own python script
2:51
so then basically we just did the mode
2:53
here equal to predict we have the model
2:55
equal to the YOLO V8 Nano model we can
2:57
also choose a small model medium model
2:59
large model and so on
3:00
we can set up a confidence score here so
3:02
we basically just filter out like
3:04
um predictions with a low confidence
3:06
score we can also like increase this and
3:08
then we just set the source here equal
3:10
to the image that we actually want to
3:12
make a prediction on you can also do
3:14
like for example a webcam here if you
3:15
just specify zero but then you will have
3:17
to have it in your own environment on on
3:19
your local computer but basically here
3:22
we're just going to run it to see a
3:23
single example and then we will train
3:25
our own yellow model on our own custom
3:27
data set so that'll be way more exciting
3:30
than just sharing this example but here
3:31
I'm just going to show you how we can do
3:33
appearance on a single example with a
3:35
pre-trained YOLO V8 model then the
3:38
results you are saved it runs
3:40
um slash detect slash predict and then
3:43
we can basically just go in uh specify
3:45
the file name here for that and show it
3:46
as an image here in Google collab
3:50
here we see the predictions we get a
3:51
talk we get a person we just acts like a
3:54
really low confidence score for the talk
3:56
and it also like extends the boundary
3:58
box all the way to the bottom we also
4:00
have to take the car in the background
4:01
so again we get some really nice
4:03
predictions we can like here again we're
4:05
using the Nano model we can also use
4:06
like the larger models and and so on and
4:09
we'll get way higher accuracy and
4:11
confidence in our predictions
4:14
so I was not trained the yellow V8 model
4:15
on a custom data set first of all here
4:18
I'll go inside my roboflow account you
4:20
can basically just go in here as well
4:21
take one of my data sets or you can go
4:23
to the universe where you can basically
4:25
like find all the public data sets when
4:27
we're doing object detection you can
4:28
just press like optic section here
4:31
and then you will get all the different
4:32
kind of like update sections uh projects
4:35
that is available here publicly in the
4:37
roboflow universe uh you can basically
4:39
just go ahead and take one of them
4:40
export data set as I'm going to do now
4:44
but then basically we just go inside our
4:46
roboflow project in this example we are
4:49
doing cup detection and I have this
4:51
version two
4:52
then we can basically just go in here
4:53
hit export inside of our projects we
4:56
have the yellow V8 format show
4:57
downloadable code and then we hit
4:59
continue it will then zip our folder and
5:01
we can basically just take this code
5:03
copy it and paste it into our Google
5:04
collab and then we can specify the path
5:06
to that to that folder inside of our
5:09
notebook
5:11
so if we just go back here again to our
5:13
training of the yellow V8 model we can
5:15
then just specify copy paste in the data
5:18
set then when we actually want to do
5:20
detections I'm just going to run these
5:21
blocks of code so here we download the
5:23
data set it will unsub it and we will
5:25
get it over here to the left
5:27
in our directory we can also then run
Train YOLOv8 Model
5:29
the detection mode here we can run train
5:31
so we hit we want to do optic detection
5:33
we can set the mode equal to train we
5:35
just specified a model that we want to
5:37
train as well so here we can actually
5:39
just try with this with the media model
5:41
and then for data set we basically just
5:43
have our data set variable from roboflow
5:45
so this will be our data set variable up
5:47
here we can specify the location for
5:49
that and over here to the left we can
5:51
now see that I have my data set so we
5:52
have computation version 2.3 we have a
5:55
test set of train set and a validation
5:57
set so this is basically old images and
6:00
also the labels for our Optics that we
6:02
want to train our custom ylv8 model for
6:06
so again we just specified the data set
6:08
location number of epochs we want to
6:10
train it for and also our image size
6:13
then when we want to train our model we
6:15
just run this block of code again this
6:17
notebook here will be a weightable on
6:19
GitHub there's a link down description
6:20
so basically just go check that out copy
6:22
paste it run it on your own custom data
6:25
set export the model and then use it in
6:27
your own applications and projects so
6:29
this is actually really easy it's really
6:31
cool that we can set it up so quickly
6:32
run object detection models and and even
6:35
like just
6:36
train these models here data sets you
6:39
can just label your data set with
6:40
roleflow export them and use it in your
6:42
own projects so here we're just going to
6:44
run word for for the epochs first of all
6:46
we get a summary of our model we have a
6:49
bunch of convolutional layers um
6:50
concatenation layers we do some up
6:52
sampling and so on
6:54
we need Optimizer so we're using SGD
6:56
with a learning rate of 0.01
6:59
we can also see some um data
7:03
augmentations here we can see the
7:04
validation set and then we will
7:06
basically get the training down here at
7:08
the bottom
7:09
so here we can see that we have Epoch
7:11
one of 30. uh we see we get a box loss
7:14
class loss and some instances so how
7:16
many instances are we actually detecting
7:18
we also we will also get some
7:21
some information about the mean average
7:23
position of 50 and also mean average
7:25
position 50 to 95 in intervals of five
7:29
so this is the most important like uh
7:31
like metrics to look at I mean you have
7:34
positions so we basically just we can
7:36
basically just see here that our
7:37
minerals position is acts like
7:39
increasing for uh for each Epoch that
7:42
we're training and again we're using the
7:44
medium yellow V8 model so now we will
7:47
just let it run for the epochs we'll go
7:49
back in show some results and how to act
7:51
like export this model and also do
7:54
validation of it
8:07
so now model is done training for 30
8:10
epochs we can basically just see the
8:12
results here at the end we get some
8:13
really uh really nice results uh if you
8:17
just go to epoch30 out of 30 we have it
8:20
up here we can see that we get really
8:21
high
8:22
um we have we get really high mean
8:24
average station 50 and also really high
8:26
mean average position 52.95 we get
8:28
around like an average position of 0.87
8:33
and over here to the right we also get
8:35
my point 58 here 40 million hours
8:38
position of 5 to 95 if we just compare
8:40
this with the Benchmark of the yolav8
8:42
model on the cargo data set they get
8:45
around like Point 44 45 in this metric
8:49
here
8:50
so these are some really nice results we
8:51
can also just see down for all the
8:53
classes that we're trying to detect in
8:55
our object detector we get some really
8:57
nice results with really high precision
8:59
the the results are saved in runs detect
9:02
uh train so we can basically just go
9:04
inside that so this is the the file
9:05
directory for that so we have the
9:07
content run detect train and then we're
9:09
here we're just going to show the
9:10
confusion Matrix
9:13
so in the confusion Matrix we will have
9:15
our um we will have our true labels down
9:17
at the Y X axis and then we have to
9:19
predicted labels on the y-axis we can
9:22
see that we get some really nice
9:23
detections for that we have some
9:25
problems with this cup with the with the
9:27
lowercase C because this is a like some
9:29
kind of like redundant class there
9:31
there's like there's only like a couple
9:32
of samples in that so it's really
9:34
underrepresented in our data set we can
9:36
also go in and see that inside of our
9:38
roboflow data set so if you just go down
9:41
to the health check we can see that this
9:43
cup here only has like 10 and like nine
9:45
nine samples so this is an error in the
9:47
data set you will go in and correct for
9:49
that and also just a standard cup here
9:51
with the upper KC
9:53
um we only have 42 samples so this is
9:55
under represented
9:56
um this is an underrepresented class in
9:58
our data set
10:00
but this is actually a really good
10:01
confusion Matrix we can see the rest of
10:03
the classes it is really confident about
10:05
that and then we can basically just show
10:06
uh the training results as well
10:10
so here we should be able to see the
10:11
training graphs you can also get all
10:13
these files in if you're inside the runs
10:15
detect and then we have our train and
10:18
then you'll basically see all both the
10:20
the F1 Precision recall curve the
10:22
confusion Matrix and then the results
10:24
that we're sharing right now so we'll
10:25
have that inside your train folder so
10:28
we'll just close this one here and then
10:29
we can see that the Box loss is still
10:31
decreasing and hasn't really converged
10:32
yet we get a really nice decline in our
10:34
parked lot and also the class laws we
10:36
can see the Precision it is really
10:38
stable up here we'll get the mean air
10:40
position it is kind of like converts
10:42
here around like 80 83 uh here for the
10:45
for the other metric for the mineral
10:47
position of 50 to 95 we can still we can
10:50
see that our average position is act
10:53
like still increasing so we could train
10:54
for more epochs and get actually we can
10:57
get better results but we already have
10:59
some really nice results so we're just
11:00
going to go with that here we're going
11:02
to run into the validation mode so we
11:04
basically just set the mode equal to
11:05
validation and now we just specify the
11:08
weights that we want to do validation
11:09
off so this will be our train weights
11:12
and then the best so these are also the
11:13
ways that we're going to export I can
11:15
just do that right now and again we just
11:17
specified the path to our data set so if
11:20
we go to the folder to the left
11:22
we can then see under our train we have
11:24
the this weight file we can get the best
11:26
best weights here so the best results
11:28
for the metrics uh while we have been
11:31
training our neural network or our
11:32
yellow V8 model so if you're going to
11:34
train it for like 100 epochs and at some
11:37
point like around like 50 epochs it
11:39
doesn't really learn more or it it
11:41
starts to increase in loss and decrease
11:43
in performance again you can just go in
11:45
here and take the best model throughout
11:47
the whole training process if you just
11:48
want to get the last Model you can also
11:50
take that but here we're going to go
11:51
with the best model you just right click
11:53
and hit download then it will download
11:55
your computer and then we can import
11:57
that into our own python file and run it
11:59
with the new like yolavi yolv8
12:02
architecture
12:03
in our own custom python file and we
12:05
will actually be able to run it in real
12:07
time as you're going to see in just a
12:09
second
12:10
so here we will just download this model
12:12
to our computer we can go inside and
12:13
just run the validation mode again I'm
12:16
just going to run this block of code for
12:17
the object detection
12:20
and here it's basically just passing all
12:21
the images through our validation or
12:24
like through a model for a validation
12:25
and then we should get some outputs down
12:27
here at the bottom so again we get some
12:30
really nice precision mean air positions
12:32
on our on our classes for our validation
12:35
so after we're validated our model we
12:38
can now set it to prediction mode and
12:40
then we basically just do predictions on
12:42
images that has not been trained or
12:43
validated on we set the task equal to
12:45
detect and the mode we set that equal to
12:47
protect now we basically just again we
12:49
specified the path to the best weight
12:51
and then also the data set location for
12:54
our test images and we also set up a
12:56
confidence score a threshold so here
12:58
we're just going to run this code code
13:00
with this YOLO command and then after
13:01
that we can just go down have a follow
13:03
running through a bunch of images and
13:05
show the results to ours
13:07
so here the results are saved in runs
13:10
detect on slash predict 2.
13:13
if you want to get these images again
13:14
you can go inside the folder you can
13:17
inside runs detect and then predict two
13:19
and then we will get all the images here
13:21
you can download them or just show them
13:22
directly in the tab if you double tap
13:25
but here we're just going to show it
13:27
with code we're just going to show free
13:28
examples and then we can basically just
13:31
run this block of code
13:33
so now we can see the results we get a
13:34
really nice prediction of the white cup
13:36
here even though it is seen from above
13:38
another example here is the Kaka cup uh
13:41
cop the standard cup with a really high
13:43
confidence score we have the
13:44
hand-painted cup uh Halloween Cup and
13:46
the white cup uh if I just scroll a bit
13:49
up here let's try a couple of more
13:50
examples so let's go with five
13:55
and it will basically just run the
13:57
predictions and specialize the results
13:59
so again you can just see that these
14:01
results here are really nice we have the
14:02
stand of a cup we have the a cup
14:04
white cup hand painted cup and Halloween
14:06
cup and all of them has had like really
14:08
high confidence score so now when our
14:11
model is actually exported we can go
14:13
inside our own python script then we can
14:15
actually go in and do live inference
14:16
with a webcam we'll just open up a
14:18
webcam do live inference on this custom
14:20
off detection data set and then we're
14:22
going to see some really nice real-time
Deploying YOLOv8 Model
14:24
results so we're not jumping into Visual
14:26
Studio I'm going to show you how we can
14:28
actually run this live inference I
14:30
basically just export the model I
14:31
downloaded and just specify the path
14:34
here to to the model and I just renamed
14:37
it to cup detector here with this yellow
14:39
from ultrolytics import yellow we're
14:41
basically just going to to specify the
14:43
path to our model and then it will load
14:45
in it will load in the model and we can
14:47
call this predict method on it
14:49
afterwards we can specify the source you
14:51
can either have like a video image we
14:54
have folder with images or we can
14:55
specify this zero and it will then use
14:57
the webcam which is it show equal to
15:00
true so this function basically just
15:02
takes in all like the command the
15:04
command arguments that we did in Google
15:05
call lab as well so we have to predict
15:07
we specify the source to show we want to
15:10
visualize the results that we're doing
15:12
on our live webcam and we can also
15:14
specify the confidence score so if our
15:16
confidence score for the predictions are
15:18
is greater than like 0.5
15:21
then we will visualize the predictions
15:23
and if they're lower than this threshold
15:25
we'll just discard the detections so
15:27
this is basically everything you need to
15:29
do in another video I'm going to show
15:30
you how we can actually modify
15:32
um this like this dataction predict the
15:34
class up here at the top and how we can
15:36
export export the results ourself using
15:39
our own applications and all those
15:40
different things so definitely hit make
15:42
sure to hit subscribe button and
15:43
notification under the video so you can
15:45
notify it when I upload and now another
15:47
video where we actually see how we can
15:49
modify this code for our own
15:51
applications and projects I'm also going
15:53
to create a video where we have a
15:54
Tracker Where we basically track these
15:56
Optics uh that we're that we're
15:58
detecting in our image but here
16:00
basically we just run this predict mode
16:02
predict function on the model and now
16:04
I'm going to run the program and then we
16:06
can just see the results so this should
16:08
be able to run in real time I have an
16:09
Nvidia RTX 4090 car we can see all the
16:13
parameters here we can see the the
16:14
inference time so here we have around
16:15
like five five milliseconds
16:18
that in my five milliseconds so that's
16:20
around like 200 Hertz so this actually
16:22
runs on 200 with 200 frames per second
16:26
and again this was the medium model that
16:28
we trained
16:30
so you can see the predictions we just
16:31
have the webcam up here running
16:32
sometimes it gets some false predictions
16:34
it takes this as a cop but with a really
16:36
low confidence score I'll just take the
16:38
webcam down here and then I'll grab a
16:40
bunch of cops or mocks and then let's
16:42
see if we can actually do a live
16:44
inference on the x-like Optics that we
16:46
have trained the models on
16:59
so here if I just turn the webcam around
17:01
we should be able to detect these mucks
17:03
so here we can see we have to hand paint
17:04
a cup we have the Halloween cup and we
17:06
also have the Cocker cup we can see here
17:08
we have the wine cup and we should be
17:10
able to take the standard cover as well
17:11
this was underrepresented in our data
17:13
set sometimes it loses track maybe we
17:16
should try to take it a bit further out
17:20
so it's not really that happy about this
17:22
um this standard cup just try to move it
17:25
a bit over here to the left
17:27
so here we detected as the as the
17:29
standard cup
17:30
but with a really low confidence score
17:33
yeah so so all these other cups here are
17:35
really nice we can see that we can run
17:37
in real time we had around like five
17:39
milliseconds so this is 200 frames per
17:41
second uh I have a a 14 490 RTX and from
17:44
Nvidia so this is actually like a really
17:46
uh crazy graphics card but you can run
17:48
this on like
17:50
um just standard gpus and you can even
17:52
run running like CPUs you can have
17:53
different Frameworks to export uh your
17:56
models you'll be a also supports that
17:59
these are just some really nice results
18:01
I just take this cob up and try to like
18:02
play around with it and we should be
18:04
able to still get some some detection so
18:06
we see we have this
18:08
cargo cop detection let's see if we can
18:10
get it from it above can really take it
18:13
here so yeah we take the yellow collar
18:15
here's the Cocker crop it is really hard
18:16
to see that this acts like a Cocker cup
18:18
just from above here
18:20
um but when we move it around
18:22
it is able to actually detect this as a
18:24
cargo cup again really crazy model like
18:26
these models here are so fast you just
18:29
saw how fast it was to actually like go
18:30
in train it in Google lab export the
18:32
model and then just throw it into in
18:34
here to your own python script and then
18:35
you can play around with it
18:37
so this is actually like a really cool
18:38
model it can be used for a lot of
18:40
different kind of things I hope you guys
18:41
are going to create some really nice
18:42
projects on your own custom data sets
18:44
definitely throw down in the comment
18:46
section if you have some nice ideas of
18:48
what we should try to do with these only
18:49
8 models and also if you get some really
18:52
nice results so thank you guys for
18:53
watching this video here and again
18:54
remember to subscribe button and Bell
18:56
notification under the video also like
18:58
this video here if you like the content
18:59
and you want more in future it will help
19:01
me and your channel out in a massive way
19:03
I'm doing these computer vision and deep
19:05
learning tutorials wherever you're over
19:06
the basics about computer vision deep
19:08
learning how to train neural networks
19:10
what are the different kind of like high
19:12
parameters and how is new networks
19:14
actually like trained so if you're
19:15
interested in that I'll link to one of
19:17
the tutorials up here or else I'll see
19:19
you next video guys bye for now
Skip to content
Register now for

Ultralytics YOLO Vision
Shenzhen

logo
Ultralytics YOLO Docs
Train




Search
Ctrl
K
 ultralytics/ultralytics
v8.3.221
47.8k
9.2k
Home
Quickstart
Modes
Tasks
Models
Datasets
Solutions 🚀
Guides
Integrations
HUB
Reference
Help
Tasks
Detect
Segment
Classify
Pose
OBB
Modes
Train
Val
Predict
Export
Track
Benchmark
Table of contents
Introduction
Why Choose Ultralytics YOLO for Training?
Key Features of Train Mode
Usage Examples
Multi-GPU Training
Idle GPU Training
Apple Silicon MPS Training
Resuming Interrupted Trainings
Train Settings
Augmentation Settings and Hyperparameters
Logging
Comet
ClearML
TensorBoard
FAQ
How do I train an object detection model using Ultralytics YOLO11?
What are the key features of Ultralytics YOLO11's Train mode?
How do I resume training from an interrupted session in Ultralytics YOLO11?
Can I train YOLO11 models on Apple silicon chips?
What are the common training settings, and how do I configure them?
Model Training with Ultralytics YOLO
Ultralytics YOLO ecosystem and integrations

Introduction
Training a deep learning model involves feeding it data and adjusting its parameters so that it can make accurate predictions. Train mode in Ultralytics YOLO11 is engineered for effective and efficient training of object detection models, fully utilizing modern hardware capabilities. This guide aims to cover all the details you need to get started with training your own models using YOLO11's robust set of features.



Watch: How to Train a YOLO model on Your Custom Dataset in Google Colab.

Why Choose Ultralytics YOLO for Training?
Here are some compelling reasons to opt for YOLO11's Train mode:

Efficiency: Make the most out of your hardware, whether you're on a single-GPU setup or scaling across multiple GPUs.
Versatility: Train on custom datasets in addition to readily available ones like COCO, VOC, and ImageNet.
User-Friendly: Simple yet powerful CLI and Python interfaces for a straightforward training experience.
Hyperparameter Flexibility: A broad range of customizable hyperparameters to fine-tune model performance.
Key Features of Train Mode
The following are some notable features of YOLO11's Train mode:

Automatic Dataset Download: Standard datasets like COCO, VOC, and ImageNet are downloaded automatically on first use.
Multi-GPU Support: Scale your training efforts seamlessly across multiple GPUs to expedite the process.
Hyperparameter Configuration: The option to modify hyperparameters through YAML configuration files or CLI arguments.
Visualization and Monitoring: Real-time tracking of training metrics and visualization of the learning process for better insights.
Tip

YOLO11 datasets like COCO, VOC, ImageNet and many others automatically download on first use, i.e. yolo train data=coco.yaml
Usage Examples
Train YOLO11n on the COCO8 dataset for 100 epochs at image size 640. The training device can be specified using the device argument. If no argument is passed GPU device=0 will be used if available, otherwise device='cpu' will be used. See Arguments section below for a full list of training arguments.

Windows Multi-Processing Error

On Windows, you may receive a RuntimeError when launching the training as a script. Add a if __name__ == "__main__": block before your training code to resolve it.

Single-GPU and CPU Training Example

Device is determined automatically. If a GPU is available then it will be used (default CUDA device 0), otherwise training will start on CPU.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.yaml")  # build a new model from YAML
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)
model = YOLO("yolo11n.yaml").load("yolo11n.pt")  # build from YAML and transfer weights

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

Multi-GPU Training
Multi-GPU training allows for more efficient utilization of available hardware resources by distributing the training load across multiple GPUs. This feature is available through both the Python API and the command-line interface. To enable multi-GPU training, specify the GPU device IDs you wish to use.

Multi-GPU Training Example

To train with 2 GPUs, CUDA devices 0 and 1 use the following commands. Expand to additional GPUs as required.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model with 2 GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[0, 1])

# Train the model with the two most idle GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[-1, -1])

Idle GPU Training
Idle GPU Training enables automatic selection of the least utilized GPUs in multi-GPU systems, optimizing resource usage without manual GPU selection. This feature identifies available GPUs based on utilization metrics and VRAM availability.

Idle GPU Training Example

To automatically select and use the most idle GPU(s) for training, use the -1 device parameter. This is particularly useful in shared computing environments or servers with multiple users.


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolov8n.pt")  # load a pretrained model (recommended for training)

# Train using the single most idle GPU
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=-1)

# Train using the two most idle GPUs
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device=[-1, -1])

The auto-selection algorithm prioritizes GPUs with:

Lower current utilization percentages
Higher available memory (free VRAM)
Lower temperature and power consumption
This feature is especially valuable in shared computing environments or when running multiple training jobs across different models. It automatically adapts to changing system conditions, ensuring optimal resource allocation without manual intervention.

Apple Silicon MPS Training
With the support for Apple silicon chips integrated in the Ultralytics YOLO models, it's now possible to train your models on devices utilizing the powerful Metal Performance Shaders (MPS) framework. The MPS offers a high-performance way of executing computation and image processing tasks on Apple's custom silicon.

To enable training on Apple silicon chips, you should specify 'mps' as your device when initiating the training process. Below is an example of how you could do this in Python and via the command line:

MPS Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model with MPS
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device="mps")

While leveraging the computational power of the Apple silicon chips, this enables more efficient processing of the training tasks. For more detailed guidance and advanced configuration options, please refer to the PyTorch MPS documentation.

Resuming Interrupted Trainings
Resuming training from a previously saved state is a crucial feature when working with deep learning models. This can come in handy in various scenarios, like when the training process has been unexpectedly interrupted, or when you wish to continue training a model with new data or for more epochs.

When training is resumed, Ultralytics YOLO loads the weights from the last saved model and also restores the optimizer state, learning rate scheduler, and the epoch number. This allows you to continue the training process seamlessly from where it was left off.

You can easily resume training in Ultralytics YOLO by setting the resume argument to True when calling the train method, and specifying the path to the .pt file containing the partially trained model weights.

Below is an example of how to resume an interrupted training using Python and via the command line:

Resume Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("path/to/last.pt")  # load a partially trained model

# Resume training
results = model.train(resume=True)

By setting resume=True, the train function will continue training from where it left off, using the state stored in the 'path/to/last.pt' file. If the resume argument is omitted or set to False, the train function will start a new training session.

Remember that checkpoints are saved at the end of every epoch by default, or at fixed intervals using the save_period argument, so you must complete at least 1 epoch to resume a training run.

Train Settings
The training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model's performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.

Argument	Type	Default	Description
model	str	None	Specifies the model file for training. Accepts a path to either a .pt pretrained model or a .yaml configuration file. Essential for defining the model structure or initializing weights.
data	str	None	Path to the dataset configuration file (e.g., coco8.yaml). This file contains dataset-specific parameters, including paths to training and validation data, class names, and number of classes.
epochs	int	100	Total number of training epochs. Each epoch represents a full pass over the entire dataset. Adjusting this value can affect training duration and model performance.
time	float	None	Maximum training time in hours. If set, this overrides the epochs argument, allowing training to automatically stop after the specified duration. Useful for time-constrained training scenarios.
patience	int	100	Number of epochs to wait without improvement in validation metrics before early stopping the training. Helps prevent overfitting by stopping training when performance plateaus.
batch	int or float	16	Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).
imgsz	int	640	Target image size for training. Images are resized to squares with sides equal to the specified value (if rect=False), preserving aspect ratio for YOLO models but not RTDETR. Affects model accuracy and computational complexity.
save	bool	True	Enables saving of training checkpoints and final model weights. Useful for resuming training or model deployment.
save_period	int	-1	Frequency of saving model checkpoints, specified in epochs. A value of -1 disables this feature. Useful for saving interim models during long training sessions.
cache	bool	False	Enables caching of dataset images in memory (True/ram), on disk (disk), or disables it (False). Improves training speed by reducing disk I/O at the cost of increased memory usage.
device	int or str or list	None	Specifies the computational device(s) for training: a single GPU (device=0), multiple GPUs (device=[0,1]), CPU (device=cpu), MPS for Apple silicon (device=mps), or auto-selection of most idle GPU (device=-1) or multiple idle GPUs (device=[-1,-1])
workers	int	8	Number of worker threads for data loading (per RANK if Multi-GPU training). Influences the speed of data preprocessing and feeding into the model, especially useful in multi-GPU setups.
project	str	None	Name of the project directory where training outputs are saved. Allows for organized storage of different experiments.
name	str	None	Name of the training run. Used for creating a subdirectory within the project folder, where training logs and outputs are stored.
exist_ok	bool	False	If True, allows overwriting of an existing project/name directory. Useful for iterative experimentation without needing to manually clear previous outputs.
pretrained	bool or str	True	Determines whether to start training from a pretrained model. Can be a boolean value or a string path to a specific model from which to load weights. Enhances training efficiency and model performance.
optimizer	str	'auto'	Choice of optimizer for training. Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for automatic selection based on model configuration. Affects convergence speed and stability.
seed	int	0	Sets the random seed for training, ensuring reproducibility of results across runs with the same configurations.
deterministic	bool	True	Forces deterministic algorithm use, ensuring reproducibility but may affect performance and speed due to the restriction on non-deterministic algorithms.
single_cls	bool	False	Treats all classes in multi-class datasets as a single class during training. Useful for binary classification tasks or when focusing on object presence rather than classification.
classes	list[int]	None	Specifies a list of class IDs to train on. Useful for filtering out and focusing only on certain classes during training.
rect	bool	False	Enables minimum padding strategy—images in a batch are minimally padded to reach a common size, with the longest side equal to imgsz. Can improve efficiency and speed but may affect model accuracy.
multi_scale	bool	False	Enables multi-scale training by increasing/decreasing imgsz by up to a factor of 0.5 during training. Trains the model to be more accurate with multiple imgsz during inference.
cos_lr	bool	False	Utilizes a cosine learning rate scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.
close_mosaic	int	10	Disables mosaic data augmentation in the last N epochs to stabilize training before completion. Setting to 0 disables this feature.
resume	bool	False	Resumes training from the last saved checkpoint. Automatically loads model weights, optimizer state, and epoch count, continuing training seamlessly.
amp	bool	True	Enables Automatic Mixed Precision (AMP) training, reducing memory usage and possibly speeding up training with minimal impact on accuracy.
fraction	float	1.0	Specifies the fraction of the dataset to use for training. Allows for training on a subset of the full dataset, useful for experiments or when resources are limited.
profile	bool	False	Enables profiling of ONNX and TensorRT speeds during training, useful for optimizing model deployment.
freeze	int or list	None	Freezes the first N layers of the model or specified layers by index, reducing the number of trainable parameters. Useful for fine-tuning or transfer learning.
lr0	float	0.01	Initial learning rate (i.e. SGD=1E-2, Adam=1E-3). Adjusting this value is crucial for the optimization process, influencing how rapidly model weights are updated.
lrf	float	0.01	Final learning rate as a fraction of the initial rate = (lr0 * lrf), used in conjunction with schedulers to adjust the learning rate over time.
momentum	float	0.937	Momentum factor for SGD or beta1 for Adam optimizers, influencing the incorporation of past gradients in the current update.
weight_decay	float	0.0005	L2 regularization term, penalizing large weights to prevent overfitting.
warmup_epochs	float	3.0	Number of epochs for learning rate warmup, gradually increasing the learning rate from a low value to the initial learning rate to stabilize training early on.
warmup_momentum	float	0.8	Initial momentum for warmup phase, gradually adjusting to the set momentum over the warmup period.
warmup_bias_lr	float	0.1	Learning rate for bias parameters during the warmup phase, helping stabilize model training in the initial epochs.
box	float	7.5	Weight of the box loss component in the loss function, influencing how much emphasis is placed on accurately predicting bounding box coordinates.
cls	float	0.5	Weight of the classification loss in the total loss function, affecting the importance of correct class prediction relative to other components.
dfl	float	1.5	Weight of the distribution focal loss, used in certain YOLO versions for fine-grained classification.
pose	float	12.0	Weight of the pose loss in models trained for pose estimation, influencing the emphasis on accurately predicting pose keypoints.
kobj	float	2.0	Weight of the keypoint objectness loss in pose estimation models, balancing detection confidence with pose accuracy.
nbs	int	64	Nominal batch size for normalization of loss.
overlap_mask	bool	True	Determines whether object masks should be merged into a single mask for training, or kept separate for each object. In case of overlap, the smaller mask is overlaid on top of the larger mask during merge.
mask_ratio	int	4	Downsample ratio for segmentation masks, affecting the resolution of masks used during training.
dropout	float	0.0	Dropout rate for regularization in classification tasks, preventing overfitting by randomly omitting units during training.
val	bool	True	Enables validation during training, allowing for periodic evaluation of model performance on a separate dataset.
plots	bool	False	Generates and saves plots of training and validation metrics, as well as prediction examples, providing visual insights into model performance and learning progression.
compile	bool or str	False	Enables PyTorch 2.x torch.compile graph compilation with backend='inductor'. Accepts True → "default", False → disables, or a string mode such as "default", "reduce-overhead", "max-autotune-no-cudagraphs". Falls back to eager with a warning if unsupported.
Note on Batch-size Settings

The batch argument can be configured in three ways:

Fixed Batch Size: Set an integer value (e.g., batch=16), specifying the number of images per batch directly.
Auto Mode (60% GPU Memory): Use batch=-1 to automatically adjust batch size for approximately 60% CUDA memory utilization.
Auto Mode with Utilization Fraction: Set a fraction value (e.g., batch=0.70) to adjust batch size based on the specified fraction of GPU memory usage.
Augmentation Settings and Hyperparameters
Augmentation techniques are essential for improving the robustness and performance of YOLO models by introducing variability into the training data, helping the model generalize better to unseen data. The following table outlines the purpose and effect of each augmentation argument:

Argument	Type	Default	Supported Tasks	Range	Description
hsv_h	float	0.015	detect, segment, pose, obb, classify	0.0 - 1.0	Adjusts the hue of the image by a fraction of the color wheel, introducing color variability. Helps the model generalize across different lighting conditions.
hsv_s	float	0.7	detect, segment, pose, obb, classify	0.0 - 1.0	Alters the saturation of the image by a fraction, affecting the intensity of colors. Useful for simulating different environmental conditions.
hsv_v	float	0.4	detect, segment, pose, obb, classify	0.0 - 1.0	Modifies the value (brightness) of the image by a fraction, helping the model to perform well under various lighting conditions.
degrees	float	0.0	detect, segment, pose, obb	0.0 - 180	Rotates the image randomly within the specified degree range, improving the model's ability to recognize objects at various orientations.
translate	float	0.1	detect, segment, pose, obb	0.0 - 1.0	Translates the image horizontally and vertically by a fraction of the image size, aiding in learning to detect partially visible objects.
scale	float	0.5	detect, segment, pose, obb, classify	>=0.0	Scales the image by a gain factor, simulating objects at different distances from the camera.
shear	float	0.0	detect, segment, pose, obb	-180 - +180	Shears the image by a specified degree, mimicking the effect of objects being viewed from different angles.
perspective	float	0.0	detect, segment, pose, obb	0.0 - 0.001	Applies a random perspective transformation to the image, enhancing the model's ability to understand objects in 3D space.
flipud	float	0.0	detect, segment, pose, obb, classify	0.0 - 1.0	Flips the image upside down with the specified probability, increasing the data variability without affecting the object's characteristics.
fliplr	float	0.5	detect, segment, pose, obb, classify	0.0 - 1.0	Flips the image left to right with the specified probability, useful for learning symmetrical objects and increasing dataset diversity.
bgr	float	0.0	detect, segment, pose, obb	0.0 - 1.0	Flips the image channels from RGB to BGR with the specified probability, useful for increasing robustness to incorrect channel ordering.
mosaic	float	1.0	detect, segment, pose, obb	0.0 - 1.0	Combines four training images into one, simulating different scene compositions and object interactions. Highly effective for complex scene understanding.
mixup	float	0.0	detect, segment, pose, obb	0.0 - 1.0	Blends two images and their labels, creating a composite image. Enhances the model's ability to generalize by introducing label noise and visual variability.
cutmix	float	0.0	detect, segment, pose, obb	0.0 - 1.0	Combines portions of two images, creating a partial blend while maintaining distinct regions. Enhances model robustness by creating occlusion scenarios.
copy_paste	float	0.0	segment	0.0 - 1.0	Copies and pastes objects across images to increase object instances.
copy_paste_mode	str	flip	segment	-	Specifies the copy-paste strategy to use. Options include 'flip' and 'mixup'.
auto_augment	str	randaugment	classify	-	Applies a predefined augmentation policy ('randaugment', 'autoaugment', or 'augmix') to enhance model performance through visual diversity.
erasing	float	0.4	classify	0.0 - 0.9	Randomly erases regions of the image during training to encourage the model to focus on less obvious features.
These settings can be adjusted to meet the specific requirements of the dataset and task at hand. Experimenting with different values can help find the optimal augmentation strategy that leads to the best model performance.

Info

For more information about training augmentation operations, see the reference section.

Logging
In training a YOLO11 model, you might find it valuable to keep track of the model's performance over time. This is where logging comes into play. Ultralytics YOLO provides support for three types of loggers - Comet, ClearML, and TensorBoard.

To use a logger, select it from the dropdown menu in the code snippet above and run it. The chosen logger will be installed and initialized.

Comet
Comet is a platform that allows data scientists and developers to track, compare, explain and optimize experiments and models. It provides functionalities such as real-time metrics, code diffs, and hyperparameters tracking.

To use Comet:

Example


Python

# pip install comet_ml
import comet_ml

comet_ml.init()

Remember to sign in to your Comet account on their website and get your API key. You will need to add this to your environment variables or your script to log your experiments.

ClearML
ClearML is an open-source platform that automates tracking of experiments and helps with efficient sharing of resources. It is designed to help teams manage, execute, and reproduce their ML work more efficiently.

To use ClearML:

Example


Python

# pip install clearml
import clearml

clearml.browser_login()

After running this script, you will need to sign in to your ClearML account on the browser and authenticate your session.

TensorBoard
TensorBoard is a visualization toolkit for TensorFlow. It allows you to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.

To use TensorBoard in Google Colab:

Example


CLI

load_ext tensorboard
tensorboard --logdir ultralytics/runs # replace with 'runs' directory

To use TensorBoard locally run the below command and view results at http://localhost:6006/.

Example


CLI

tensorboard --logdir ultralytics/runs # replace with 'runs' directory

This will load TensorBoard and direct it to the directory where your training logs are saved.

After setting up your logger, you can then proceed with your model training. All training metrics will be automatically logged in your chosen platform, and you can access these logs to monitor your model's performance over time, compare different models, and identify areas for improvement.

FAQ
How do I train an object detection model using Ultralytics YOLO11?
To train an object detection model using Ultralytics YOLO11, you can either use the Python API or the CLI. Below is an example for both:

Single-GPU and CPU Training Example


Python
CLI

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load a pretrained model (recommended for training)

# Train the model
results = model.train(data="coco8.yaml", epochs=100, imgsz=640)

For more details, refer to the Train Settings section.

What are the key features of Ultralytics YOLO11's Train mode?
The key features of Ultralytics YOLO11's Train mode include:

Automatic Dataset Download: Automatically downloads standard datasets like COCO, VOC, and ImageNet.
Multi-GPU Support: Scale training across multiple GPUs for faster processing.
Hyperparameter Configuration: Customize hyperparameters through YAML files or CLI arguments.
Visualization and Monitoring: Real-time tracking of training metrics for better insights.
These features make training efficient and customizable to your needs. For more details, see the Key Features of Train Mode section.

How do I resume training from an interrupted session in Ultralytics YOLO11?
To resume training from an interrupted session, set the resume argument to True and specify the path to the last saved checkpoint.

Resume Training Example


Python
CLI

from ultralytics import YOLO

# Load the partially trained model
model = YOLO("path/to/last.pt")

# Resume training
results = model.train(resume=True)

Check the section on Resuming Interrupted Trainings for more information.

Can I train YOLO11 models on Apple silicon chips?
Yes, Ultralytics YOLO11 supports training on Apple silicon chips utilizing the Metal Performance Shaders (MPS) framework. Specify 'mps' as your training device.

MPS Training Example


Python
CLI

from ultralytics import YOLO

# Load a pretrained model
model = YOLO("yolo11n.pt")

# Train the model on Apple silicon chip (M1/M2/M3/M4)
results = model.train(data="coco8.yaml", epochs=100, imgsz=640, device="mps")

For more details, refer to the Apple Silicon MPS Training section.

What are the common training settings, and how do I configure them?
Ultralytics YOLO11 allows you to configure a variety of training settings such as batch size, learning rate, epochs, and more through arguments. Here's a brief overview:

Argument	Default	Description
model	None	Path to the model file for training.
data	None	Path to the dataset configuration file (e.g., coco8.yaml).
epochs	100	Total number of training epochs.
batch	16	Batch size, adjustable as integer or auto mode.
imgsz	640	Target image size for training.
device	None	Computational device(s) for training like cpu, 0, 0,1, or mps.
save	True	Enables saving of training checkpoints and final model weights.
For an in-depth guide on training settings, check the Train Settings section.



📅
Created 1 year ago
✏️
Updated 4 months ago
glenn-jocher
Laughing-q
UltralyticsAssistant
MatthewNoyce
Y-T-G
JairajJangle
jk4e
RizwanMunawar
dependabot
fcakyon
Burhan-Q

Tweet

Share

Comments
 Back to top
Previous
Ultralytics YOLO11 Modes
Next
Val
© 2025 Ultralytics Inc. All rights reserved.
Made with Material for MkDocs

Ask AI
Chat avatar

Search
Write
Sign up

Sign in



TDS Archive
TDS Archive
An archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.

Follow publication

The Comprehensive Guide to Training and Running YOLOv8 Models on Custom Datasets
It’s now easier than ever to train your own computer vision models on custom datasets using Python, the command line, or Google Colab.
Oliver Ma
Oliver Ma

Follow
15 min read
·
Oct 2, 2024
140


4



Press enter or click to view image in full size

Image created by author using ChatGPT Auto.
Ultralytics’ cutting-edge YOLOv8 model is one of the best ways to tackle computer vision while minimizing hassle. It is the 8th and latest iteration of the YOLO (You Only Look Once) series of models from Ultralytics, and like the other iterations uses a convolutional neural network (CNN) to predict object classes and their bounding boxes. The YOLO series of object detectors has become well known for being accurate and quick, and provides a platform built on top of PyTorch that simplifies much of the process of creating models from scratch.

Importantly, YOLOv8 is also a very flexible model. That is, it can be trained on a variety of platforms, using any dataset of your choice, and the prediction model can be ran from many sources. This guide will act as a comprehensive tutorial covering the many different ways to train and run YOLOv8 models, as well as the strengths and limitations of each method that will be most relevant in helping you choose the most appropriate procedure depending on your hardware and dataset.

Note: all images that were used in the creation of this example dataset were taken by the author.

Environment
To get started with training our YOLOv8 model, the first step is to decide what kind of environment we want to train our model in (keep in mind that training and running the model are separate tasks).

The environments that are available for us to choose can largely be broken down into two categories: local-based and cloud-based.

With local-based training, we are essentially running the process of training directly on our system, using the physical hardware of the device. Within local-based training, YOLOv8 provides us with two options: the Python API and the CLI. There is no real difference in the results or speed of these two options, because the same process is being run under the hood; the only difference is in how the training is setup and run.

On the other hand, cloud-based training allows you to take advantage of the hardware of cloud servers. By using the Internet, you can connect to cloud runtimes and execute code just as you would on your local machine, except now it runs on the cloud hardware.

By far, the most popular cloud platform for machine learning has been Google Colab. It uses a Jupyter notebook format, which allows users to create “cells” in which code snippets can be written and run, and offers robust integrations with Google Drive and Github.

Which environment you decide to use will largely depend on the hardware that is available to you. If you have a powerful system with a high-end NVIDIA GPU, local-based training will likely work well for you. If your local machine’s hardware isn’t up to spec for machine learning, or if you just want more computation power than you have locally, Google Colab may be the way to go.

One of the greatest benefits of Google Colab is that it offers some computing resources for free, but also has a simple upgrade path that allows you to leverage faster computing hardware. Even if you already have a powerful system, you could consider using Google Colab if the faster GPUs offered in their higher tier plans represent a significant performance improvement over your existing hardware. With the free plan, you are limited to the NVIDIA T4, which performs roughly equivalent to an RTX 2070. With higher tier plans, the L4 (about the performance of a 4090) and A100 (about the performance of 2 4090s) are available. Keep in mind when comparing GPUs that the amount of VRAM is the primary determinant of machine learning performance.

Dataset
In order to start training a model, you need lots of data to train it on. Object detection datasets normally consist of a collection of images of various objects, in addition to a “bounding box” around the object that indicates its location within the image.

Press enter or click to view image in full size

Example of a bounding box around a detected object. Image by author.
YOLOv8-compatible datasets have a specific structure. They are primarily divided into valid, train, and test folders, which are used for validation, training, and testing of the model respectively (the difference between validation and testing is that during validation, the results are used to tune the model to increase its accuracy, whereas during testing, the results are only used to provide a measure of the model’s real-world accuracy).

Within each of these folders the dataset is further divided into two folders: the images and labels folders. The content of these two folders are closely linked with each other.

The images folder, as its name suggests, contains all of the object images of the dataset. These images usually have a square aspect ratio, a low resolution, and a small file size.

The labels folder contains the data of the bounding box’s position and size within each image as well as the type (or class) of object represented by each image. For example:

5 0.8762019230769231 0.09615384615384616 0.24519230769230768 0.18990384615384615
11 0.8846153846153846 0.2800480769230769 0.057692307692307696 0.019230769230769232
11 0.796875 0.2668269230769231 0.04807692307692308 0.02403846153846154
17 0.5649038461538461 0.29927884615384615 0.07211538461538461 0.026442307692307692
8 0.48197115384615385 0.39663461538461536 0.06490384615384616 0.019230769230769232
11 0.47716346153846156 0.7884615384615384 0.07932692307692307 0.10576923076923077
11 0.3425480769230769 0.5745192307692307 0.11057692307692307 0.038461538461538464
6 0.43509615384615385 0.5216346153846154 0.019230769230769232 0.004807692307692308
17 0.4855769230769231 0.5264423076923077 0.019230769230769232 0.004807692307692308
2 0.26322115384615385 0.3713942307692308 0.02403846153846154 0.007211538461538462
Each line represents an individual object that is present in the image. Within each line, the first number represents the object’s class, the second and third numbers represent the x- and y-coordinates of the center of the bounding box, and the fourth and fifth numbers represent the width and height of the bounding box.

The data within the images and labels folders are linked together by file names. Every image in the images folder will have a corresponding file in the labels folder with the same file name, and vice versa. Within the dataset, there will always be matching pairs of files within the images and labels folders with the same file name, but with different file extensions; .jpg is used for the images whereas .txt is used for the labels. The data for the bounding box(es) for each object in a .jpg picture is contained in the corresponding .txt file.

Press enter or click to view image in full size

Typical file structure of a YOLOv8-compatible dataset. Source: Ultralytics YOLO Docs (https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format)
There are several ways to obtain a YOLOv8-compatible dataset to begin training a model. You can create your own dataset or use a pre-configured one from the Internet. For the purposes of this tutorial, we will use CVAT to create our own dataset and Kaggle to find a pre-configured one.

CVAT
CVAT (cvat.ai) is a annotation tool that lets you create your own datasets by manually adding labels to images and videos.

After creating an account and logging in, the process to start annotating is simple. Just create a project, give it a suitable name, and add the labels for as many types/classes of objects as you want.

Press enter or click to view image in full size

Creating a new project and label on cvat.ai. Video by author.
Create a new task and upload all the images you want to be part of your dataset. Click “Submit & Open”, and a new task should be created under the project, with one job.

Press enter or click to view image in full size

Creating a new task and job on cvat.ai. Video by author.
Opening this job will allow you to start the annotation process. Use the rectangle tool to create bounding boxes and labels for each of the images in your dataset.

Press enter or click to view image in full size

Using the rectangle tool on cvat.ai to create bounding boxes. Video by author.
After annotating all your images, go back to the task and select Actions → Export task dataset, and choose YOLOv8 Detection 1.0 as the Export format. After downloading the task dataset, you will find that it only contains the labels folder and not the images folder (unless you selected the “Save images” option while exporting). You will have to manually create the images folder and move your images there (you may want to first compress your images to a lower resolution e.g. 640x640). Remember to not change the file names as they must match the file names of the .txt files in the labels folder. You will also need to decide how to allocate the images between valid, train, and test (train is the most important out of these).

Press enter or click to view image in full size

Example dataset exported from cvat.ai. Image by author.
Your dataset is completed and ready to use!

Kaggle
Kaggle (kaggle.com) is one of the largest online data science communities and one of the best websites to explore datasets. You can try finding a dataset you need by simply searching their website, and unless you are looking for something very specific, chances are you will find it. However, many datasets on Kaggle are not in a YOLOv8-compatible format and/or are unrelated to computer vision, so you may want to include “YOLOv8” in your query to refine your search.

You can tell if a dataset is YOLOv8-compatible by the file structure in the dataset’s Data Explorer (on the right side of the page).


Example of a YOLOv8-compatible dataset on Kaggle. Image by author.
If the dataset is relatively small (a few MB) and/or you are training locally, you can download the dataset directly from Kaggle. However, if you are planning on training with a large dataset on Google Colab, it is better to retrieve the dataset from the notebook itself (more info below).

Training
The training process will differ depending on if you are training locally or on the cloud.

Local
Create a project folder for all the training files. For this tutorial we will call it yolov8-project. Move/copy the dataset to this folder.

Set up a Python virtual environment with required YOLOv8 dependencies:

python3 -m venv venv
source venv/bin/activate
pip3 install ultralytics
Create a file named config.yaml. This is where important dataset information for training will be specified:

path: /Users/oliverma/yolov8-project/dataset/ # absolute path to dataset
test: test/images # relative path to test images
train: train/images # relative path to training images
val: val/images # relative path to validation images

# classes
names:
  0: bottle
In path put the absolute file path to the dataset’s root directory. You can also use a relative file path, but that will depend on the relative location of config.yaml.

In test, train, and val, put the locations of the images for testing, training, and validation (if you only have train images, just use train/images for all 3).

Under names, specify the name of each class. This information can usually be found in the data.yaml file of any YOLOv8 dataset.

As previously mentioned, both the Python API or the CLI can be used for local training.

Python API

Create another file named main.py. This is where the actual training will begin:

from ultralytics import YOLO

model = YOLO("yolov8n.yaml")

model.train(data="config.yaml", epochs=100)
By initializing our model as YOLO("yolov8n.yaml") we are essentially creating a new model from scratch. We are using yolov8n because it is the fastest model, but you may also use other models depending on your use case.

Press enter or click to view image in full size

Performance metrics for YOLOv8 variants. Source: Ultralytics YOLO Docs (https://docs.ultralytics.com/models/yolov8/#performance-metrics)
Finally, we train the model and pass in the config file and the number of epochs, or rounds of training. A good baseline is 300 epochs, but you may want to tweak this number depending on the size of your dataset and the speed of your hardware.

There are a few more helpful settings that you may want to include:

imgsz: resizes all images to the specified amount. For example, imgsz=640 would resize all images to 640x640. This is useful if you created your own dataset and did not resize the images.
device: specifies which device to train on. By default, YOLOv8 tries to train on GPU and uses CPU training as a fallback, but if you are training on an M-series Mac, you will have to use device="mps" to train with Apple’s Metal Performance Shaders (MPS) backend for GPU acceleration.
For more information on all the training arguments, visit https://docs.ultralytics.com/modes/train/#train-settings.

Your project directory should now look similar to this:

Press enter or click to view image in full size

Example file structure of the project directory. Image by author.
We are finally ready to start training our model. Open a terminal in the project directory and run:

python3 main.py
The terminal will display information about the training progress for each epoch as the training progresses.

Press enter or click to view image in full size

Training progress for each epoch displayed in the terminal. Image by author.
The training results will be saved in runs/detect/train (or train2, train3, etc.). This includes the weights (with a .pt file extension), which will be important for running the model later, as well as results.png which shows many graphs containing relevant training statistics.

Press enter or click to view image in full size

Example graphs from results.png. Image by Author.
CLI

Get Oliver Ma’s stories in your inbox
Join Medium for free to get updates from this writer.

Enter your email
Subscribe
Open a new terminal in the project directory and run this command:

yolo detect train data=config.yaml model=yolov8n.yaml epochs=100
This command can be modified with the same arguments as listed above for the Python API. For example:

yolo detect train data=config.yaml model=yolov8n.yaml epochs=300 imgsz=640 device=mps
Training will begin, and progress will be displayed in the terminal. The rest of the training process is the same as with the Python CLI.

Google Colab
Go to https://colab.research.google.com/ and create a new notebook for training.

Before training, make sure you are connected to a GPU runtime by selecting Change runtime type in the upper-right corner. Training will be extremely slow on a CPU runtime.

Press enter or click to view image in full size

Changing the notebook runtime from CPU to T4 GPU. Video by author.
Before we can begin any training on Google Colab, we first need to import our dataset into the notebook. Intuitively, the simplest way would be to upload the dataset to Google Drive and import it from there into our notebook. However, it takes an exceedingly long amount of time to upload any dataset that is larger than a few MB. The workaround to this is to upload the dataset onto a remote file hosting service (like Amazon S3 or even Kaggle), and pull the dataset directly from there into our Colab notebook.

Import from Kaggle

Here are instructions on how to import a Kaggle dataset directly into a Colab notebook:

In Kaggle account settings, scroll down to API and select Create New Token. This will download a file named kaggle.json.

Run the following in a notebook cell:

!pip install kaggle
from google.colab import files
files.upload()
Upload the kaggle.json file that was just downloaded, then run the following:

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d [DATASET] # replace [DATASET] with the desired dataset ref
The dataset will download as a zip archive. Use the unzip command to extract the contents:

!unzip dataset.zip -d dataset
Start Training

Create a new config.yaml file in the notebook’s file explorer and configure it as previously described. The default working directory in a Colab notebook is /content/, so the absolute path to the dataset will be /content/[dataset folder]. For example:

path: /content/dataset/ # absolute path to dataset
test: test/images # relative path to test images
train: train/images # relative path to training images
val: val/images # relative path to validation images

# classes
names:
  0: bottle
Make sure to check the file structure of your dataset to make sure the paths specified in config.yaml are accurate. Sometimes datasets will be nestled within multiple levels of folders.

Run the following as cells:

!pip install ultralytics
import os

from ultralytics import YOLOmodel = YOLO("yolov8n.yaml")

results = model.train(data="config.yaml", epochs=100)
The previously mentioned arguments used to modify local training settings also apply here.

Similar to local training, results, weights, and graphs will be saved in runs/detect/train.

Running
Regardless of whether you trained locally or on the cloud, predictions must be run locally.

After a model has completed training, there will be two weights located in runs/detect/train/weights, named best.pt and last.pt, which are the weights for the best epoch and the latest epoch, respectively. For this tutorial, we will use best.pt to run the model.

If you trained locally, move best.pt to a convenient location (e.g. our project folder yolov8-project) for running predictions. If you trained on the cloud, download best.pt to your device. On Google Colab, right-click on the file in the notebook’s file explorer and select Download.

Press enter or click to view image in full size

Downloading weights on Google Colab. Video by author.
Similar to local training, predictions can be run either through the Python API or the CLI.

Python API
In the same location as best.pt, create a new file named predict.py:

from ultralytics import YOLO

model = YOLO("best.pt")

results = model(source=0, show=True, conf=0.25, save=True)
Similar to training, there are many useful arguments that will modify the prediction settings:

source: controls the input source for the predictions. source=0 sets the webcam as the input source. More info below.
show: if True , displays the predictions, bounding boxes, and confidences on-screen.
conf: the minimum confidence score threshold for a prediction to be considered.
save: if True , saves prediction results to runs/detect/predict (or predict2, predict3, etc.).
device: as previously stated, use device="mps" on an M-series Mac.
For the full list of prediction arguments, visit https://docs.ultralytics.com/modes/predict/#inference-arguments.

CLI
Run the following command to start the model:

python3 predict.py
Press enter or click to view image in full size

Running YOLOv8 model predictions through live webcam feed. Video by author.
CLI
yolo detect predict model=best.pt source=0 show=True conf=0.25 save=True
The arguments are the same as with the Python API.

Implementation
We have now been able to successfully run our model on a live webcam feed, but so what? How can we actually use this model and integrate it into a project?

Let’s think about it in terms of input and output. In order for this model to be of any use for us in an external application, it must be able to accept useful inputs and produce useful outputs. Thankfully, the flexibility of the YOLOv8 model makes it possible to integrate a model into a variety of use cases.

We used source=0 to set the webcam as the input source for our predictions. However, YOLOv8 models can utilize many more input sources than just this. Below are several examples:

results = model(source="path/to/image.jpg", show=True, conf=0.25, save=True) # static image
results = model(source="screen", show=True, conf=0.25, save=True) # screenshot of current screen
results = model(source="https://ultralytics.com/images/bus.jpg", show=True, conf=0.25, save=True) # image or video URL
results = model(source="path/to/file.csv", show=True, conf=0.25, save=True) # CSV file
results = model(source="path/to/video.mp4", show=True, conf=0.25, save=True) # video file
results = model(source="path/to/dir", show=True, conf=0.25, save=True) # all images and videos within directory
results = model(source="path/to/dir/**/*.jpg", show=True, conf=0.25, save=True) # glob expression
results = model(source="https://www.youtube.com/watch?v=dQw4w9WgXcQ", show=True, conf=0.25, save=True) # YouTube video URL
For the full list of prediction sources and input options, visit https://docs.ultralytics.com/modes/predict/#inference-sources.

Whenever we run a prediction, YOLOv8 returns huge amounts of valuable data in the form of a list of Results objects, which includes information about the bounding boxes, segmentation masks, keypoints, class probabilities, and oriented bounding boxes (OBBs) of a prediction.

Since we assigned the results of the prediction to the results variable in our code, we can use it to retrieve information about the prediction:

from ultralytics import YOLO

model = YOLO("best.pt")

results = model(source="bottles.jpg", show=True, conf=0.25, save=True)

print("Bounding boxes of all detected objects in xyxy format:")
for r in results:
  print(r.boxes.xyxy)

print("Confidence values of all detected objects:")
for r in results:
  print(r.boxes.conf)

print("Class values of all detected objects:")
for r in results:
  print(r.boxes.cls)
There are far too many types of output results to include in this tutorial, but you can learn more by visiting https://docs.ultralytics.com/modes/predict/#working-with-results.

This was only a very basic example of what you can do with the outputs of a YOLOv8 model, and there are countless ways you could potentially apply a model to a project of your own.

Conclusion
Congratulations for making it all the way to the end!

In this article, we were able to start from scratch and make our own YOLOv8-compatible dataset, import datasets from Kaggle, train a model using multiple environments including Python API, CLI, and Google Colab, run our model locally, and discover many input/output methods that enable us to leverage YOLOv8 models in our own projects.

Please keep in mind that the objective of this tutorial is to act as a starting point or introduction to YOLOv8 or computer vision. We have barely scratched the surface of the intricacies of the YOLOv8 model, and as you become more experienced with YOLOv8 and computer vision in general, it is definitely wise to take a deeper dive into the topic. There are plenty of articles on the Internet and here on Medium that work great for this very purpose.

That being said, if you have followed along with this tutorial and made it to the end, that is nevertheless a great accomplishment. I hope that this article has helped you to gain a basic understanding of machine learning, computer vision, and the YOLOv8 model. Perhaps you have even found a passion for the subject, and will continue to learn more as you progress to more advanced topics in the future.

Thanks for reading, and have a great day!

Yolov8
Computer Vision
Machine Learning
Artificial Intelligence
Data Science
140


4


TDS Archive
Published in TDS Archive
829K followers
·
Last published Feb 3, 2025
An archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.


Follow
Oliver Ma
Written by Oliver Ma
24 followers
·
2 following
Student, full-stack webdev, and competitive programmer. Interned at Verizon, UC COSMOS participant, Programmer for VEX 1669X. Interested in machine learning.


Follow
Responses (4)

Write a response

What are your thoughts?

Cancel
Respond
Vipin
Vipin

Oct 2, 2024


Great
54

Reply

Anatole Martins
Anatole Martins

Oct 5, 2024


A thought-provoking exploration of how AI shapes our societal landscape. Excited to see where this leads!
2

Reply

Corpainen
Corpainen

Oct 2


Great article, got tips on how yolov8 runs. Currently training on epoch 36/50, trying out how well we can recognize cars on a live feed.
Reply

See all responses
More from Oliver Ma and TDS Archive
Understanding LLMs from Scratch Using Middle School Math
TDS Archive
In

TDS Archive

by

Rohit Patel

Understanding LLMs from Scratch Using Middle School Math
In this article, we talk about how LLMs work, from scratch — assuming only that you know how to add and multiply two numbers. The article…
Oct 19, 2024
8K
99
10 Common Software Architectural Patterns in a nutshell
TDS Archive
In

TDS Archive

by

Vijini Mallawaarachchi

10 Common Software Architectural Patterns in a nutshell
Ever wondered how large enterprise scale systems are designed? Before major software development starts, we have to choose a suitable…

Sep 4, 2017
42K
148
How to Implement Graph RAG Using Knowledge Graphs and Vector Databases
TDS Archive
In

TDS Archive

by

Steve Hedden

How to Implement Graph RAG Using Knowledge Graphs and Vector Databases
A Step-by-Step Tutorial on Implementing Retrieval-Augmented Generation (RAG), Semantic Search, and Recommendations
Sep 6, 2024
2K
19
Recommender Systems — A Complete Guide to Machine Learning Models
TDS Archive
In

TDS Archive

by

Francesco Casalegno

Recommender Systems — A Complete Guide to Machine Learning Models
Leveraging data to help users discovering new contents
Nov 24, 2022
539
5
See all from Oliver Ma
See all from TDS Archive
Recommended from Medium
Image Processing: Resize, Crop, Filtering, Edge Detection In Computer Vision
Python in Plain English
In

Python in Plain English

by

Muhammad Syaoki Faradisa

Image Processing: Resize, Crop, Filtering, Edge Detection In Computer Vision
Computer Vision is a fascinating field that allows computers to “see” and interpret the world around them. At its core, Computer Vision…

Aug 22
5
How I Built a Custom YOLO ML Backend for Instant Pre-Labeling in Label Studio
Ansari Ehteesham Aqeel
Ansari Ehteesham Aqeel

How I Built a Custom YOLO ML Backend for Instant Pre-Labeling in Label Studio
Annotate Instantly: Unleash YOLOv8 Pre-Labeling Magic
May 15
1
Introducing Next-Gen OCR with Deepseek-OCR
Writing in the World of Artificial Intelligence
In

Writing in the World of Artificial Intelligence

by

Abish Pius

Introducing Next-Gen OCR with Deepseek-OCR
Optical Character Recognition (OCR) has come a long way — from reading scanned receipts to powering AI document agents. But most OCR tools…

2d ago
2
Day 32: Image Classification with ResNet and Your Own Dataset
Shabana Ashraphi
Shabana Ashraphi

Day 32: Image Classification with ResNet and Your Own Dataset
📌 Missed the last one? Day 31: Transfer Learning — Use Pretrained Models to Save Time and Compute

Jun 13
50
1
Weekly AI Newsletter : What You Need to Know Before Monday (Oct 20, 2025)
Towards AI
In

Towards AI

by

AIversity

Weekly AI Newsletter : What You Need to Know Before Monday (Oct 20, 2025)
Discover the top AI news, product launches, and expert insights shaping the world of artificial intelligence this week — ideal for…

5d ago
Top Python Libraries Every Computer Vision Engineer Should Know in 2025
FAUN.dev() 🐾
In

FAUN.dev() 🐾

by

Sundar Balamurugan

Top Python Libraries Every Computer Vision Engineer Should Know in 2025
Computer vision is evolving fast, and staying ahead means mastering the right tools. In 2025, these Python libraries dominate the computer…

May 4
7
See more recommendations
Help

Status

About

Careers

Press

Blog

Privacy

Rules

Terms

Text to speech
Roboflow logo
Pricing
Docs
Blog
Sign In
Book a demo
Get Started
Blog
How to Train YOLOv8 Object Detection on a Custom Dataset
How to Train YOLOv8 Object Detection on a Custom Dataset
Piotr Skalski
Published Jan 10, 2023 • 9 min read
💡
We open-sourced all the Python code used in this tutorial. You can find it in our Roboflow Notebooks repository. Open up the notebook and follow along.

YOLOv8 is the latest installment in the highly influential family of models that use the YOLO (You Only Look Once) architecture. YOLOv8 was developed by Ultralytics, a team known for its work on YOLOv3 and YOLOv5.

Following the trend set by  YOLOv6 and YOLOv7, we have at our disposal object detection, but also instance segmentation, and image classification. The model itself is created in PyTorch and runs on both the CPU and GPU. As with YOLOv5, we also have a number of various exports such as TF.js or CoreML.

For a deep dive into the YOLOv8 architecture, see our What's New in YOLOv8 post.

Play
We are still waiting for the Papers with Code benchmark comparing YOLOv8 with the other SOTA real-time models. In the meantime, we matched v8 against YOLOv5 using the RF100 dataset. YOLOv8 scores higher 64% of the time, and when it performs worse, the difference is negligible.

In this tutorial, we will take you through each step of training the YOLOv8 object detection model on a custom dataset. You will learn how to use the new API, how to prepare the dataset, and most importantly how to train and validate the model.

The steps to train a YOLOv8 object detection model on custom data are:

Install YOLOv8 from pip
Create a custom dataset with labelled images
Export your dataset for use with YOLOv8
Use the yolo command line utility to run train a model
Run inference with the YOLO command line application
You can try a YOLOv8 model with the following Workflow:


In the Workflow above, you can drag and drop an image and the system will detect the objects in it using a YOLOv8 model. You can also see how the model compares to YOLO-World, a zero-shot model.

Let's begin!

💡
You can train YOLOv8 models in a few lines of code and without labeling data using Autodistill, an open-source ecosystem for distilling large foundation models into smaller models trained on your data.

Check out our Autodistill guide for more information, and our Autodistill YOLOv8 documentation.

How to Install YOLOv8
YOLOv8 can be installed in two ways :  from the source and via pip. This is because it is the first iteration of YOLO to have an official package.

From pip (recommended)
To install YOLOv8 from pip, use the following command:

pip install "ultralytics<=8.3.40"
From source
You can install the model from the source on GitHub using these commands:

git clone https://github.com/ultralytics/ultralytics
cd ultralytics
pip install -e ultralytics
The New YOLOv8 API
The developers of YOLOv8 decided to break away from the standard YOLO project design : separate train.py, detect.py, val.py, and export.py scripts. In the short term it will probably cause some confusion while in the long term, it is a fantastic decision!

This pattern has been around since YOLOv3, and every YOLO iteration has replicated it. It was relatively simple to understand but notoriously challenging to deploy especially in real-time processing and tracking scenarios.

The new approach is much more flexible because it allows YOLOv8 to be used independently through the terminal, as well as being part of a complex computer vision application.

The YOLOv8 CLI
YOLOv8 comes with a command line interface that lets you train, validate or infer models on various tasks and versions. The CLI requires no customization or code. You can run all tasks from the terminal. Usage is fairly similar to the scripts we are familiar with. The example below shows how to leverage the CLI to detect objects in a given image.

yolo task=detect \
mode=predict \
model=yolov8n.pt \
conf=0.25 \
source='https://media.roboflow.com/notebooks/examples/dog.jpeg'

Expected inference result
The YOLOv8 Python SDK
Ultralytics YOLO comes with a pythonic Model and Trainer interface. This finally allows us to use the YOLO model inside a custom Python script in only a few lines of code. Something like this has been impossible until now without doing a repository fork and making your own changes to the code.

from ultralytics import YOLO

model = YOLO('yolov8n.pt')
model.predict(
   source='https://media.roboflow.com/notebooks/examples/dog.jpeg',
   conf=0.25
)
Preparing a custom dataset for YOLOv8
Building a custom dataset can be a painful process. It might take dozens or even hundreds of hours to collect images, label them, and export them in the proper format. Fortunately, Roboflow makes this process straightforward. Let me show you how!

Create a project
Before you start, you need to create a Roboflow account. Once you do that, you can create a new project in the Roboflow dashboard. Keep in mind to choose the right project type. In this case choose, "Object Detection".


Upload your images
Next, add data to your newly created project. You can do it via API or through our web interface. If you don’t have a dataset, you can grab one from Roboflow Universe or use the football-players-detection dataset which we will be showing later in this tutorial.

If you drag and drop a directory with a dataset in a supported format, the Roboflow dashboard will automatically read the images and annotations together.


Label your images
If you only have images, you can label them in Roboflow Annotate. When starting from scratch, consider annotating large batches of images via API or use the model-assisted labeling tool to speed things up.


💡
Roboflow also offers Auto Label, an automated labeling solution. With Auto Label, you can use foundation models like Grounding DINO and Segment Anything to automatically label images in your dataset. Refer to our Auto Label launch post for more information about how Auto Label works, and how you can use it with your project.
Generate a new version of your dataset
Now that we have our images and annotations added, we can Generate a Dataset Version. When Generating a Version, you may elect to add preprocessing and augmentations. This step is entirely optional, however, it can allow you to improve the robustness of your model significantly.


Export your dataset
Once the dataset version is generated, we have a hosted dataset we can load directly into our notebook for easy training.

One way to download a dataset from Roboflow Universe is to use our pip package. You can generate the appropriate code snippet directly in our UI. On a dataset’s Universe home page, click the Download this Dataset button and select YOLO v5 PyTorch export format.

After a few seconds, you will see a code similar to the one below, except with all the necessary parameters filled in. You can copy and paste it into your Jupyter Notebook or a similar environment. When you execute it, the dataset will be downloaded to your machine in the appropriate format. Magic!

from roboflow import Roboflow

rf = Roboflow(api_key='YOUR_API_KEY')
project = rf.workspace('WORKSPACE').project('PROJECT')
dataset = project.version(1).download('yolov8')

Train YOLOv8 on a custom dataset
After pasting the dataset download snippet into your YOLOv8 Colab notebook, you are ready to begin the training process. You can do so using this command:

yolo task=detect \
mode=train \
model=yolov8s.pt \
data={dataset.location}/data.yaml \
epochs=100 \
imgsz=640
Your model will begin training and run for several minutes, or hours, depending on how big the dataset is and which training options you chose.

Here are the results of training a player detection model with YOLOv8:


The confusion matrix returned after training

Key metrics tracked by YOLOv8

Example YOLOv8 inference on a validation batch
Validate with a new model
When the training is over, it is good practice to validate the new model on images it has not seen before. Therefore, when creating a dataset, we divide it into three parts, and one of them that we will use now as a test dataset.

yolo task=detect \
mode=val \
model={HOME}/runs/detect/train/weights/best.pt \
data={dataset.location}/data.yaml

YOLOv8 model evaluation results
Predict with a custom model
To predict data using a custom model, use the following command:

yolo task=detect \
mode=predict \
model={HOME}/runs/detect/train/weights/best.pt \
conf=0.25 \
source={dataset.location}/test/images

Example of YOLOv8 custom model inference results
Export and Upload Weights
Once you have finished training your YOLOv8 model, you’ll have a set of trained weights ready for use with a hosted API endpoint. These weights will be in the “/runs/detect/train/weights/best.pt” folder of your project. You can upload your model weights to Roboflow Deploy with the deploy() function in the Roboflow pip package to use your trained weights.

To upload model weights, add the following code to the “Inference with Custom Model” section in the notebook:

project.version(DATASET_VERSION).deploy(model_type=”yolov8”, model_path=f”{HOME}/runs/detect/train/”)
Replace the DATASET_VERSION value with the version number associated with your project. If you downloaded your weights from Roboflow using the code snippet provided earlier, the “DATASET_VERSION” number is stored in the “dataset.version” value.

When you run the code above, you’ll see a green checkmark appear next to the version for which you have uploaded weights. You will also see a label that says you uploaded your weights using the YOLOv8 weight upload feature:


The Roboflow dashboard showing that we uploaded our own YOLOv8 weights in the "Model Type" label.
Click on "Deploy" in the sidebar of the Roboflow dashboard to see the deployment options available to you. Directly in the browser, you can test your model and share the model with others by sending them the URL.

You'll be able to test and show model performance by uploading an image or video:


and by using your webcam, or by pasting in a YouTube link to a video on which you want to run inference.


Once you've uploaded the model weights, your custom trained YOLOv8 model can be built into production applications or shared externally for others to see and use. Explore pre-trained YOLOv8 models on Roboflow Universe.

Deploy Your Model to the Edge
In addition to using the Roboflow hosted API for deployment, you can use Roboflow Inference, an open source inference solution that has powered millions of API calls in production environments. Inference works with CPU and GPU, giving you immediate access to a range of devices, from the NVIDIA Jetson to TRT-compatible devices to ARM CPU devices.


With Roboflow Inference, you can self-host and deploy your model, on-device and at the edge.

You can deploy applications using the Inference Docker containers or the pip package. In this guide, we are going to use the Inference Docker deployment solution. First, install Docker on your device. Then, review the Inference documentation to find the Docker container for your device.

For this guide, we'll use the GPU Docker container:

docker pull roboflow/roboflow-inference-server-gpu
This command will download the Docker container and start the inference server. This server is available at http://localhost:9001. To run inference, we can use the following Python code:

import requests

workspace_id = ""
model_id = ""
image_url = ""
confidence = 0.75
api_key = ""

infer_payload = {
    "image": {
        "type": "url",
        "value": image_url,
    },
    "confidence": confidence,
    "iou_threshold": iou_thresh,
    "api_key": api_key,
}
res = requests.post(
    f"http://localhost:9001/{workspace_id}/{model_id}",
    json=infer_object_detection_payload,
)

predictions = res.json()
Above, set your Roboflow workspace ID, model ID, and API key.

Find your workspace and model ID
Find your API key
Also, set the URL of an image on which you want to run inference. This can be a local file.

To use your YOLOv8 model commercially with Inference, you will need a Roboflow Enterprise license, through which you gain a pass-through license for using YOLOv8. An enterprise license also grants you access to features like advanced device management, multi-model containers, auto-batch inference, and more.

To learn more about deploying commercial applications with Roboflow Inference, contact the Roboflow sales team.

Conclusion
YOLOv8 is the latest release in the family of YOLO models, defining a new state-of-the-art in object detection. When benchmarked on Roboflow 100, we saw a significant performance boost between v8 and v5.

The YOLOv8 software is designed to be as intuitive as possible for developers to use. With a new Ultralytics YOLOv8 pip package, using the model in your code has never been easier. There is also a new command line interface that makes training more intuitive, too.

Now you have all you need to start training YOLOv8 models. Happy building!

Cite this Post
Use the following entry to cite this post in your research:

Piotr Skalski. (Jan 10, 2023). How to Train YOLOv8 Object Detection on a Custom Dataset. Roboflow Blog: https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/

Stay Connected
Get the Latest in Computer Vision First
Enter email
Unsubscribe at any time. Review our Privacy Policy.
Written by
Piotr Skalski
ML Growth Engineer @ Roboflow | Owner @ github.com/SkalskiP/make-sense (2.4k stars) | Blogger @ skalskip.medium.com/ (4.5k followers)
View more posts
Topics
Roboflow Train
Model Training
Object Detection
YOLOv8
Roboflow Train
Model Training
Object Detection
YOLOv8
Table of Contents
How to Install YOLOv8
The New YOLOv8 API
The YOLOv8 CLI
Preparing a custom dataset for YOLOv8
Export and Upload Weights
Deploy Your Model to the Edge
Conclusion
Roboflow logo
Product
Universe
Annotate
Train
Workflows
Deploy
Pricing
Talk to Sales
Enterprise
Ecosystem
Notebooks
Autodistill
Supervision
Inference
Roboflow 100
Open Source
Hardware
Developers
Documentation
User Forum
Changelog
What is computer vision?
Weekly Product Webinar
Convert Annotation Formats
Computer Vision Models
Model Playground
Industries
Customer Stories
Aerospace & Defense
Automotive
Consumer Goods
Energy and Utilities
Healthcare & Medicine
Industrial Manufacturing
Logistics
Manufacturing
Media & Entertainment
Retail & Service
Transportation
Warehousing
Models
RF-DETR
YOLO11
YOLOv8
YOLOv5
Florence-2
SAM 2
Multimodal Models
Explore All Models
Company
About Us
Blog
Careers
Press
Contact
Service Status
Badge AWS QualifiedBadge AWS QualifiedG2 Badge logoBadge SOC-NonCPA2024 Prometeus Awards Winner Badge
Terms of Service
Enterprise Terms
Privacy Policy
Sitemap
© 2025 Roboflow, Inc. All rights reserved.
EJ Technology Consultants Logo
Home
Services 
Machine Learning
Computer Vision
System Design
Prototyping
Embedded Hardware
Documentation
Portfolio 
Automated Assembly Inspection
Mask Detection Camera
RAIN MAN 2.0
Conference Presentations
Learn
Contact
30 December 2024 
YOLO
Object Detection
Datasets
How to Train YOLO 11 Object Detection Models Locally with NVIDIA
This guide provides step-by-step instructions for training a custom YOLO 11 object detection model on a local PC using an NVIDIA GPU.

Evan Juras
Evan Juras
Computer Vision Engineer
How to Train YOLO 11 Object Detection Models Locally with NVIDIA
Ultralytics recently released YOLO11, a family of computer vision models that provides state-of-the-art performance in classification, object detection, and image segmentation. It utilizes an improved architecture that allows it to more accurately find object features and run faster than previous YOLO generations such as YOLOv8 and YOLOv5. Best of all, it is easy to train, convert, and deploy YOLO11 models in a variety of environments.


Figure 1. This article steps through the process of training a YOLO object detection model.

This guide provides step-by-step instructions for training (or fine-tuning) a custom YOLO11 object detection model on a local PC. It will go through the process of preparing data, installing Ultralytics, training a model, and running inference with a custom Python script. The guide shows how to run training on a local computer and graphics card (GPU). The guide is targeted for Windows PCs, but with some modifications, it can also be used for Linux or macOS systems. These instructions also work for training YOLOv8 and YOLOv5 models.

As an example, we’ll train a custom candy detection model that can locate and count the different types of candy in an image, video, or webcam feed. We’ll use images of popular candy (Skittles, Starburst, Snickers, etc.) to train the model.

NOTE: A CUDA-compatible NVIDIA graphics card is strongly recommended for training models. If you don’t have an NVIDA GPU, consider using our Google Colab notebook for training YOLO models instead, which allows you to run training on cloud-based GPUs. Most NVIDIA GeForce desktop and laptop cards will work for training models. A full list of CUDA-compatible graphics cards is available here.

Step 1 - Install Anaconda
First, we need to install Anaconda, which is a great tool for creating and managing Python environments. It allows you to install Python libraries without worrying about version conflicts with existing installations on your operating system.


Figure 2. Download the Anaconda Installer for your OS from the Anaconda download page.

Go to the Anaconda download page at https://anaconda.com/download, click the “skip registration” button, and then download the package for your OS. When it’s finished downloading, run the installer and click through the installation steps. You can use the default options for installation.

Once it’s finished installing, search for “Anaconda Prompt” in the Start Bar and run it to open an Anaconda command terminal. We’ll work inside this terminal to install Python libraries and run training.


Figure 3. The Anaconda Prompt terminal we'll be working in throughout the guide.

Step 2 - Create New Environment and Install Ultralytics
Now, let’s create a new Anaconda Python environment for the Ultralytics library. Work through the following steps Create a new environment by issuing:

conda create --name yolo11-env python=3.12 -y
When it’s finished creating the environment, activate it by issuing:

conda activate yolo11-env
When it’s active, you’ll see the name of the environment in parentheses at the left of the command line. Next, we’ll install Ultralytics, a Python library for training and running YOLO models. Install it by issuing the following command. It downloads a large number of Python packages, so it will take a while.

pip install ultralytics
When installing Ultralytics, the Python package manager also installs several other powerful libraries, like OpenCV, Numpy, and PyTorch. It installs the CPU version of PyTorch by default. To run training on the GPU, we need to use the GPU-enabled version of PyTorch. Run the following command to install the GPU version of PyTorch:

pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
NOTE: PyTorch regularly releases updated versions with support for newer versions of CUDA. The command above installs PyTorch with CUDA v12.4. To see the command for installing the latest version, go to https://pytorch.org/get-started/locally/.

This command automatically installs the necessary versions of CUDA, cuDNN, and PyTorch-GPU inside the Anaconda virtual environment. It’s a very handy way to install CUDA drivers and libraries without having to go through the pain of downloading and installing them manually. You can confirm PyTorch-GPU is correctly installed by issuing the following command in the Anaconda Prompt terminal:

python -c "import torch; print(torch.cuda.get_device_name(0))"
If CUDA was installed correctly, this command will return the name of the GPU installed in your system.

Step 3 - Gather and Label Images
Before we start training, we need to gather and label images that will be used for training the object detection model. This guide will give a brief overview of the process, but for more detailed walkthrough on how to gather and label effective training images, see our article on the topic.


Figure 4. Click this image to visit our article that gives tips on how to gather and label images.

A good starting point for a proof-of-concept model is 100 to 200 images. The training images should show the objects in a variety of backgrounds, perspectives, and lighting conditions that are similar to what the model will see in the field. It’s also helpful to include other random objects along with the desired objects to help your model learn what NOT to detect.

NOTE: If you just want to try training a model, you can use our premade Coin Detection Dataset for the rest of this guide. It has 750 labeled images for training a model that will detect pennies, nickels, dimes, and quarters. Download it at this link and then continue to Step 4.

Once the images are gathered, label them using a labeling tool like LabelImg or Label Studio. Draw a box around each object in each image, making sure to fit the object tightly inside the label box. For more detailed instructions, see our YouTube video on training YOLO models, which shows how to label images using Label Studio.


Figure 5. An example of a labeled image for the custom candy detection model

If you used Label Studio to label and export the images, they’ll be exported in a .zip file that contains the following:

An “images” folder containing the images
A “labels” folder containing the labels in YOLO annotation format
A “classes.txt” labelmap file the contains the list of classes
Extract the .zip folder to a folder named “my_dataset” or similar. An example of the folder is shown below. If you obtained your dataset from another source (like Roboflow Universe or Kaggle) or used another tool to label your dataset, make sure the files are organized in the same folder structure (see my Coin Detection Dataset for an example).


Figure 6. Organize your data in the folders shown here, where the images folder contains the training images, the labels folder contains annotation files, and the classes.txt file contains a list of classes.

In the next step, we’ll split these files into training and validation sets and organize them into the folder structure required for training YOLO models.

Step 4 - Set Up Folder Structure
Ultralytics requires a particular folder structure to store training data for models. The root folder is named “data”. Inside, there are two main folders:

train: Contains the images and labels that are fed to the model during the training process. The training algorithm adjusts the model’s weights to fit to the data in these images.
validation: Contains images and labels that are periodically used to test the model during training. At the end of each training epoch, the model is run on these images to determine metrics like precision, recall, and mAP.
Typically, 80% of the images in a dataset are split into the “train” folder, while 20% are split into the “validation” folder. In each of these folders is a “images” folder and a “labels” folder, which hold the image files and annotation files respectively.

The overall folder structure required for training Ultralytics YOLO models is shown below. In this section of the article, we’ll set up this folder structure using an automated Python script.


Figure 7. The Ultralytics library requires datasets to be in the folder structure shown here when training custom models.

4.1 Create root folder
We need to recreate the folder structure shown in Figure 7 before we train our model. Let’s start by making a folder to work from, where we’ll store both the data and the trained models. Create a folder named “yolo” in your Documents folder and move into it by issuing the following commands. Note, %USERPROFILE% points to your User folder in Windows (e.g., C:\Users\Evan):

mkdir %USERPROFILE%\Documents\yolo
cd %USERPROFILE%\Documents\yolo
Now you are working inside the C:\Users\username\Documents\yolo directory. We’ll use this directory as the working folder for the rest of the guide. Next, create the data folder by issuing:

mkdir data
You can manually move all the images and label files into the folder structure described above, or you can run our automated Python script to do it for you. The commands below show how to run the script, which will set up the folder structure and randomly split your images between training and validation.

4.2 Split dataset into train and val folders using automated Python script
First, download the script by issuing:

curl --output train_val_split.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py
Next, find the path to your folder from Step 3 containing the images and label files. For example, if the files are in a folder named “my_dataset” in your Documents folder, the path would be “C:\Users<username>\Documents\my_dataset”.

The “train_val_test.py” script has two arguments, “datapath” and “train_pct”:

datapath : absolute path to the folder containing all the images and label files with double backslashes (example: “C:\\Users\\Evan\\Documents\\my_dataset”)
train_pct : specifies the percentage for the train split (the remaining portion goes to validation). Typically, an 80%/20% train/val split is used (example: .8)
Run the script by issuing the following command:

python yolo_train_val_split.py --datapath="C:\\path\\to\\data" --train_pct=.8
The script will go into the data folder, find every image (.jpg, .jpeg, .png, or .bmp) and label file (.txt), and randomly copy them into the train and validation folders. When the script finishes, your images and label files will be stored in the correct folder structure for training as shown in Figure 7.

Step 5 - Configure Training
There’s one last step before we can run training: we need to create the Ultralytics training configuration YAML file. This file specifies the location of your train and validation data, and it also defines the model’s classes. An example configuration file for my candy detection model is shown below.


Figure 8. Example of the data.yaml configuration file for the candy detection model.

To create the configuration file, open a text editor such as Notepad and copy + paste the following text into it.

path: C:\Users\<username>\Documents\yolo\data
train: train\images
val: validation\images

nc: 5

names: ["class1", "class2", "class3", "class4", "class5"]
Make the following changes:

Change “path:” to point at the data folder that was set up in Step 4 (e.g. C:\Users\Evan\Documents\yolo\data ).
Change the number after “nc:” to the number of classes your model is being trained to detect (e.g. “4” if you are using the Coin Detection Dataset).
Add the list of class names after “names:”. The classes should be in the same order of the labelmap defined in “classes.txt”. (e.g. [“penny”,”nickel”,”dime”,”quarter”] if you are using the Coin Detection Dataset).
Save the file as “data.yaml” in the “yolo” folder.

Once everything is configured, here’s what your “C:\Users\username\Documents\yolo” folder should look like.


Figure 9. Contents of our working folder prior to training YOLO model.

Now that we have the data is in the correct folder structure and we created the training configuration file, we’re ready to start training!

Step 6 - Train Model!
Before we run training, there are a couple important parameters we need to decide on: which model to use and how many epochs to train for.

Selecting a model
There are a range of model sizes available to train. Information on the YOLO11 model options is listed in the table below (source: Ultralytics YOLO11 Performance Metrics).


The important parameters to consider are mAP, params, and FLOPs. The mAP value indicates the relative accuracy of the model (i.e. how good it is at detecting objects) compared to the other models. The params and FLOPs columns indicate how “large” the model is: the higher these numbers are, the more compute resources the model will require. In general, the larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. If you aren’t sure which model size to use, the “yolo11s.pt” model is a good starting point.

You can also easily train YOLOv8 or YOLOv5 models by just using “yolov8” or “yolov5” when running the training command. We did a comparison of different model sizes from the YOLOv5, YOLOv8, and YOLO11 families to test their relative accuracy and speed on two different datasets and hardware platforms. To see the results of our comparison, see our YOLO Model Performance Comparison video on YouTube.

Model resolution
YOLO models are typically trained and inferenced at a 640x640 resolution. The model’s input resolution can be adjusted using the “–imgsz” argument during training. Model resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. Generally, you should just stick with using the default 640x640 resolution. If you want your model to run faster or know you will be working with low-resolution images, try using a lower resolution like 480x480. Keep an eye on our Learn page for an article providing more information on training and inference resolution for Ultralytics YOLO models.

Number of epochs
In machine learning, one “epoch” is one single pass through the full training dataset. In each epoch, every image in the dataset is fed into the model, and the learning algorithm updates the model’s internal weights to better fit the data in the image. Typically, many epochs are needed for the model to adjust its weights to the data in the images.

Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset, the model architecture, and the particular objects the model is being trained to detect. If your dataset has less than 200 images, a good starting point is 60 epochs. If your dataset has more than 200 images, a good starting point is 40 epochs.

For more information and tips on training models, see the Ultralytics Model Training Tips page.

Training the model
The “yolo detect train” command is used to run training. It has a few important arguments:

data: specifies the path to the training configuration file (which we set up in Step 5)
model: specifies which model architecture to train (e.g. “yolo11s.pt, “yolo11l.pt”). You can also train YOLOv5 and YOLOv8 models by replacing “yolo11” with “yolov5” or “yolov8” (e.g. “yolov5l.pt” or “yolov8s.pt”).
epochs: sets the number of epochs to train for
imgsz: sets the input dimension (i.e. resolution) of the YOLO model
Run the following command to begin training:

yolo detect train data=data.yaml model=yolo11s.pt epochs=60 imgsz=640
The training algorithm will parse the images in the training and validation directories and then start training the model. At the end of each training epoch, the program runs the model on the validation dataset and reports the resulting mAP, precision, and recall. As training continues, the mAP should generally increase with each epoch. Training will end once it goes through the number of epochs specified by “epochs”.


Figure 10. The progress of each training epoch will be reported as the training algorithm runs.

When training is finished (or if training is ended early with Ctrl+C), the trained model weights will be saved in “yolo\runs\detect\train\weights”. The “last.pt” file contains the model weights from the last training epoch. The “best.pt” contains the model weights from the training epoch that had the best mAP, precision, and recall. Generally, it’s recommended to use the “best.pt” model file for inference.

Additional information about training is saved in the “yolo\runs\detect\train” folder, including a “results.png” file that shows how loss, precision, recall, and mAP progressed over each epoch. If the mAP50 score doesn’t increase above 0.60, there is likely something wrong with your dataset (such as incorrect or conflicting labels).


Figure 11. The results.png image shows how the model's loss, precision, recall, and accuracy progressed during training.

Step 7 - Run Model
Now that the model has been trained, let’s run it on some images, a video, or a live webcam feed!


Figure 12. The candy detection model in action!

We wrote a basic Python script that shows how to load a model, run inference on an image source, parse the inference results, and display boxes around each detected class in the image. This script shows how to work with Ultralytics YOLO models in Python, and it can be used as a starting point for more advanced applications. Download the script to your PC by issuing:

curl -o yolo_detect.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/yolo_detect.py
The script takes two arguments, “model” and “source”:

–model : Path to the trained model weights (e.g. “runs/detect/train/weights/best.pt”)
–source: Path to a image file (“test_img.jpg”), a folder of images (“img_dir”), a video file (“test_vid.mp4”), or the index for a connected USB camera (“usb0”).
Here are example commands for running the script:

python yolo_detect.py --model=runs/detect/train/weights/best.pt --source=usb0  # Run on USB webacm
python yolo_detect.py --model=yolo11s.pt --source=test_vid.mp4 resolution=1280x720  # Run on test_vid.mp4 video file at 1280x720 resolution
When the script runs, it loads the model file and begin inferencing images from the image source. The script draws the detected bounding boxes on each image and displays it to the screen. Press “q” to stop the script.

Conclusions and Next Steps
Congratulations! You’ve successfully trained and deployed a YOLO object detection model. There are several things you can try next:

Extend your application beyond just drawing boxes around detected objects. Add functionality like logging the number of objects detected over time or taking a picture when certain objects are detected. Check out some example applications at our GitHub repository: https://github.com/EdjeElectronics/Train-and-Deploy-YOLO-Models
Deploy your model on the Raspberry Pi by following the instructions in our How to Run YOLO Detection Models on the Raspberry Pi article.
Keep an eye on our Learn page and YouTube channel for articles and videos showing code examples and instructions for how to deploy YOLO models to other platforms.
If the model isn’t doing great at detecting objects, you can improve the model’s accuracy by adding more images to the training dataset. You should also review your training dataset to make sure there aren’t any mistakes with the labels.

EJ Technology Consultants has developed production-quality YOLO models for various applications, like traffic safety, casino game monitoring, hazard detection, and more. If you need help with any part of the model development process, whether it’s building a dataset, finding optimal training parameters, or deploying the model in an application, please contact us at info@ejtech.io or schedule a meeting on our Contact page. In the meantime, keep on training, and good luck with your projects!

EJ Technology Consultants
EJTC provides deep learning, computer vision, and embedded design services.

Company
Home
Services
Portfolio
About
Resources
Learn
Contact
Contact Us
Email: info@ejtech.io
Address: 544 E Main St Unit B, Bozeman, MT 59715
(View Map)
© 2025 EJ Technology Consultants
Skip to main content

Ultralytics

Sign Up

Log In

​

​
Yolo11 training with custom dataset
Support
code
,
discussion
,
yolo

2.1k
views

6
likes

6
links



Jan 19
Feb 17

Muhammad_Anas

1
Jan 19
Hi everyone,

I’m working on training a YOLOv11 model for object detection on a custom dataset of 11 classes. I have 334 annotated images formatted in the “YOLOV8 DETECTION 1.0” label format using CVAT. However, my data is unbalanced.

Here’s the training code I used:


from google.colab import drive
drive.mount('/content/drive')
dataset_path = '/content/drive/MyDrive/Files/dataset'

from ultralytics import YOLO

model = YOLO("yolo11n.pt") 
model.train(data=f"{dataset_path}/data.yaml", epochs=200, imgsz=640)

val_results = model.val(data=f"{dataset_path}/data.yaml")

test_results = model.val(data=f"{dataset_path}/data.yaml", split='test')

metrics = model.val() 
metrics.box.map # map50-95
metrics.box.map50 # map50
metrics.box.map75 # map75
metrics.box.maps # a list contains map50-95 of each category
Unfortunately, the current model performance isn’t meeting my expectations (see attached results). I’m hoping to improve the accuracy and I’d appreciate any guidance from the community.

Specifically, I have the following questions:

How many more annotated images are typically needed for good transfer learning results?
My classes mostly match the YOLO classes with the same IDs. Why is my trained model (best.pt) not detecting objects (cars, buses, etc.) that the pre-trained model (yolo11n.pt) does?
Is there a way to get a precise accuracy measurement in percentage?
How can I prevent overfitting during training?
Thanks in advance for any advice!

Best regards,
Muhammad Anas

F1_curve
F1_curve
2250×1500 320 KB


2.1k
views

6
likes

6
links



22 days later

Reply

Related topics
Topic list, column headers with buttons are sortable.
Topic	Replies	Views	Activity
Yolo very bad results on custom dataset val
YOLO
3	358	Oct 2024
I Need Help with YOLOv5 Training for Custom Object Detection
YOLO
support
,
yolov5
1	438	Jul 2024
Training custom dataset
Discussion
discussion
,
question
7	514	Nov 2024
Ultralytics YOLO11 Released tada
Discussion
showcase
,
ultralytics-official
1	575	Oct 2024
Helping with elevating YOLOv11’s performance in Human Detection task
YOLO
3	455	Feb 18
This website uses cookies to ensure you get the best experience on our website. Learn more
Got it!
Skip to main content

Image.sc Forum
zulipchat-light
relatedforums-light

Sign Up

Log In

​
  Community Partners
Step by Step Procedure to Train a Model Using YOLOv8 on a Custom Dataset
Image Analysis
yolov8
Feb 2024
Feb 2024

CJ MEI
cj.mei_ikonisys.com

1
Feb 2024
Step by Step Procedure to Train a Model Using YOLOv8 with a Custom Dataset

Easy to use should always be one of the most important requirements of algorithm, tool and method development. It is also a journey of continuing progressing. AI models have shown the great power in image processing. But the cumbersome annotation training dataset preparation and the extensive training computation scared away a lot of beginners like me; or stopped some like the below quoted user.

Thank for the great works from numerous unknown researchers, YOLOv8, a Py Torch based NN system, seems starting to provide an easier to use tool to train a model on custom dataset quite conveniently. The below quoted user had inquired the knowledge of training on custom data using yolov7 in two posts, but no answer. Now it comes. There had been helpful discussions with @bnorthan. Please accept my sincere thanks.

The “Step by Step Procedure to Train a Model Using YOLOv8 on a Custom Dataset” has already been posted on YouTube:
" Complete YOLO v8 Custom Object Detection Tutorial | Windows & Linux (youtube.com)"
which I dare not taking the credit. But I personally tested the procedure and it worked beautifully. With only two images with simple box annotation, the model can be trained to an amazing level. It took me not more than 2 hours. The whole training computation can be done on Google colab. The total computation time is less than 10 minutes (with GPU).

I started with the following 2 images:

img_d

img_u

I divided them into 16 smaller images. I took 12 as training set and 4 as validation set. With simple box annotation, I annotated the objects into 4 classes. After about 5 minutes training, the detect of one class is as following:

77

If more training data added and trained further, much better result can be expected. The amazing thing is that it looks like a demo, but already of practical usage.

@nishata24
“Yolov7 training on custom data”
“Yolov7 custom training for pose estimation”



7.1k
views

4
links

Reply

Related topics
Topic list, column headers with buttons are sortable.
Topic	Replies	Views	Activity
Yolov7 custom training for pose estimation
Image Analysis
deeplabcut
,
machine-learning
,
computer-vision
,
yolo
,
pose-estimation
,
custom-training
,
yolov7
0	876	Feb 2023
Yolov7 training on custom data
Usage & Issues
deeplabcut
,
training
,
computer-vision
,
yolo
,
pose-estimation
2	764	Feb 2023
Computing requirements
Image Analysis
cellprofiler
1	263	Oct 2012
ZeroCostDL4Mic - YOLO2
Usage & Issues
zerocostdl4mic
2	445	Dec 2020
Tool for segmenting images with weak supervision
Image Analysis
segmentation
8	1.1k	May 2019
